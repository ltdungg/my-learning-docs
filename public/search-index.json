[{"content":"Well-thought-through product announcements will help increase feature awareness and engage users with new functionality. Just like sharing your public roadmap, it\u0026rsquo;s also a great way to let potential customers see that you\u0026rsquo;re constantly improving.\nFurther reading Read How to announce product updates and features ","date":"2025-01-20","id":0,"permalink":"/blog/example-post/","summary":"You can use blog posts for announcing product updates and features.","tags":[],"title":"Example Post"},{"content":"","date":"2023-09-07","id":1,"permalink":"/blog/","summary":"","tags":[],"title":"Blog"},{"content":"","date":"2023-09-07","id":2,"permalink":"/de-docs/overview/","summary":"","tags":[],"title":"Giới thiệu"},{"content":"","date":"2023-09-07","id":3,"permalink":"/docs/overview/","summary":"","tags":[],"title":"Giới thiệu"},{"content":"","date":"2023-09-07","id":4,"permalink":"/de-docs/spark/","summary":"","tags":[],"title":"Mở đầu về Spark"},{"content":"\rSpark là gì? Apache Spark là một hệ thống xử lý dữ liệu phân tán mã nguồn mở và được sử dụng cho các công việc xử lý dữ liệu lớn. Có thể sử dụng trên: on premises hay trên cloud. Spark sử dụng khả năng lưu trữ và xử lý dữ liệu trên bộ nhớ thay vì sử dụng ổ cứng (HDD/SSD). Làm tăng khả năng xử lý dữ liệu, khả năng xử lý nhanh hơn Hadoop MapReduce. Apache Spark cung cấp API phát triển bằng ngôn ngữ Python, Scala, Java. Có thể sử dụng cho việc: Machine Learning (MLlib). Truy vấn, tương tác với dữ liệu (Spark SQL). Xử lý dữ liệu thời gian thực (Structured Streaming). Xử lý đồ thị (GraphX). Spark được thiết kế xung quanh 4 triết lý chính: Tốc độ (Speed) Dễ sử dụng (Ease of use) Tính mô đun (Modularity) Khả năng mở rộng (Extensibility) Spark có cộng đồng lớn và một danh sách các gói third-party Spark là một phần của sự phát triển của cộng đồng. ","date":"2023-09-07","id":5,"permalink":"/de-docs/spark/khai-niem/","summary":"\u003cstyle\u003ebody {text-align: justify}\u003c/style\u003e\r\n\u003ch3 id=\"spark-là-gì\"\u003eSpark là gì?\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cspan style=\"color: orange; font-weight:bold;\"\u003eApache Spark\u003c/span\u003e là một hệ thống xử lý dữ liệu phân tán mã nguồn mở và được sử dụng cho các công việc xử lý dữ liệu lớn.\u003c/li\u003e\n\u003cli\u003eCó thể sử dụng trên: \u003cspan style=\"color: orange; font-weight:bold;\"\u003eon premises\u003c/span\u003e hay \u003cspan style=\"color: orange; font-weight:bold;\"\u003etrên cloud\u003c/span\u003e.\u003c/li\u003e\n\u003cli\u003eSpark sử dụng khả năng lưu trữ và xử lý dữ liệu trên bộ nhớ thay vì sử dụng ổ cứng (HDD/SSD).\n\u003cul\u003e\n\u003cli\u003eLàm tăng khả năng xử lý dữ liệu, khả năng xử lý nhanh hơn Hadoop MapReduce.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cspan style=\"color: orange; font-weight:bold;\"\u003eApache Spark\u003c/span\u003e cung cấp API phát triển bằng ngôn ngữ Python, Scala, Java. Có thể sử dụng cho việc:\n\u003cul\u003e\n\u003cli\u003eMachine Learning (MLlib).\u003c/li\u003e\n\u003cli\u003eTruy vấn, tương tác với dữ liệu (Spark SQL).\u003c/li\u003e\n\u003cli\u003eXử lý dữ liệu thời gian thực (Structured Streaming).\u003c/li\u003e\n\u003cli\u003eXử lý đồ thị (GraphX).\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eSpark được thiết kế xung quanh 4 triết lý chính:\n\u003cul\u003e\n\u003cli\u003eTốc độ (Speed)\u003c/li\u003e\n\u003cli\u003eDễ sử dụng (Ease of use)\u003c/li\u003e\n\u003cli\u003eTính mô đun (Modularity)\u003c/li\u003e\n\u003cli\u003eKhả năng mở rộng (Extensibility)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eSpark có cộng đồng lớn và một danh sách các gói third-party Spark là một phần của sự phát triển của cộng đồng.\n\u003cbr\u003e \u003cbr\u003e\n\r\n\r\n\u003cimg\r\n  src=\"/de-docs/spark/images/spark-third-party_hu_842eb3a923965df8.webp\"\r\n  width=\"940\"\r\n  height=\"701\"\r\n  decoding=\"async\"\r\n  fetchpriority=\"auto\"\r\n  loading=\"lazy\"\r\n  alt=\"spark-third-party\"id=\"h-rh-i-0\"\r\n/\u003e\u003c/li\u003e\n\u003c/ul\u003e","tags":[],"title":"1. Giới thiệu về Spark"},{"content":"\rSpark bao gồm 5 thành phần chính: Spark Core, Spark SQL, Spark Streaming, MLlib và GraphX.\nSpark SQL Như tên gọi của nó, thì mô-đun này làm việc tốt với structured data. Có thể đọc được dữ liệu lưu trữ trong bảng RDBMS hay các file có định dạng lưu trữ có cấu trúc như (csv, text, JSON, Avro, ORC, Parquet, \u0026hellip;). Khi sử dụng Spark qua API bằng Java, Python, Scala hay R. Bạn có thể sử dụng lệnh SQL để truy vấn dữ liệu. Ví dụ: Bạn có thể đọc file JSON lưu trữ trong Amazon S3, tạo một bảng tạm thời, và sử dụng lệnh SQL để đọc dữ liệu.\n// In Scala // Đọc dữ liệu từ S3 Bucket vào Spark DataFrame spark.read.json(\u0026#34;s3://apache_spark/data/committers.json\u0026#34;) .createOrReplaceTempView(\u0026#34;committers\u0026#34;) // Sử dụng SQL để trả về kết quả là một Spark DataFrame val results = spark.sql(\u0026#34;\u0026#34;\u0026#34;SELECT name, org, module, release, num_commits FROM committers WHERE module = \u0026#39;mllib\u0026#39; AND num_commits \u0026gt; 10 ORDER BY num_commits DESC\u0026#34;\u0026#34;\u0026#34;)\rBạn có thể viết một đoạn code như vậy với Python, R hay Java. Các mã byte sẽ được tạo giống hệt nhau, trả ra kết quả với hiệu suất tương tự.\nSpark MLlib Spark mang đến một thư viện các thuật toán Machine Learning phổ biến gọi là MLlib. Với hiệu suất được cải thiện đáng kể từ khi lần đầu ra mắt thư viện này. API này cho phép trích xuất hay chuyển đổi tính năng, xây dựng pipelines (cho việc huấn luyện và đánh giá), và persist models (cho việc saving và reloading models) trong quá trình triển khai. Thêm vào đó, có các tiện ích bổ sung bao gồm sử dụng các phép toán đại số tuyến tính và thống kê phổ biến. Ví dụ: Dưới đây là một đoạn code python bao gồm những hoạt động cơ bản của một data scientist sẽ làm khi xây một mô hình:\n# In Python from pyspark.ml.classification import LogisticRegression ... training = spark.read.csv(\u0026#34;s3://...\u0026#34;) test = spark.read.csv(\u0026#34;s3://...\u0026#34;) # Load training data lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8) # Fit the model lrModel = lr.fit(training) # Predict lrModel.transform(test) ...\rSpark Structured Streaming Được xây dựng dựa trên Spark SQL engine và DataFrame-based APIs. Từ phiên bản Spark 2.2, Structured Straming có thể được sử dụng cả trên môi trường production. Spark Structured Streaming cần thiết cho Big Data Developers để kết hợp và phản ứng với dữ liệu tại thời gian thực từ các nguồn như Apache Kafka, hay các nguồn khác. Developers có thể coi như đây là một bảng dữ liệu có cấu trúc và xử lý, truy vấn như một bảng tĩnh. Ví dụ: Dưới đây là một đoạn code mẫu về Spark Structured Streaming\n# In Python # Đọc luồng dữ liệu từ local host from pyspark.sql.functions import explode, split lines = (spark .readStream .format(\u0026#34;socket\u0026#34;) .option(\u0026#34;host\u0026#34;, \u0026#34;localhost\u0026#34;) .option(\u0026#34;port\u0026#34;, 9999) .load()) # Thực hiện việc chuyển đổi # Chia các dòng thành các từ words = lines.select(explode(split(lines.value, \u0026#34; \u0026#34;))).alias(\u0026#34;word\u0026#34;) # Tạo bộ đếm từ word_counts = words.groupBy(\u0026#34;word\u0026#34;).count() # Ghi vào Kafka query = (word_counts .writeStream .format(\u0026#34;kafka\u0026#34;) .option(\u0026#34;topic\u0026#34;, \u0026#34;output\u0026#34;))\rGraphX Như tên gọi, GraphX là một thư viện để thao tác với đồ thị (ví dụ: đồ thị mạng xã hội, đồ thị network,\u0026hellip;). GraphX cung cấp thuật toán đồ thị tiêu chuẩn cho việc phân tích, kết nối và truyền tải được đóng góp bởi cộng đồng như: PageRank, Connected Components và Triangle Counting. Ví dụ: Đoạn code dưới đây là một ví dụ về việc join 2 graph sử dụng GraphX APIs.\n// In Scala val graph = Graph(verticles, edges) messages = spark.textFile(\u0026#34;hdfs://...\u0026#34;) val graph2 = graph.joinVertices(messages) { (id, vertex, msg) =\u0026gt; ... }\r","date":"2023-09-07","id":6,"permalink":"/de-docs/spark/thanh-phan-cua-spark/","summary":"\u003cstyle\u003ebody {text-align: justify}\u003c/style\u003e\r\n\u003cp\u003e\u003cbr\u003e\n\r\n\r\n\u003cimg\r\n  src=\"/de-docs/spark/images/spark-components_hu_15d87a9c5be03bb3.webp\"\r\n  width=\"1314\"\r\n  height=\"391\"\r\n  decoding=\"async\"\r\n  fetchpriority=\"auto\"\r\n  loading=\"lazy\"\r\n  alt=\"Spark Components\"id=\"h-rh-i-0\"\r\n/\u003e\u003c/p\u003e\n\u003cp\u003eSpark bao gồm 5 thành phần chính: Spark Core, Spark SQL, Spark Streaming, MLlib và GraphX.\u003c/p\u003e","tags":[],"title":"2. Thành phần của Spark"},{"content":"\rHãy bắt đầu bằng cách nhìn từng thành phần của Spark trong ảnh. Spark Driver Là một phần của Spark application chịu trách nhiệm cho việc khởi tạo SparkSession. Spark Driver có nhiều vai trò như:\nGiao tiếp với cluster manager. Gửi yêu cầu sử dụng tài nguyên (CPU, memory,\u0026hellip;) từ cluster manager cho Spark\u0026rsquo;s Executor (JVMs). Chuyển đổi spark operations thành DAG computations, schedules (tính toán, lên lịch). Spark Session Kể tử Spark 2.0, SparkSession được coi như là cách kết nối thống nhất đến tất cả các hoạt động của Spark và dữ liệu. Gộp các entrypoint cũ như SparkContext, SQLContext, HiveContext, SparkConf và StreamingContext. Khiến việc sử dụng Spark trở nên dễ dàng hơn. Tức là kể từ giờ SparkSession là entrypoint cho tất cả các tính năng của Spark. Ví dụ: Dưới đây là một ví dụ về việc sử dụng SparkSession và thực hiện một số hoạt động truy vấn dữ liệu.\n// In Scala import org.apache.spark.sql.SparkSession // Build SparkSession val spark = SparkSession .builder .appName(\u0026#34;LearnSpark\u0026#34;) .config(\u0026#34;spark.sql.shuffle.partitions\u0026#34;, 6) .getOrCreate() // Sử dụng session để đọc file JSON val people = spark.read.json(\u0026#34;...\u0026#34;) // Sử dụng sesion để truy vấn một query và lưu thành DataFrame val resultsDF = spark.sql(\u0026#34;SELECT city, pop, state, zip FROM table_name\u0026#34;)\rCluster Manager Cluster Manager chịu trách nhiệm cho việc quản lý và phân bố tài nguyên cho cụm các node cho Spark. Hiện tại, Spark hỗ trợ 4 cluster managers:\ncluster managers được tích hợp sẵn trong Spark. Apache Hadoop YARN Apache Mesos Kubernetes Spark Executor Spark Executor chạy trên từng worker node trong cụm. Spark Executor giao tiếp với Spark Driver và chịu trách nhiệm cho việc thực thi các nhiệm vụ trên các worker. Trong hầu hết các chế độ triển khai (Deployment modes), chỉ có một executor trên một node.\nDeployment modes Hỗ trợ nhiều chế độ triển khai, giúp Spark chạy trên các configurations và environments khác nhau. Một số môi trường phổ biến như Apache Hadoop Yarn và Kubernetes và có thể hoạt động trên nhiều chế độ.\nDistributed data and partitions Dữ liệu vật lý thực tế được phân phối trên bộ lưu trữ dưới dạng các phân vùng (partitions) nằm trong HDFS hay cloud storage. Trong khi dữ liệu được phân phối thành các partitions trên các cụm vật lý (physical cluster). Spark xử lý từng partition dưới dạng trừu tượng dữ liệu logic cấp cao - như một DataFrame trong bộ nhớ.\nViệc phân vùng dữ liệu giúp cho hiệu suất xử lý song song hiệu quả. Việc này giúp cho Spark Executor chỉ xử lý dữ liệu gần với chúng nhất, giảm thiểu băng thông mạng.\nVí dụ: Đoạn code sau đây sẽ chuyển đổi dữ liệu vật lý lưu trên clusters thành 8 partition, và từng executor sẽ có một hoặc nhiều partitions để đọc vào memory.\n# In Python log_df = spark.read.text(\u0026#34;path_to_large_text_file\u0026#34;).repartition(8) print(log_df.rdd.getNumPartitions())\rVí dụ 2: Đoạn code dưới đây sẽ tạo một DataFrame với 10000 số nguyên phân bố trên 8 partitions trong bộ nhớ:\n# In Python df = spark.range(0, 10000, 1, 8) print(df.rđ.getNumPartitions())\r2 ví dụ trên đều có output là 8.\n","date":"2023-09-07","id":7,"permalink":"/de-docs/spark/cach-hoat-dong/","summary":"\u003cstyle\u003ebody {text-align: justify}\u003c/style\u003e\r\n\u003cp\u003eHãy bắt đầu bằng cách nhìn từng thành phần của Spark trong ảnh.\n\u003cbr\u003e\n\u003cbr\u003e\n\r\n\r\n\u003cimg\r\n  src=\"/de-docs/spark/images/architecture_hu_3d09a1ec182b9465.webp\"\r\n  width=\"724\"\r\n  height=\"372\"\r\n  decoding=\"async\"\r\n  fetchpriority=\"auto\"\r\n  loading=\"lazy\"\r\n  alt=\"Architecture\"id=\"h-rh-i-0\"\r\n/\u003e\u003c/p\u003e\n\u003ch3 id=\"spark-driver\"\u003eSpark Driver\u003c/h3\u003e\n\u003cp\u003eLà một phần của Spark application chịu trách nhiệm cho việc khởi tạo SparkSession.\nSpark Driver có nhiều vai trò như:\u003c/p\u003e","tags":[],"title":"3. Cách Spark hoạt động"},{"content":"\rMột số khái niệm cần biết: Application: Một chương trình người dùng được xây dựng trên Spark và sử dụng API của nó. Bao gồm chương trình điều khiển (driver program) và executors trong cụm. SparkSession Một đối tượng cung cấp điểm vào để tương tác với các chức năng của Spark. Trong Spark Shell đã cung cấp sẵn SparkSession cho bạn nhưng trong Spark Application, bạn phải tự tạo SparkSession riêng. Job Một chương chình tính toán song song bao gồm nhiều tác vụ được tạo ra để phản hồi cho một Spark action (ví dụ: save(), collect()) Stage Mỗi job được phân chia thành các tác vụ nhỏ hơn được gọi là stages, chúng phụ thuộc lẫn nhau. Task Một đơn vị công việc (work) hay thực thi (executor) được gửi cho Spark Executor.\nSpark Jobs Spark Driver chuyển đổi Spark Application của bạn thành một hay nhiều Spark Jobs. Nó được chuyển đổi từng job vào một DAG (Directed Acyclic Graph)\nHình 1: Spark Driver tạo thành một hay nhiều Spark Jobs.\nSpark Stages Là một phần của DAG nodes, stages được tạo nên dựa trên các hoạt động có thể được thực hiện theo chuỗi hay song song. Không phải hoạt động nào của Spark cũng có thể xảy ra trên 1 stage, vì vậy nó có thể chia thành nhiều stage.\nHình 2: Spark Job tạo một hay nhiều stage (giai đoạn)\nSpark Tasks Mỗi stage bao gồm các Spark Tasks (một đơn vị thực thi), được liên kết trên mỗi Spark executor. Ví dụ một Executor với 16 cores có thể có 16 task hoặc hơn làm việc trên 16 partitions hoặc hơn, làm cho việc xử lý task cực kỳ song song!\nHình 3: Spark Stage tạo một hay nhiều task được phân phối đến executor.\nTransformation, Actions and Lazy Evaluation. Spark operations trên dữ liệu phân tán có thể phân thành 2 loại: tranformations và actions. Transformations: Chuyển đổi Spark DataFrame thành một DataFrame mới mà không làm thay đổi data gốc, mang lại cho nó đặc tính bất biến (immutability). Tất cả các tranformations được đánh giá một cách lười biếng (Evaluated Lazily), có nghĩa là kết quả không được tính toán trực tiếp, mà nó được ghi lại như là một lineage . Việc này cho phép Spark sau đó sắp xếp lại một số transformations nhất định, hợp nhất hay tối ưu hóa chúng thành các giai đoạn với hiệu quả cao hơn. Đồng thời mang lại khả năng phục hồi khi xảy ra lỗi.\nHình 4: Trong hình, Các transformations T được lưu lại cho đến khi action A được gọi đến. Mỗi tranformations tạo ra một DataFrame mới. Hình 5: Một số ví dụ về Tranformations và Actions trong Spark\nVí dụ: Không có gì xuất hiện khi lệnh .conut() dược thực thi:\n# In Python strings = spark.read.text(\u0026#34;../README.md\u0026#34;) filtered = strings.filter(strings.value.contains(\u0026#34;Spark\u0026#34;)) filtered.count() # Output: 20\rNarrow and Wide Transformations Transformations có thể phân loại thành:\nNarrow dependencies (Phụ thuộc hẹp): Bất kỳ Transformation nào mà không phải qua biến đổi mà đầu ra có thể được tính toán duy nhất từ partitions đầu vào. Ví dụ: filter(), contains() là narrow dependencies. Wide dependencies (Phụ thuộc rộng): Ngược lại thì để thực hiện narrow dependencies thì các partitions được đọc, kết hợp lại và được ghi vào trong disk. Ví dụ: groupBy() , orderBy() Hình 6: Narrow vs Wide transformations.\n","date":"2023-09-07","id":8,"permalink":"/de-docs/spark/spark-application-concept/","summary":"\u003cstyle\u003ebody {text-align: justify}\u003c/style\u003e\r\n\u003ch3 id=\"một-số-khái-niệm-cần-biết\"\u003eMột số khái niệm cần biết:\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eApplication:\u003c/em\u003e\u003c/strong\u003e\n\u003cbr\u003e\nMột chương trình người dùng được xây dựng trên Spark và sử dụng API của nó. Bao gồm chương trình điều khiển (driver program) và executors trong cụm.\n\u003cbr\u003e\n\u003cbr\u003e\n\u003cstrong\u003e\u003cem\u003eSparkSession\u003c/em\u003e\u003c/strong\u003e\n\u003cbr\u003e\nMột đối tượng cung cấp điểm vào để tương tác với các chức năng của Spark.\nTrong Spark Shell đã cung cấp sẵn SparkSession cho bạn nhưng trong Spark Application, bạn phải tự tạo SparkSession riêng.\n\u003cbr\u003e\n\u003cbr\u003e\n\u003cstrong\u003e\u003cem\u003eJob\u003c/em\u003e\u003c/strong\u003e\n\u003cbr\u003e\nMột chương chình tính toán song song bao gồm nhiều tác vụ được tạo ra để phản hồi cho một Spark action (ví dụ: save(), collect())\n\u003cbr\u003e\n\u003cbr\u003e\n\u003cstrong\u003e\u003cem\u003eStage\u003c/em\u003e\u003c/strong\u003e\n\u003cbr\u003e\nMỗi job được phân chia thành các tác vụ nhỏ hơn được gọi là stages, chúng phụ thuộc lẫn nhau.\n\u003cbr\u003e\n\u003cbr\u003e\n\u003cstrong\u003e\u003cem\u003eTask\u003c/em\u003e\u003c/strong\u003e\n\u003cbr\u003e\nMột đơn vị công việc (work) hay thực thi (executor) được gửi cho Spark Executor.\u003c/p\u003e","tags":[],"title":"4. Các khái niệm Spark Application"},{"content":"\rSpark DataFrame được lấy cảm hứng từ pandas DataFrame về cấu trúc, định dạng và một số thao tác cụ thể. Ví dụ về một Spark DataFrame được hiển thị tại Hình 1.\nHình 1: Table-like format of a DataFrame\nDataFrame trong Spark bất biến và Spark giữ lại và theo dõi tất cả các transformations (Được nêu ở phần 4). Vì thế bạn có thể sửa đổi tên hoặc data types của cột, tạo DataFrame mới mà bản cũ vẫn được bảo tồn.\nSpark\u0026rsquo;s Basic Data Types Những data types cơ bản có trong Python cũng được Spark hỗ trợ. Ví dụ như Hình 2 dưới đây.\nHình 2: Basic Python data types in Spark\nSpark\u0026rsquo;s Structured and Complex Data Types Để dùng cho những việc phân tích phức tạp, bạn không chỉ đối mặt với những kiểu dữ liệu cơ bản trên. Mà dữ liệu của bạn sẽ phức tạp hơn, thường có cấu trúc hoặc lồng nhau. Vì vậy, bạn cần Spark có thể xử lý được những kiểu dữ liệu phức tạp đó. Có thể như: maps, arrays, structs, dates, timestamps, fields,\u0026hellip;\nTrong Python, các kiểu dữ liệu tương đương cũng được Spark hỗ trợ, ví dụ\nHình 3: Python structured data in Spark\nSchemas and Creating DataFrames Một schema trong Spark xác định tên cột và liên kết với kiểu dữ liệu cho một DataFrame. Schema thường được sử dụng khi đọc dữ liệu có cáu trúc từ nguồn dữ liệu bên ngoài. Tác dụng của Schema:\nGiúp Spark có thể tránh khỏi việc tự suy luận ra kiểu dữ liệu. Giúp Spark ngăn chặn việc tạo một công việc riêng để đọc một khối lượng lớn dữ liệu để xác định ra lược đồ (schema). Với một lượng dữ liệu lớn, điều này có thể gây tốn kém và tốn thời gian. Bạn có thể phát hiện lỗi sớm nếu dữ liệu không khớp với lược đồ (schema). Có 2 cách để định nghĩa một schema Một là, định nghĩa một cách rất lập trình (programmatically). Hai là, triển khai một DDL (Data Definition Language) String, cách này đơn giản và dễ đọc hơn. Để định nghĩa một schema programmatically cho một DataFrame với 3 cột: author, title và pages, bạn có thể sử dụng Spark DataFrame API. Ví dụ:\n// In Scala import org.apache.spark.sql.types._ val schema = StructType(Array(StructField(\u0026#34;author\u0026#34;, StringType, false), StructField(\u0026#34;title\u0026#34;, StringField, false), StructField(\u0026#34;pages\u0026#34;, IntegerType, false)))\r# In Python from pyspark.sql.types import * schema = StructType([StructField(\u0026#34;author\u0026#34;, StringType(), False), StructField(\u0026#34;title\u0026#34;, StringType(), False), StructField(\u0026#34;pages\u0026#34;, IntegerType(), False)])\rCách định nghĩa thứ 2 sử dụng DDL đơn giản hơn rất nhiều, ví dụ:\n// In Scala val schema = \u0026#34;author STRING, title STRING, pages INT\u0026#34;\r# In Python schema = \u0026#34;author STRING, title STRING, pages INT\u0026#34;\rBạn có thể chọn cách định nghĩa schema tùy thích.\nVí dụ: Tạo SparkSession và xác định lược đồ và thêm dữ liệu vào trong DataFrame.\n# In Python # Example-3_6.py from pyspark.sql import SparkSession # Xác định lược đồ của dữ liệu bằng DDL schema = \u0026#34;`Id` INT, `First` STRING, `Url` STRING, `Published` STRING, `Hits` INT, `Campaigns` ARRAY\u0026lt;STRING\u0026gt;\u0026#34; # Tạo static data data = [[1, \u0026#34;Jules\u0026#34;, \u0026#34;Damji\u0026#34;, \u0026#34;https://tinyurl.1\u0026#34;, \u0026#34;1/4/2016\u0026#34;, 4535, [\u0026#34;twitter\u0026#34;, \u0026#34;LinkedIn\u0026#34;]], [2, \u0026#34;Brooke\u0026#34;,\u0026#34;Wenig\u0026#34;, \u0026#34;https://tinyurl.2\u0026#34;, \u0026#34;5/5/2018\u0026#34;, 8908, [\u0026#34;twitter\u0026#34;,\u0026#34;LinkedIn\u0026#34;]], [3, \u0026#34;Denny\u0026#34;, \u0026#34;Lee\u0026#34;, \u0026#34;https://tinyurl.3\u0026#34;, \u0026#34;6/7/2019\u0026#34;, 7659, [\u0026#34;web\u0026#34;, \u0026#34;twitter\u0026#34;, \u0026#34;FB\u0026#34;, \u0026#34;LinkedIn\u0026#34;]], [4, \u0026#34;Tathagata\u0026#34;, \u0026#34;Das\u0026#34;, \u0026#34;https://tinyurl.4\u0026#34;, \u0026#34;5/12/2018\u0026#34;, 10568, [\u0026#34;twitter\u0026#34;, \u0026#34;FB\u0026#34;]], [5, \u0026#34;Matei\u0026#34;,\u0026#34;Zaharia\u0026#34;, \u0026#34;https://tinyurl.5\u0026#34;, \u0026#34;5/14/2014\u0026#34;, 40578, [\u0026#34;web\u0026#34;, \u0026#34;twitter\u0026#34;, \u0026#34;FB\u0026#34;, \u0026#34;LinkedIn\u0026#34;]], [6, \u0026#34;Reynold\u0026#34;, \u0026#34;Xin\u0026#34;, \u0026#34;https://tinyurl.6\u0026#34;, \u0026#34;3/2/2015\u0026#34;, 25568, [\u0026#34;twitter\u0026#34;, \u0026#34;LinkedIn\u0026#34;]]] # Hàm chính if __name__ == \u0026#34;__main__\u0026#34;: # Khởi tạo SparkSession spark = (SparkSession .builder .appName(\u0026#34;CreatingDataFrame\u0026#34;) .getOrCreate()) # Tạo một DataFrame bằng schema đã được định nghĩa blogs_df = spark.createDataFrame(data=data, schema=schema) # Hiển thị DataFrame blogs_df.show() # Hiển thị schema print(blogs_df.printSchema())\rChạy chương trình trong console sẽ được output như sau:\n$ spark-submit Example-3_6.py ... +-------+---------+-------+-----------------+---------+-----+------------------+ |Id |First |Last |Url |Published|Hits |Campaigns | +-------+---------+-------+-----------------+---------+-----+------------------+ |1 |Jules |Damji |https://tinyurl.1|1/4/2016 |4535 |[twitter,...] | |2 |Brooke |Wenig |https://tinyurl.2|5/5/2018 |8908 |[twitter,...] | |3 |Denny |Lee |https://tinyurl.3|6/7/2019 |7659 |[web, twitter...] | |4 |Tathagata|Das |https://tinyurl.4|5/12/2018|10568|[twitter, FB] | |5 |Matei |Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter,...]| |6 |Reynold |Xin |https://tinyurl.6|3/2/2015 |25568|[twitter,...] | +-------+---------+-------+-----------------+---------+-----+------------------+ root |-- Id: integer (nullable = false) |-- First: string (nullable = false) |-- Last: string (nullable = false) |-- Url: string (nullable = false) |-- Published: string (nullable = false) |-- Hits: integer (nullable = false) |-- Campaigns: array (nullable = false) | |-- element: string (containsNull = false) Columns and Expression Có thể sử dụng biểu thức toán học, logic lên các cột. Ví dụ bạn có thể tạo một biểu thức cơ bản sử dụng expr(\u0026ldquo;ColumnName\u0026rdquo; * 5) hoặc (expr(\u0026ldquo;columnName\u0026rdquo;) - 5 \u0026gt; col(anotherColumnName)). Câu lệnh expr() là một phần gói của pyspark.sql.functions (Python) và org.apache.spark.sql.functions (Scala). Câu lệnh này lấy đối số mà Spark có thể phân tích thành biểu thức và tính toán ra kết quả.\nNOTE: Cả Scala, Java, Python đều có một phương thức public liên kết với cột. Trong Spark document đề cập đến col và Column. Column là tên của đối tượng, còn col() là một hàm để trả về một Column.\nBạn hãy xem ví dụ dưới đây để thấy được chúng ta có thể làm gì với cột trong Spark.\n// In Scala scala\u0026gt; import org.apache.spark.sql.functions._ scala\u0026gt; blogsDF.columns res2: Array[String] = Array(Campaigns, First, Hits, Id, Last, Published, Url) // Truy cập một cột cụ thể và trả về loại Cột scala\u0026gt; blogsDF.cols(\u0026#34;Id\u0026#34;) res3: org.apache.spark.sql.Column = id // Sử dụng biểu thức để tính toán scala\u0026gt; blogsDF.select(expr(\u0026#34;Hits * 2\u0026#34;)).show(2) // Hoặc sử dụng col() để tính toán giá trị cùng kết quả với câu lệnh trên scala\u0026gt; blogsDF.select(col(\u0026#34;Hits\u0026#34;) * 2).show(2) +----------+ |(Hits * 2)| +----------+ | 9070 | | 17816 | +----------+ // Sử dụng biểu thức để tính toán những người nổi tiếng về blog // Câu lệnh này sẽ tạo ra cột mới, tên \u0026#34;Big Hitters\u0026#34;, dựa trên biểu thức điều kiện blogsDF.withColumn(\u0026#34;Big Hitters\u0026#34;, (expr(\u0026#34;Hits \u0026gt; 10000\u0026#34;))).show() Output: +---+---------+-------+---+---------+-----+--------------------+-----------+ | Id| First | Last |Url|Published| Hits| Campaigns |Big Hitters| +---+---------+-------+---+---------+-----+--------------------+-----------+ | 1 | Jules | Damji |...| 1/4/2016| 4535| [twitter, LinkedIn]| false | | 2 | Brooke | Wenig |...| 5/5/2018| 8908| [twitter, LinkedIn]| false | | 3 | Denny | Lee |...| 6/7/2019| 7659|[web, twitter, FB...| false | | 4 |Tathagata| Das |...|5/12/2018|10568| [twitter, FB] | true | | 5 | Matei |Zaharia|...|5/14/2014|40578|[web, twitter, FB...| true | | 6 | Reynold | Xin |...| 3/2/2015|25568| [twitter, LinkedIn]| true | +---+---------+-------+---+---------+-----+--------------------+-----------+\r// Kết hợp 3 cột, tạo cột mới và hiển thị. blogsDF .withColumn(\u0026#34;AuthorsId\u0026#34;, (concat(expr(\u0026#34;First\u0026#34;), expr(\u0026#34;Last\u0026#34;), expr(\u0026#34;Id\u0026#34;)))) .select(col(\u0026#34;AuthorsId)) .show(4) Output: +-------------+ | AuthorsId | +-------------+ | JulesDamji1 | | BrookeWenig2| | DennyLee3 | |TathagataDas4| +-------------+\r// Những câu lệnh dưới đây cùng trả về chung giá trị: blogsDF.select(expr(\u0026#34;Hits\u0026#34;)).show(2) blogsDF.select(col(\u0026#34;Hits\u0026#34;)).show(2) blogsDF.select(\u0026#34;Hits\u0026#34;).show(2)\r// Sắp xếp giảm dần cột \u0026#34;Id\u0026#34; blogsDF.sort(col(\u0026#34;Id\u0026#34;).desc).show() blogsDF.sort($\u0026#34;Id\u0026#34;.desc).show()\rTrong ví dụ cuối, col(\u0026ldquo;Id\u0026rdquo;.desc) và $\u0026ldquo;Id\u0026rdquo;.desc đều giống nhau. Đều sắp xếp cột có tên \u0026ldquo;Id\u0026rdquo; theo thứ tự giảm dần. Trong đó:\ncol(\u0026ldquo;Id\u0026rdquo;) trả về một Column object Còn sử dụng $ trước tên cột sẽ chuyển đổi tên cột thành Column. Rows Một hàng trong Spark chứa một hay nhiều cột. Bạn có thể khởi tạo một Row trong Spark hay truy cập vào các field của nó bằng index bắt đầu từ 0:\n// In Scala import org.apache.spark.sql.Row // Tạo một Row val blogRow = Row(6, \u0026#34;Reynold\u0026#34;, \u0026#34;Xin\u0026#34;, \u0026#34;https://tinyurl.6\u0026#34;, 255568, \u0026#34;3/2/2015\u0026#34;, Array(\u0026#34;twitter\u0026#34;, \u0026#34;LinkedIn\u0026#34;)) // Truy cập bằng index scala\u0026gt; blogRow(1) res: Any = Reynold\r# In Python from pyspark.sql import Row blog_row = Row(6, \u0026#34;Reynold\u0026#34;, \u0026#34;Xin\u0026#34;, \u0026#34;https://tinyurl.6\u0026#34;, 255568, \u0026#34;3/2/2015\u0026#34;,[\u0026#34;twitter\u0026#34;, \u0026#34;LinkedIn\u0026#34;]) # Truy cập bằng index blog_row[1] \u0026gt; \u0026#39;Reynold\u0026#39;\rRow object có thể sử dụng để tạo DataFrame:\n# In Python rows = [Row(\u0026#34;Matei Zaharia\u0026#34;, \u0026#34;CA\u0026#34;), Row(\u0026#34;Reynold Xin\u0026#34;, \u0026#34;CA\u0026#34;)] authors_df = spark.createDataFrame(rows, [\u0026#34;Authors\u0026#34;, \u0026#34;State\u0026#34;]) authors_df.show() +-------------+-----+ | Author |State| +-------------+-----+ |Matei Zaharia| CA | | Reynold Xin | CA | +-------------+-----+\r","date":"2023-09-07","id":9,"permalink":"/de-docs/spark/dataframe-api/","summary":"\u003cstyle\u003ebody {text-align: justify}\u003c/style\u003e\r\n\u003cp\u003eSpark DataFrame được lấy cảm hứng từ pandas DataFrame về cấu trúc, định dạng và một số thao tác cụ thể. Ví dụ về một Spark DataFrame được hiển thị tại Hình 1.\u003c/p\u003e","tags":[],"title":"5. DataFrame API"},{"content":"\rDataFrameReader và DataFrame Writer DataFrameReader cho phép bạn đọc data từ nhiều nguồn khác nhau vào DataFrame, ví dụ JSON, CSV, Parquet, Text, Arvo, ORC, \u0026hellip; DataFrameWriter cho phép bạn ghi DataFrame vào nguồn data dưới một định dạng cụ thể. Cùng bắt đầu với một ví dụ, chúng ta sẽ đọc một file CSV lớn chứa dữ liệu từ San Franciso Fire Department calls. Bao gồm 28 cột và 4,380,660 records. Dữ liệu bạn có thể tải tại đây. Với một lượng dữ liệu lớn thì việc tự định nghĩa schema mang lại hiệu quả tốt hơn so với Spark tự xác định schema. Cùng xem cách để làm ví dụ này:\n# In Python from pyspark.sql import SparkSession from pyspark.sql.types import * # Định nghĩa schema theo cách lập trình fire_schema = StructType([StructField(\u0026#39;CallNumber\u0026#39;, IntegerType(), True), StructField(\u0026#39;UnitID\u0026#39;, StringType(), True), StructField(\u0026#39;IncidentNumber\u0026#39;, IntegerType(), True), StructField(\u0026#39;CallType\u0026#39;, StringType(), True), StructField(\u0026#39;CallDate\u0026#39;, StringType(), True), StructField(\u0026#39;WatchDate\u0026#39;, StringType(), True), StructField(\u0026#39;CallFinalDisposition\u0026#39;, StringType(), True), StructField(\u0026#39;AvailableDtTm\u0026#39;, StringType(), True), StructField(\u0026#39;Address\u0026#39;, StringType(), True), StructField(\u0026#39;City\u0026#39;, StringType(), True), StructField(\u0026#39;Zipcode\u0026#39;, IntegerType(), True), StructField(\u0026#39;Battalion\u0026#39;, StringType(), True), StructField(\u0026#39;StationArea\u0026#39;, StringType(), True), StructField(\u0026#39;Box\u0026#39;, StringType(), True), StructField(\u0026#39;OriginalPriority\u0026#39;, StringType(), True), StructField(\u0026#39;Priority\u0026#39;, StringType(), True), StructField(\u0026#39;FinalPriority\u0026#39;, IntegerType(), True), StructField(\u0026#39;ALSUnit\u0026#39;, BooleanType(), True), StructField(\u0026#39;CallTypeGroup\u0026#39;, StringType(), True), StructField(\u0026#39;NumAlarms\u0026#39;, IntegerType(), True), StructField(\u0026#39;UnitType\u0026#39;, StringType(), True), StructField(\u0026#39;UnitSequenceInCallDispatch\u0026#39;, IntegerType(), True), StructField(\u0026#39;FirePreventionDistrict\u0026#39;, StringType(), True), StructField(\u0026#39;SupervisorDistrict\u0026#39;, StringType(), True), StructField(\u0026#39;Neighborhood\u0026#39;, StringType(), True), StructField(\u0026#39;Location\u0026#39;, StringType(), True), StructField(\u0026#39;RowID\u0026#39;, StringType(), True), StructField(\u0026#39;Delay\u0026#39;, FloatType(), True)]) # Khởi tạo Spark Session spark = (SparkSession .builder .appName(\u0026#39;Common_DataFrame_Operations\u0026#39;) .getOrCreate()) # Sử dụng DataFrameReader để đọc CSV file. sf_fire_file = \u0026#34;/databricks-datasets/learning-spark-v2/sf-fire/sf-fire-calls.csv\u0026#34; fire_df = spark.read.csv(sf_fire_file, header=True, schema=fire_schema)\rCâu lệnh spark.read.csv() đọc CSV file và trả về DataFrame với các hàng và tên cột được định nghĩa từ schema đã làm. Bây giờ, chúng ta sẽ cùng ghi dữ liệu vào nguồn data với định dạng tùy bạn chọn với DataFrameWriter. Parquet là một định dạng kiểu columnar-format khá phổ biến, sử dụng linh hoạt để nén dữ liệu, schema sẽ được lưu như metadata trong file Parquet này, vì vậy khi sử dụng Spark sẽ không cần thủ công định nghĩa schema. Lưu DataFrame dưới dạng Parquet hoặc bảng SQL.\n# In Python và cũng tương tự với Scala parquetPath = ... fire_df.write.format(\u0026#34;parquet\u0026#34;).save(parquetPath)\rHoặc bạn có thể lưu dưới dạng bảng, sẽ được đăng ký metadata với Hive metastore (Sẽ được tìm hiểu sâu hơn vào phần sau).\n# In Python parquet_table = ... # Tên table fire_df.write.format(\u0026#34;parquet\u0026#34;).saveAsTable(parquet_table)\rTransformations và Actions 1. Truy vấn dữ liệu Trong Spark, sử dụng select() để truy vấn, đồng thời có thể sử dụng filter() hoặc where() để thực hiện các điều kiện truy vấn. Ví dụ:\n# In Python few_fire_df = (fire_df .select(\u0026#34;IncidentNumber\u0026#34;, \u0026#34;AvaiableDtTm\u0026#34;, \u0026#34;CallType\u0026#34;) .where(col(\u0026#34;CallType\u0026#34;) != \u0026#34;Medical Incident\u0026#34;)) few_fire_df.show(5, truncate=False) Output: +--------------+----------------------+--------------+ |IncidentNumber|AvailableDtTm |CallType | +--------------+----------------------+--------------+ |2003235 |01/11/2002 01:47:00 AM|Structure Fire| |2003235 |01/11/2002 01:51:54 AM|Structure Fire| |2003235 |01/11/2002 01:47:00 AM|Structure Fire| |2003235 |01/11/2002 01:47:00 AM|Structure Fire| |2003235 |01/11/2002 01:51:17 AM|Structure Fire| +--------------+----------------------+--------------+ only showing top 5 rows\rMột số lệnh khác như isNotNull(), countDistinct(), distinct().\n# In Python, trả về số lượng các loại cuộc gọi (Call of types) khác nhau sử dụng countDistinct() from pyspark.sql.functions import * (fire_df .select(\u0026#34;CallType\u0026#34;) .where(col(CallType).isNotNull()) .agg(countDistinct(\u0026#34;CallType\u0026#34;).alias(\u0026#34;DistinctCallTypes\u0026#34;)) .show()) Output: +-----------------+ |DistinctCallTypes| +-----------------+ | 32| +-----------------+\rChúng ta có thể hiện các loại cuộc gọi khác nhau sử dụng câu lệnh sau:\n# In Python (fire_df .select(\u0026#34;CallType\u0026#34;) .where(col(\u0026#34;CallType\u0026#34;).isNotNull()) .distinct() .show(10, False)) Output: 32 +-----------------------------------+ |CallType | +-----------------------------------+ |Elevator / Escalator Rescue | |Marine Fire | |Aircraft Emergency | |Confined Space / Structure Collapse| |Administrative | |Alarms | |Odor (Strange / Unknown) | |Lightning Strike (Investigation) | |Citizen Assist / Service Call | |HazMat | +-----------------------------------+ only showing top 10 rows\r2. Đổi tên, thêm và xóa cột Để đổi tên cột, ta có thể sử dụng withColumnRenamed().\n# In Python new_fire_df = fire_df.withColumnRenamed(\u0026#34;Delay\u0026#34;, \u0026#34;ResponseDelayedInMins\u0026#34;)\rTrong nhiều trường hợp, dữ liệu raw và không thể sử dụng được khi phân tích như dữ liệu ngày tháng là String thay vì Timestamp. Vì vậy, chúng ta có thể thay đổi bằng cách thêm cột mới với dữ liệu mong muốn và xóa đi cột. Thật may vì trong Spark cũng hỗ trợ việc này với withColumn() để có thể thêm cột, drop() để xóa cột và to_timestamp() hay to_date() để chuyển từ kiểu dữ liệu string sang date hay timestamp như sau:\n# In Python fire_ts_df = (new_fire_df .withColumn(\u0026#34;IncidentDate\u0026#34;, to_timestamp(col(\u0026#34;CallDate\u0026#34;), \u0026#34;MM/dd/yyyy\u0026#34;)) .drop(\u0026#34;CallDate\u0026#34;) .withColumn(\u0026#34;OnWatchDate\u0026#34;), to_timestamp(col(\u0026#34;WatchDate\u0026#34;), \u0026#34;MM/dd/yyyy\u0026#34;) .drop(\u0026#34;WatchDate\u0026#34;) .withColumn(\u0026#34;AvailableDtTS\u0026#34;), to_timestamp(col(\u0026#34;AvailableDtTm\u0026#34;), \u0026#34;MM/dd/yyyy hh:mm:ss a\u0026#34;) .drop(\u0026#34;AvailableDtTm\u0026#34;))\rSpark cũng hỗ trợ nhiều phương thức xử lý dữ liệu từ spark.sql.functions như month(), year() và day().\n3. Tổng hợp dữ liệu Một số phương thức thực hiện tổng hợp dữ liệu như: groupBy(), orderBy(), count(). Cùng thực hiện với một câu hỏi: Đâu là loại cuộc gọi cứu hóa nhiều nhất?\n# In Python (fire_ts_df .select(\u0026#34;CallType\u0026#34;) .where(col(\u0026#34;CallType\u0026#34;).isNotNull()) .groupBy(\u0026#34;CallType\u0026#34;) .count() .orderBy(\u0026#34;count\u0026#34;, ascending=False) .show(n=10, truncate=False)) Output: +-------------------------------+-------+ |CallType |count | +-------------------------------+-------+ |Medical Incident |2843475| |Structure Fire |578998 | |Alarms |483518 | |Traffic Collision |175507 | |Citizen Assist / Service Call |65360 | |Other |56961 | |Outside Fire |51603 | |Vehicle Fire |20939 | |Water Rescue |20037 | |Gas Leak (Natural and LP Gases)|17284 | +-------------------------------+-------+\rVậy câu trả lời là: Medical Incident.\n4. Một số thao tác với DataFrame khác DataFrame API cũng cung cấp một số phương thức thống kê như: min(), max(), sum() và avg(). Một cách import Pythonic để không gây xung đột giữa các built-in Python function như sau:\n# In Python import pyspark.sql.functions as F (fire_ts_df .select(F.sum(\u0026#34;NumAlarms\u0026#34;), F.avg(\u0026#34;ResponseDelayedInMins\u0026#34;), F.min(\u0026#34;ResponseDelayedInMins\u0026#34;), F.max(\u0026#34;ResponseDelayedInMins\u0026#34;)) .show()) Output: +--------------+--------------------------+--------------------------+---------+ |sum(NumAlarms)|avg(ResponseDelayedInMins)|min(ResponseDelayedInMins)|max(...) | +--------------+--------------------------+--------------------------+---------+ | 4403441| 3.902170335891614| 0.016666668|1879.6167| +--------------+--------------------------+--------------------------+---------+\rCác phương thức thống kê nâng cao thường gặp cho công việc khoa học dữ liệu như: stat(), describe(), correlation(), covariance(), sampleBy(), approxQuantile(), frequentItems(), \u0026hellip;\n","date":"2023-09-07","id":10,"permalink":"/de-docs/spark/common-dataframe-operation/","summary":"\u003cstyle\u003ebody {text-align: justify}\u003c/style\u003e\r\n\u003ch3 id=\"dataframereader-và-dataframe-writer\"\u003eDataFrameReader và DataFrame Writer\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eDataFrameReader\u003c/strong\u003e cho phép bạn đọc data từ nhiều nguồn khác nhau vào DataFrame, ví dụ JSON, CSV, Parquet, Text, Arvo, ORC, \u0026hellip;\n\u003cbr\u003e\n\u003cstrong\u003eDataFrameWriter\u003c/strong\u003e cho phép bạn ghi DataFrame vào nguồn data dưới một định dạng cụ thể.\n\u003cbr\u003e\n\u003cbr\u003e\nCùng bắt đầu với một ví dụ, chúng ta sẽ đọc một file CSV lớn chứa dữ liệu từ San Franciso Fire Department calls. Bao gồm 28 cột và 4,380,660 records.\nDữ liệu bạn có thể tải \u003ca href=\"https://github.com/databricks/LearningSparkV2/tree/master\"\u003etại đây\u003c/a\u003e. Với một lượng dữ liệu lớn thì việc tự định nghĩa schema mang lại hiệu quả tốt hơn so với Spark tự xác định schema.\n\u003cbr\u003e\n\u003cbr\u003e\nCùng xem cách để làm ví dụ này:\u003c/p\u003e","tags":[],"title":"6. Một số thao tác với DataFrame"},{"content":"\rĐề bài Hãy sử dụng data set San Francisco Fire Department và sử dụng Spark để thực hiện các yêu cầu sau:\nWhat were all the different types of fire call in 2018? What months within the year 2018 saw the highest number of fire calls? Which neighborhoods had the worst response times to ffire calls in 2018? Which week in the year in 2018 had the most fire calls? Is there a correlation between neighborhood, zip code, and number of fire calls? How can we use Parquet file or SQL tables to store this data and read it back? Lời giải Khuyến khích thực hiện những bài tập trên trước khi đọc lời giải, hãy chỉ lấy lời giải làm tham khảo, nếu chỉ đọc lời giải mà không thực hành sẽ không thể nhớ lâu được. Lời giải của sẽ được làm trên ngôn ngữ Python thông qua thư viện PySpark.\n","date":"2023-09-07","id":11,"permalink":"/de-docs/spark/bai-tap-lan-mot/","summary":"\u003cstyle\u003ebody {text-align: justify}\u003c/style\u003e\r\n\u003ch3 id=\"đề-bài\"\u003eĐề bài\u003c/h3\u003e\n\u003cp\u003eHãy sử dụng data set San Francisco Fire Department và sử dụng Spark để thực hiện các yêu cầu sau:\u003c/p\u003e","tags":[],"title":"7. Bài tập DataFrame lần 1"},{"content":"","date":"2023-09-07","id":12,"permalink":"/de-docs/overview/loi-mo-dau/","summary":"\u003cstyle\u003ebody {text-align: justify}\u003c/style\u003e","tags":[],"title":"Lời mở đầu"},{"content":"\rGiới thiệu bản thân Xin chào mình tên là Lương Tiến Dũng. Là sinh viên công nghệ thông tin năm thứ 2 tại trường đại học Giao thông Vận tải. Đang theo đuổi con đường trở thành một Data Engineer.\n","date":"2023-09-07","id":13,"permalink":"/docs/overview/l%E1%BB%9Di-m%E1%BB%9F-%C4%91%E1%BA%A7u/","summary":"\u003cstyle\u003ebody {text-align: justify}\u003c/style\u003e\r\n\u003ch4 id=\"giới-thiệu-bản-thân\"\u003eGiới thiệu bản thân\u003c/h4\u003e\n\u003cp\u003eXin chào mình tên là Lương Tiến Dũng. Là sinh viên công nghệ thông tin năm thứ 2 tại trường đại học Giao thông Vận tải.\nĐang theo đuổi con đường trở thành một Data Engineer.\u003c/p\u003e","tags":[],"title":"Lời mở đầu"},{"content":"","date":"2023-09-07","id":14,"permalink":"/docs/ly-thuyet/","summary":"","tags":[],"title":"Lý thuyết"},{"content":"","date":"2023-09-07","id":15,"permalink":"/docs/thuc-hanh/","summary":"","tags":[],"title":"Thực hành"},{"content":"\rĐiện toán đám mây là gì? Điện toán đám mây là việc phân phối các tài nguyên CNTT theo nhu cầu qua internet với chính sách thanh toán theo mức sử dụng.\nLợi ích của điện toán đám mây Sử dụng bao nhiêu tính tiền bấy nhiêu, cung cấp khả năng tối ưu hóa chi phí Tăng tốc độ phát triển, nhờ tận dụng các tính năng tự động hóa và quản trị bởi nhà cung cấp dịch vụ Linh hoạt, thêm bớt tài nguyên tùy ý. Mở rộng quy mô ứng dụng lên toàn cầu AWS, điều gì tạo nên sự khác biệt? AWS là nhà cung cấp hạ tầng Cloud dẫn đầu trong 13 năm liên tiếp. [Tính tới hết 2023] AWS là nhà cung cấp khác biệt về tầm nhìn và văn hóa. Triết lý về giá của AWS: Khách hàng sẽ càng ngày càng trả ít tiền hơn cho cùng dịch vụ / tính năng / tài nguyên sử dụng. AWS đưa việc mang lại giá trị thực sự cho khách hàng lên hàng đầu trong tất cả các nguyên tắc lãnh đạo (Leadership Principle) của mình Bắt đầu hành trình lên mây như thế nào? Là nhà cung cấp điện toán đám mây hàng đầu, số lượng khóa học nhiều nhất, có chiều sâu nhất trong tất cả các nhà cung cấp dịch vụ Hoàn toàn có thể tự học để trỏ thành chuyên gia Hãy kết bạn và cùng nhau học hỏi. Đăng ký tài khoản AWS càng sớm càng tốt và trải nghiệm thông qua Free Tier ","date":"2023-09-07","id":16,"permalink":"/docs/ly-thuyet/1.-%C4%91i%E1%BB%87n-to%C3%A1n-%C4%91%C3%A1m-m%C3%A2y/","summary":"\u003cstyle\u003ebody {text-align: justify}\u003c/style\u003e\r\n\u003ch3 id=\"điện-toán-đám-mây-là-gì\"\u003eĐiện toán đám mây là gì?\u003c/h3\u003e\n\u003cp\u003eĐiện toán đám mây là việc phân phối các tài nguyên CNTT theo nhu cầu qua internet\nvới \u003cspan style=\"color: orange; font-weight:bold;\"\u003echính sách thanh toán theo mức sử dụng.\u003c/span\u003e\u003c/p\u003e","tags":[],"title":"1. Điện toán đám mây"},{"content":"\rTrung tâm dữ liệu của AWS (Data Center) Một trung tâm dữ liệu có thể chứa hàng chục ngàn máy chủ.\nTất cả trung tâm dữ liệu của AWS đều sử dụng các thiết bị được tối ưu hóa dành riêng cho hoạt động của AWS\nMore Info\nAvailability Zone Một Availability Zone (AZ) bao gồm một hoặc nhiều trung tâm dữ liệu, các (AZ) được thiết kế để không xảy ra sự cố ảnh hưởng đồng thời 2 AZ một lúc (fault isolation) Giữa 2 AZ là đường kết nối riêng tốc độ cao AWS khuyến nghị nên triển khai ứng dụng tối thiểu trên 2 AZ Region Một AWS Region bao gồm tối thiểu 3 Availability Zone. Hiện tại có hơn 25 Region trên toàn cầu. Các Region được kết ối với nhau bởi mạng backbone của AWS. Mặc định dữ liệu và dịch vụ ở các Region độc ập với nhau. (Trừ một số dịch vụ quy mô Global) Edge Locations Là mạng lưới trung tâm dữ liệu AWS được thiết kế để cung cấp dịch vụ với độ trễ thấp nhất có thể. Các dịch vụ AWS hoạt động tại Edge Locations (POP) bao gồm CloudFront (CDN) Web Application Firewall (WAF) Route 53 (DNS Service) ","date":"2023-09-07","id":17,"permalink":"/docs/ly-thuyet/2.-h%E1%BA%A1-t%E1%BA%A7ng-to%C3%A0n-c%E1%BA%A7u-c%E1%BB%A7a-aws/","summary":"\u003cstyle\u003ebody {text-align: justify}\u003c/style\u003e\r\n\u003ch3 id=\"trung-tâm-dữ-liệu-của-aws-data-center\"\u003eTrung tâm dữ liệu của AWS (Data Center)\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eMột trung tâm dữ liệu có thể chứa hàng chục ngàn máy chủ.\u003c/p\u003e","tags":[],"title":"2. Hạ tầng toàn cầu của AWS"},{"content":"\rAWS Management Console - Root Login Chúng ta có thể login bằng tài khoản root hoặc bằng tài khoản IAM User (tài khoản con giúp quản lý truy xuất các tài nguyên của AWS) Link AWS Console\nAWS Management Console - IAM Login Khi login bằng IAM User chúng ta cần cung cấp thêm thông tin Account ID (Chuỗi 12 chữ số) để xác định account AWS\nAWS Management Console - Service Search Sau khi login có thể tìm kiếm và sử dụng các dịch vụ của AWS AWS Management Console - Support Center Tạo support case để yêu cầu trợ giúp\nAWS Command Line Interface (CLI) Open source, cho phép tương tác với các dịch vụ AWS bằng command. Chức năng tương đương với chức năng được cung cấp bởi AWS Management Console dựa trên trình duyệt Sử dụng Access key / Secret Access Key để sử dụng CLI. AWS SDK Đơn giản hóa việc sử dụng AWS bằng cách cung cấp một bộ thư viện nhất quán và quen thuộc cho đội ngũ phát triển ứng dụng. Sử dụng Access key / Secret Access Key để sử dụng SDK. Phát triển ứng dụng sử dụng AWS dễ dàng. ","date":"2023-09-07","id":18,"permalink":"/docs/ly-thuyet/3.-c%C3%B4ng-c%E1%BB%A5-qu%E1%BA%A3n-l%C3%BD-aws-services/","summary":"\u003cstyle\u003ebody {text-align: justify}\u003c/style\u003e\r\n\u003ch3 id=\"aws-management-console---root-login\"\u003eAWS Management Console - Root Login\u003c/h3\u003e\n\u003cp\u003eChúng ta có thể login bằng tài khoản root hoặc bằng tài khoản IAM User (tài khoản con giúp quản lý truy xuất các tài nguyên của AWS)\n\u003cbr\u003e \u003ca href=\"https://aws.amazon.com/console/\"\u003eLink AWS Console\u003c/a\u003e\u003c/p\u003e","tags":[],"title":"3. Công cụ quản lý AWS Services"},{"content":"\rTối ưu hóa chi phí trên AWS Lựa chọn cấu hình tài nguyên tính toán và nơi lưu trữ dữ liệu phù hợp. Tận dụng các phương thức thanh toán giảm giá như reserved instance , saving plan, spot. Xoá các tài nguyên không sử dụng, bật tắt tự động các tài nguyên không cần chạy 24/7. Tận dụng các dịch vụ serverless. Thiết kế kiến trúc tối ưu Cài đặt và sử dụng AWS Budget. Quản lý chi phí theo phòng ban / ứng dụng với cost allocation tag. Liên tục theo dõi và tối ưu hóa chi phí. Công cụ tính toán chi phí https://calculator.aws/#/\nCho phép tạo các estimate các dịch vụ thông dụng. Có thể chia sẻ ca estimate cho nguười khác. Chi phí sẽ khác biệt theo từng Region Làm việc với AWS Support AWS có 4 gói hỗ trợ chính: Basic (Explore) Developer (Dev/Test) Business (Production) Enterprise (Large Enterprise) Có thể nâng cấp gói hỗ trợ trong thời gian ngắn để đẩy nhanh tốc độ hỗ trợ khi có sự cố quan trọng cần xử lý nhanh. ","date":"2023-09-07","id":19,"permalink":"/docs/ly-thuyet/4.-t%E1%BB%91i-%C6%B0u-h%C3%B3a-chi-ph%C3%AD-tr%C3%AAn-aws-v%C3%A0-l%C3%A0m-vi%E1%BB%87c-v%E1%BB%9Bi-aws-support/","summary":"\u003cstyle\u003ebody {text-align: justify}\u003c/style\u003e\r\n\u003ch3 id=\"tối-ưu-hóa-chi-phí-trên-aws\"\u003eTối ưu hóa chi phí trên AWS\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eLựa chọn cấu hình tài nguyên tính toán và nơi lưu trữ dữ liệu phù hợp.\u003c/li\u003e\n\u003cli\u003eTận dụng các phương thức thanh toán giảm giá như \u003cspan style=\"color: orange; font-weight:bold;\"\u003ereserved instance\u003c/span\u003e\n, \u003cspan style=\"color: orange; font-weight:bold;\"\u003esaving plan\u003c/span\u003e, \u003cspan style=\"color: orange; font-weight:bold;\"\u003espot\u003c/span\u003e.\u003c/li\u003e\n\u003cli\u003eXoá các tài nguyên không sử dụng, bật tắt tự động các tài nguyên không cần chạy 24/7.\u003c/li\u003e\n\u003cli\u003eTận dụng các dịch vụ \u003cspan style=\"color: orange; font-weight:bold;\"\u003eserverless\u003c/span\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"thiết-kế-kiến-trúc-tối-ưu\"\u003eThiết kế kiến trúc tối ưu\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eCài đặt và sử dụng \u003cspan style=\"color: orange; font-weight:bold;\"\u003eAWS Budget\u003c/span\u003e.\u003c/li\u003e\n\u003cli\u003eQuản lý chi phí theo phòng ban / ứng dụng với \u003cspan style=\"color: orange; font-weight:bold;\"\u003ecost allocation tag\u003c/span\u003e.\u003c/li\u003e\n\u003cli\u003e\u003cspan style=\"color: orange; font-weight:bold;\"\u003eLiên tục\u003c/span\u003e theo dõi và tối ưu hóa chi phí.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"công-cụ-tính-toán-chi-phí\"\u003eCông cụ tính toán chi phí\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://calculator.aws/#/\"\u003ehttps://calculator.aws/#/\u003c/a\u003e\u003c/p\u003e","tags":[],"title":"4. Tối ưu hóa chi phí trên AWS và Làm việc với AWS Support"},{"content":"\rAmazon Virtual Private Cloud (Amazon VPC) cho phép bạn khởi chạy các tài nguyên AWS vào một mạng ảo mà bạn đã xác định. VPC nằm trong 1 Region, khi tạo VPC cần khai báo 1 lớp mạng CIDR IPv4 (bắt buộc) và IPv6 (optional) Giới hạn của VPC hiện tại là 5 VPC trên 1 AWS Region trên 1 AWS Account. Mục đích chính sử dụng VPC thường dùng để phân tách các môi trường (Production / Dev / Test / Staging) Lưu ý: Nếu muốn các tài nguyên tách biệt hẳn (User không thể nhìn thầy một tài nguyên cụ thể thì cần tách thành nhiều AWS Account, nhiều VPC không giải quyết được vấn đề này) Amazon Virtual Private Cloud (VPC) - Subnet Amazon VPC cho phép tạo nhiều mạng ảo và chia các mạng ảo này thành các mạng con (subnet) VPC Subnet sẽ năm trong 1 Availability Zone cụ thể. Khi tạo Subnet, chúng ta chỉ định CIDR cho mạng con đó và đây là một tập hợp con của khối VPC CIDR Trong mỗi Subnet, AWS sẽ giữ 5 địa chỉ IP. Ví dụ nếu Subnet có CIDR là 10.10.1.0/24 Địa chỉ netwwork (10.10.1.0) Địa chỉ broadcast (10.10.1.255) Địa chỉ cho bộ định tuyến (10.10.1.1) Địa chỉ cho DNS (10.10.1.2) Địa chỉ cho tính năng tương lai (10.10.1.3) Amazon Virtual Private Cloud (VPC) - Route Table Route Table (Bảng định tuyến), tập hợp các Route, để xác định đường đi cho mạng. Khi tạo VPC, AWS sẽ tạo một Default Rote table, không thể bị xóa và chỉ chứa 1 Route duy nhất là Route cho phép tất cả các Subnet trong VPC liên lạc với nhau. Route table sẽ được gắn vào Subnet Chúng ta có thể tạo Custom Route table, tuy nhiên sẽ không thể xóa default route (VPC CIDR - local). Amazon Virtual Private Cloud (VPC) - Elastic Network Interface (ENI) Elastic Network Interface (ENI) là một card mạng ảo, chúng ta có thể chuyển sang các EC2 Instance khác. Khi chuyển sang một máy chủ mới, một card mạng ảo sẽ vẫn duy trì: Địa chỉ IP Private Địa chỉ Elastic address Địa chỉ MAC Amazon Virtual Private Cloud (VPC) - Elastic IP address (EIP) Elastic IP address (EIP) là một địa chỉ public IPv4 tĩnh, có thể liên kết với một Elastic Network Interface. Khi không sử dụng, sẽ bị charge phí (tránh lãng phí) Khi tạo một EC2 Instance, sẽ tạo ra một Elastic Network Interface bao gồm\nVPC Private IP, Địa chỉ Private IPv4 trong subnet CIDR. Ví dụ trong ảnh Public Subnet với dải CIDR 10.10.1.0/24 thì EC2 sẽ được gán trong khoảng từ 10.10.1.4 -\u0026gt; 10.10.1.254. Trường hợp trong ảnh là 10.10.1.6 Elastic Ip Address, Địa chỉ Public IPv4 tĩnh, không thay đổi khi máy ảo restart. Trong ảnh là 134.23.42.15 Amazon Virtual Private Cloud (VPC) - VPC Endpoint VPC Endpoint cho phép chúng ta kết nối các tài nguyên nằm trong VPC tới các dịch vụ AWS được hỗ trợ (AWS PrivateLink - đi qua mạng private của AWS) mà không cần thông qua kết nối internet. Có 2 kiểu VPC Endpoint: Interface Endpoint: Sử dụng một Elastic Network Interface trong VPC cùng với một địa chỉ IP Private để kết nối tới một dịch vụ hỗ trợ. Gateway Endpoint: Sử dụng một route table để định tuyến tới endpoint của dịch vụ hỗ trợ (S3 và DynamoDB) Amazon Virtual Private Cloud (VPC) - Internet Gateway Internet Gateway là một thành phần của Amazon VPC có khả năng mở rộng quy mô theo chiều nang (scale out) cho phép các EC2 Instance trong VPC có thể truyền thông tin ra ngoài Internet. Internet Gateway được quản lý bởi AWS, chúng ta không cần cấu hình autosclae hoặc high availability. Amazon Virtual Private Cloud (VPC) - NAT Gateway NAT gateway cho phép các EC2 instance trong subnet truy ập tới interrnet hoặc các dịch vụ AWS khác. Chỉ chấp nhận kết nối chiều ra và không chấp nhận kết nối chiều vào. ","date":"2023-09-07","id":20,"permalink":"/docs/ly-thuyet/5.-aws-virtual-private-cloud/","summary":"\u003cstyle\u003ebody {text-align: justify}\u003c/style\u003e\r\n\u003cul\u003e\n\u003cli\u003eAmazon Virtual Private Cloud (Amazon VPC) cho phép bạn khởi chạy các tài nguyên AWS vào một mạng ảo mà bạn đã xác định.\u003c/li\u003e\n\u003cli\u003eVPC nằm trong 1 Region, khi tạo VPC cần khai báo 1 lớp mạng CIDR IPv4 (bắt buộc) và IPv6 (optional)\u003c/li\u003e\n\u003cli\u003eGiới hạn của VPC hiện tại là 5 VPC trên 1 AWS Region trên 1 AWS Account.\u003c/li\u003e\n\u003cli\u003eMục đích chính sử dụng VPC thường dùng để \u003cspan style=\"color: orange; font-weight:bold;\"\u003ephân tách\u003c/span\u003e các môi trường\n(Production / Dev / Test / Staging)\u003c/li\u003e\n\u003cli\u003eLưu ý: Nếu muốn các tài nguyên tách biệt hẳn (User không thể nhìn thầy một tài nguyên cụ thể thì cần tách thành nhiều AWS Account, nhiều VPC không giải quyết được vấn đề này)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"amazon-virtual-private-cloud-vpc---subnet\"\u003eAmazon Virtual Private Cloud (VPC) - Subnet\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eAmazon VPC cho phép tạo nhiều mạng ảo và chia các mạng ảo này thành các mạng con (\u003cspan style=\"color: orange; font-weight:bold;\"\u003esubnet\u003c/span\u003e)\u003c/li\u003e\n\u003cli\u003eVPC Subnet sẽ năm trong 1 Availability Zone cụ thể.\u003c/li\u003e\n\u003cli\u003eKhi tạo Subnet, chúng ta chỉ định CIDR cho mạng con đó và đây là một \u003cspan style=\"color: orange; font-weight:bold;\"\u003etập hợp con của khối VPC CIDR\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003eTrong mỗi Subnet, AWS sẽ giữ 5 địa chỉ IP. Ví dụ nếu Subnet có CIDR là 10.10.1.0/24\n\u003cul\u003e\n\u003cli\u003eĐịa chỉ netwwork (10.10.1.0)\u003c/li\u003e\n\u003cli\u003eĐịa chỉ broadcast (10.10.1.255)\u003c/li\u003e\n\u003cli\u003eĐịa chỉ cho bộ định tuyến (10.10.1.1)\u003c/li\u003e\n\u003cli\u003eĐịa chỉ cho DNS (10.10.1.2)\u003c/li\u003e\n\u003cli\u003eĐịa chỉ cho tính năng tương lai (10.10.1.3)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\r\n\r\n\u003cimg\r\n  src=\"/docs/ly-thuyet/images/subnet_hu_b4e59fd829997557.webp\"\r\n  width=\"1571\"\r\n  height=\"954\"\r\n  decoding=\"async\"\r\n  fetchpriority=\"auto\"\r\n  loading=\"lazy\"\r\n  alt=\"alt\"id=\"h-rh-i-0\"\r\n/\u003e\u003c/p\u003e","tags":[],"title":"5. AWS Virtual Private Cloud"},{"content":"\rAmazon Virtual Private Cloud (VPC) - Security Group Security Group (SG) là một tường lửa ảo có lưu trữ trạng thái (stateful) giúp kiểm soát lượng truy cập đến và đi trong tài nguyên của AWS. Security Group rule được hẹn chế theo giao thức, địa chỉ nguồn, cổng kết nối, hoặc một Security Group khác. Security Group rule chỉ cho phép rule allow. Security Group được áp dụng lên các Elastic Network Interface. Amazon Virtual Private Cloud (VPC) - Network Access Control List (NACL) Network Access Control List (NACL) là một tường lửa ảo không lưu trữ trạng thái (stateless) giúp kiểm soát lượng truy cập đến và đi trong tài nguyên của AWS NACL được hạn chế theo giao thức, địa chỉ nguồn, cổng kết nối. NACL được áp dụng lên các Amazon VPC Subnets. Mặc định NACL cho phép mọi truy cập đến và đi. Amazon Virtual Private Cloud (VPC) - VPC Flow Logs VPC Flow Logs là một tính năng cho phép bạn nắm bắt thông tin về lưu lượng IP đến và đi từ các giao diện mạng trong VPC của bạn. Các tập tin logs có thể được xuất bản lên Amazon CloudWatch Logs hoặc Amazon S3. VPC Flow Logs không capture nội dung gói tin. VPC Peering VPC Peering là tính năng giúp kết nối hai hay nhiều VPC để các tài nguyên bên trong hai VPC đó có thể liên lạc trực tiếp với nhau không cần phải thông qua Internet, góp phần gia tăng tính bảo mật cho VPC. VPC Peering là kết nối cần tạo 1:1 giữa hai VPC thành viên, không hỗ trợ transitive routing. VPC Peering không hỗ trợ khi 2 VPC bị overlap IP address space. Transit Gateway Transit Gateway được dùng để kết nối các VPC và mạng on-premises thông qua một hub trung tâm. Điều này đơn giản hóa mạng và kết thúc các mối quan hệ định tuyến phức tạp. Transit Gateway Attachment là một công cụ để gán các subnet của từng VPC cần kết nối với nhau vào một TGW đã được khởi tạo. Transit Gateway Attachment hoạt động ở quy mô AZ-level. Trong VPC, khi một subnet ở một AZ có Transit Gateway Attachment với một TGW, các subnet khác trong cuùng AZ đều có thể kết nối với TGW đó. ","date":"2023-09-07","id":21,"permalink":"/docs/ly-thuyet/6.-vpc-security-and-multi-vpc-features/","summary":"\u003cstyle\u003ebody {text-align: justify}\u003c/style\u003e\r\n\u003ch3 id=\"amazon-virtual-private-cloud-vpc---security-group\"\u003eAmazon Virtual Private Cloud (VPC) - Security Group\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cspan style=\"color: orange; font-weight:bold;\"\u003eSecurity Group\u003c/span\u003e (SG) là một tường lửa ảo có lưu trữ trạng thái (stateful) giúp kiểm soát lượng truy cập đến và đi trong tài nguyên của AWS.\u003c/li\u003e\n\u003cli\u003eSecurity Group rule được hẹn chế theo giao thức, địa chỉ nguồn, cổng kết nối, hoặc một Security Group khác.\u003c/li\u003e\n\u003cli\u003eSecurity Group rule \u003cspan style=\"color: orange; font-weight:bold;\"\u003echỉ cho phép rule allow.\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003eSecurity Group được áp dụng lên các Elastic Network Interface.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"amazon-virtual-private-cloud-vpc---network-access-control-list-nacl\"\u003eAmazon Virtual Private Cloud (VPC) - Network Access Control List (NACL)\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eNetwork Access Control List (NACL) là một tường lửa ảo không lưu trữ trạng thái (stateless) giúp kiểm soát lượng truy cập đến và đi trong tài nguyên của AWS\u003c/li\u003e\n\u003cli\u003eNACL được hạn chế theo giao thức, địa chỉ nguồn, cổng kết nối.\u003c/li\u003e\n\u003cli\u003eNACL được áp dụng lên các Amazon VPC Subnets.\u003c/li\u003e\n\u003cli\u003eMặc định NACL cho phép mọi truy cập đến và đi.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"amazon-virtual-private-cloud-vpc---vpc-flow-logs\"\u003eAmazon Virtual Private Cloud (VPC) - VPC Flow Logs\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cspan style=\"color: orange; font-weight:bold;\"\u003eVPC Flow Logs\u003c/span\u003e là một tính năng cho phép bạn nắm bắt thông tin về lưu lượng IP đến và đi từ các giao diện mạng trong VPC của bạn.\u003c/li\u003e\n\u003cli\u003eCác tập tin logs có thể được xuất bản lên Amazon CloudWatch Logs hoặc Amazon S3.\u003c/li\u003e\n\u003cli\u003eVPC Flow Logs không capture nội dung gói tin.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\r\n\r\n\u003cimg\r\n  src=\"/docs/ly-thuyet/images/vpc-flowlogs_hu_dc47f7e4cc43aafa.webp\"\r\n  width=\"1696\"\r\n  height=\"698\"\r\n  decoding=\"async\"\r\n  fetchpriority=\"auto\"\r\n  loading=\"lazy\"\r\n  alt=\"alt\"id=\"h-rh-i-0\"\r\n/\u003e\u003c/p\u003e","tags":[],"title":"6. VPC Security and Multi-VPC features"},{"content":"\rVPN Site to Site VPN Site to Site dùng trong mô hình hybird để thiết lập kết nối liên tục giữa môi trường trung tâm dữ liệu truyền thống tới môi trường VPC của AWS. Việc thiết lập kết nối sẽ cần 2 đầu endpoint ở phía AWS và phía khách hàng: Virtual Private Gateway: Được quản lý hoàn toàn bởi AWS (chia 2 endpoints ở 2 đầu ra AZ). Customer Gateway: Đầu enpoint phía khách hàng, có thể là thiết bị phần cứng hoặc software appliance. VPN Client to Site VPN Client to Site: Cho phép một host truy cập tới tài nguyên trong VPC. Khuyến kích sử dụng VPN Client to Site trong AWS Market Place AWS Direct Connect AWS Direct Connect là dịch vụ cho phép tạo kết nối riêng từ trung tâm dữ liệu truyền thống tới AWS.. Độ trễ khoảng 20ms - 30ms. AWS Direct Connect ở Việt Nam hiện tại sẽ thông qua AWS Direct Connect partners và hoạt động dưới dạng Hosted Connections. (Nếu trực tiếp tới AWS là Đeicated Connections). Băng thông Direct Connect có thể thay đổi lên / xuống tùy nhu cầu. Elastic Load Balancing Elastic Load Balancing (ELB) là một dịch vụ cân bằng tải được quản lý bởi AWS, có chức năng phân phối lưu lượng cho nhiều EC2 Instance hoặc Container. Sử dụng giao thức HTTP, HTTPS, TCP và SSL (TCP bảo mật). Có thể nằm ở public hoặc private subnet. Mỗi ELB sẽ được cấp tên DNS và kt nối thông qua DNS. Chỉ có Network Load Balancer hỗ trợ gắn IP tĩnh. ELB có tính năng health check, không gửi lưu lượng đến các Instance không đạt health check. Bao gồm 4 loại: Application Load Balancer Network Load Balancer Classic Load Balancer Gateway Load Balancer Sticky session (sesion afinity): Tính năng cho phép các kết nối được gán vào một target nhất định. Việc này đảm bảo các requests từ một user trong một session sẽ được gửi tới cùng một target. Sticky session là cần thiết trong trường hợp các máy chủ ứng dụng lưu trữ thông tin trạng thái người dùng tại server. Hoạt động trên Network Load Balancer, Application Load Balancer, Classic Load Balancer ELB cung cấp tính năng lưu trữ logs truy cập (access logs) chúng ta có thể sử dụng access logs để phân tiích truy cập, trouble shoot. Log truy cập sẽ đựược lưu trữ vào một dịch vụ lưu trữ đối tượng là Amazon S3 (Simple Storage Service) Elastic Load Balancer - Application Load Balancer Application Load Balancer (ALB) là một dịch vụ cân bằng tải được quản lý bởi AWS, hoạt động ở Layer 7. Sử dụng giao thức HTTP, HTTPS. Hỗ trợ tính năng path-based routing. (/mobile /desktop sẽ được route tới 2 target group khác nhau) Cho phép route traffic tới cả target nằm ngoài VPC (IP Address), EC2, Lambda, Container (ECS, EKS). Elastic Load Balancer - Network Load Balancer Network Load Balancer (NLB) là một dịch vụ cân bằng tải được quản lý bởi AWS, hoạt động ở layer 4. Sử dụng giao thức TCP, TLS. Hỗ trợ tính năng set IP tĩnh Hỗ trợ hiệu năng cao nhất trong các loại Load Balancer có khả năng xử lý đến hàng triệu request. Cho phép route traffic tới cả target nằm ngoài VPC (IP Address), EC2, Container (ECS, EKS). Elastic Load Balancer - Classic Load Balancer Classic Load Balancer (CLB) là một dịch vụ cân bằng tải được quản lý bởi AWS, hoạt động ở Layer 4, và Layer 7 Sử dụng giao thức HTTP, HTTPS, TCP, TLS Chi phí cao hơn so với ALB và NLB. Ít tính năng cao cấp hơn ALB và NLB, hiện tại rất ít được sử dụng. Cho phép route traffic tơởi EC2. Elastic Load Balancer - Gateway Load Balancer Gateway Load Balancer (GLB) là một dịch vụ cân bằng tải được quản lý bởi AWS, hoạt động ở Layer 3. Gateway Load Balancer lắng nghe toàn bộ IP packets và forward tới target group được chỉ định. Sử dụng GENEVE protocol trên port 6081. Cho phép route traffic tới các virtual appliance được AWS hỗ trợ. Danh sách các vendor hỗ trợ: https://aws.amazon.com/vi/elasticloadbalancing/partners/ ","date":"2023-09-07","id":22,"permalink":"/docs/ly-thuyet/7.-vpn-direct-connect-loadbalancer-extraresources/","summary":"\u003cstyle\u003ebody {text-align: justify}\u003c/style\u003e\r\n\u003ch3 id=\"vpn-site-to-site\"\u003eVPN Site to Site\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cspan style=\"color: orange; font-weight:bold;\"\u003eVPN Site to Site\u003c/span\u003e dùng trong mô hình hybird để thiết lập kết nối liên tục giữa môi trường trung tâm dữ liệu truyền thống tới môi trường VPC của AWS. Việc thiết lập kết nối sẽ cần 2 đầu endpoint ở phía AWS và phía khách hàng:\n\u003cul\u003e\n\u003cli\u003e\u003cspan style=\"color: orange; font-weight:bold;\"\u003eVirtual Private Gateway\u003c/span\u003e: Được quản lý hoàn toàn bởi AWS (chia 2 endpoints ở 2 đầu ra AZ).\u003c/li\u003e\n\u003cli\u003e\u003cspan style=\"color: orange; font-weight:bold;\"\u003eCustomer Gateway\u003c/span\u003e: Đầu enpoint phía khách hàng, có thể là thiết bị phần cứng hoặc software appliance.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"vpn-client-to-site\"\u003eVPN Client to Site\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eVPN Client to Site: Cho phép một host truy cập tới tài nguyên trong VPC.\u003c/li\u003e\n\u003cli\u003eKhuyến kích sử dụng VPN Client to Site trong AWS Market Place\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"aws-direct-connect\"\u003eAWS Direct Connect\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cspan style=\"color: orange; font-weight:bold;\"\u003eAWS Direct Connect\u003c/span\u003e là dịch vụ cho phép tạo kết nối riêng từ trung tâm dữ liệu truyền thống tới AWS..\u003c/li\u003e\n\u003cli\u003eĐộ trễ khoảng 20ms - 30ms.\u003c/li\u003e\n\u003cli\u003eAWS Direct Connect ở Việt Nam hiện tại sẽ thông qua AWS Direct Connect partners và hoạt động dưới dạng Hosted Connections. (Nếu trực tiếp tới AWS là Đeicated Connections).\n\u003cul\u003e\n\u003cli\u003eBăng thông Direct Connect có thể thay đổi lên / xuống tùy nhu cầu.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"elastic-load-balancing\"\u003eElastic Load Balancing\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eElastic Load Balancing (ELB) là một dịch vụ cân bằng tải được quản lý bởi AWS, có chức năng phân phối lưu lượng cho nhiều EC2 Instance hoặc Container.\u003c/li\u003e\n\u003cli\u003eSử dụng giao thức HTTP, HTTPS, TCP và SSL (TCP bảo mật).\u003c/li\u003e\n\u003cli\u003eCó thể nằm ở public hoặc private subnet.\u003c/li\u003e\n\u003cli\u003eMỗi ELB sẽ được cấp tên DNS và kt nối thông qua DNS. Chỉ có Network Load Balancer hỗ trợ gắn IP tĩnh.\u003c/li\u003e\n\u003cli\u003eELB có tính năng health check, không gửi lưu lượng đến các Instance không đạt health check.\u003c/li\u003e\n\u003cli\u003eBao gồm 4 loại:\n\u003cul\u003e\n\u003cli\u003eApplication Load Balancer\u003c/li\u003e\n\u003cli\u003eNetwork Load Balancer\u003c/li\u003e\n\u003cli\u003eClassic Load Balancer\u003c/li\u003e\n\u003cli\u003eGateway Load Balancer\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cspan style=\"color: orange; font-weight:bold;\"\u003eSticky session (sesion afinity)\u003c/span\u003e: Tính năng cho phép các kết nối được gán vào một target nhất định. Việc này đảm bảo các requests từ một user trong một session sẽ được gửi tới cùng một target.\nSticky session là cần thiết trong trường hợp các máy chủ ứng dụng lưu trữ thông tin trạng thái người dùng tại server.\n\u003cul\u003e\n\u003cli\u003eHoạt động trên Network Load Balancer, Application Load Balancer, Classic Load Balancer\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eELB cung cấp tính năng lưu trữ logs truy cập (access logs) chúng ta có thể sử dụng access logs để phân tiích truy cập, trouble shoot. Log truy cập sẽ đựược lưu trữ vào một dịch vụ lưu trữ đối tượng là Amazon S3 (Simple Storage Service)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"elastic-load-balancer---application-load-balancer\"\u003eElastic Load Balancer - Application Load Balancer\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cspan style=\"color: orange; font-weight:bold;\"\u003eApplication Load Balancer (ALB)\u003c/span\u003e là một dịch vụ cân bằng tải được quản lý bởi AWS, hoạt động ở Layer 7.\u003c/li\u003e\n\u003cli\u003eSử dụng giao thức HTTP, HTTPS.\u003c/li\u003e\n\u003cli\u003eHỗ trợ tính năng \u003cspan style=\"color: orange; font-weight:bold;\"\u003epath-based routing.\u003c/span\u003e (/mobile /desktop sẽ được route tới 2 target group khác nhau)\u003c/li\u003e\n\u003cli\u003eCho phép route traffic tới cả target nằm ngoài VPC (IP Address), EC2, Lambda, Container (ECS, EKS).\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"elastic-load-balancer---network-load-balancer\"\u003eElastic Load Balancer - Network Load Balancer\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cspan style=\"color: orange; font-weight:bold;\"\u003eNetwork Load Balancer (NLB)\u003c/span\u003e là một dịch vụ cân bằng tải được quản lý bởi AWS, hoạt động ở layer 4.\u003c/li\u003e\n\u003cli\u003eSử dụng giao thức TCP, TLS.\u003c/li\u003e\n\u003cli\u003e\u003cspan style=\"color: orange; font-weight:bold;\"\u003eHỗ trợ tính năng set IP tĩnh\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003eHỗ trợ \u003cspan style=\"color: orange; font-weight:bold;\"\u003ehiệu năng cao nhất trong các loại Load Balancer\u003c/span\u003e có khả năng xử lý đến hàng triệu request.\u003c/li\u003e\n\u003cli\u003eCho phép route traffic tới cả target nằm ngoài VPC (IP Address), EC2, Container (ECS, EKS).\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"elastic-load-balancer---classic-load-balancer\"\u003eElastic Load Balancer - Classic Load Balancer\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cspan style=\"color: orange; font-weight:bold;\"\u003eClassic Load Balancer (CLB)\u003c/span\u003e là một dịch vụ cân bằng tải được quản lý bởi AWS, hoạt động ở Layer 4, và Layer 7\n\u003cul\u003e\n\u003cli\u003eSử dụng giao thức HTTP, HTTPS, TCP, TLS\u003c/li\u003e\n\u003cli\u003eChi phí cao hơn so với ALB và NLB.\u003c/li\u003e\n\u003cli\u003eÍt tính năng cao cấp hơn ALB và NLB, hiện tại rất ít được sử dụng.\u003c/li\u003e\n\u003cli\u003eCho phép route traffic tơởi EC2.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"elastic-load-balancer---gateway-load-balancer\"\u003eElastic Load Balancer - Gateway Load Balancer\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cspan style=\"color: orange; font-weight:bold;\"\u003eGateway Load Balancer (GLB)\u003c/span\u003e là một dịch vụ cân bằng tải được quản lý bởi AWS, hoạt động ở Layer 3. Gateway Load Balancer lắng nghe toàn bộ IP packets và forward tới target group được chỉ định.\u003c/li\u003e\n\u003cli\u003eSử dụng GENEVE protocol trên port 6081.\u003c/li\u003e\n\u003cli\u003eCho phép route traffic tới các virtual appliance được AWS hỗ trợ.\u003c/li\u003e\n\u003cli\u003eDanh sách các vendor hỗ trợ:\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://aws.amazon.com/vi/elasticloadbalancing/partners/\"\u003ehttps://aws.amazon.com/vi/elasticloadbalancing/partners/\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e","tags":[],"title":"7. VPN - Direct Connect - LoadBalancer - ExtraResources"},{"content":"\rAmazon EC2 giống với máy chủ áo hoặc máy chủ vật lý truyền thống.\nCó khả năng:\nkhởi tạo nhanh khả năng co dãn tài nguyên mạnh mẽ linh hoạt Amazon EC2 có thể hỗ trợ workload như lưu trữ web, ứng dụng, cơ sở dử liệu và bất cứ công việc nào khác mà máy chủ thông thường có thể đáp ứng.\nCấu hình của Amazon EC2 không được tùy chọn tùy ý, mà lựa chọn thông qua các EC2 Instance Type sẽ quyết định:\nCPU (Intel / AMD / ARM (Graviton 1/2/3) / GPU ) Memory Network Storage AMI / Backup / Key Pair Sử dụng AMI (Amazon Machine Image) có thể provision ra một hoặc nhiều EC2 Instances cùng lúc. AMI có sẵn của AWS, trên AWS Market Place và custom AMI tự tạo từ EC2 AMI bao gồm root OS volumes, quyền sử dụng AMI quy định tài khoản AWS được sử dụng và mapping EBS volume sẽ được tạo và gán vào EC2 Instances EC2 Instance có thể được backup bằng cách tạo snapshot. Key pair (public key và private key) dùng để mã hóa thông tin đăng nhập cho EC2 Instance. Elastic Block Store Amazon EBS cung cấp block storage và được gán trực tiếp vào EC2 Instance, tuy được gán trực tiếp như một RAW device, EBS về bản chất hoạt động độc lập với EC2 và được kết nối thông qua mạng riêng của EBS. EBS ó 2 nhóm đĩa chính là HDD và SSD, được thiết kế để đạt độ sẵn sàng 99.999% bằng cách replicate dữ liệu giữa 3 Storage Node trong 1 AZ Có một số EC2 Instances đặc thù được tối ưu hóa hiệu năng EBS (Optimized EBS Instances) EBS volumes, mặc định chỉ được gán vào 1 EC2 Instance, EC2 Instances chạy trên Hypervisor Nitro có thể dùng 1 EBS volume gắn vào nhiều EC2 Instances. (EBS Multi attach) EBS được backup bằng cách thực hiện snapshot vào S3 (Simple Storage Service) Snapshot đầu tiên là full, tất cả các snapshot tiếp theo là incremental. Instance Store Instance store là vùng đĩa NVME tốc độ cực cao, nằm trên physical node chạy các máy ảo EC2. Instance store sẽ bị xóa hết dữ liệu khi chúng ta thực hiện stop EC2 Instance Instance store sẽ không bị xóa dữ liệu khi thực hiện restart máy hoặc bị crash. Instance store không replicate dữ liệu dự phòng nên thường không khuyến khích lưu trữ dữ liệu quan trọng. Sử dụng dữ liệu trong các trường hợp cần hiệu năng cực cao lên tới hàng triệu IOPS. Khi sử dụng thường được replicate dữ liệu vào một EBS volume để đảm bảo an toàn. ","date":"2023-09-07","id":23,"permalink":"/docs/ly-thuyet/8.-amazon-elastic-compute-cloud-ec2/","summary":"\u003cstyle\u003ebody {text-align: justify}\u003c/style\u003e\r\n\u003cp\u003e\u003cspan style=\"color: orange; font-weight:bold;\"\u003eAmazon EC2\u003c/span\u003e giống với máy chủ áo hoặc máy chủ vật lý truyền thống.\u003c/p\u003e\n\u003cp\u003eCó khả năng:\u003c/p\u003e","tags":[],"title":"8. Amazon Elastic Compute Cloud (EC2)"},{"content":"","date":"2023-09-07","id":24,"permalink":"/docs/thuc-hanh/th%E1%BB%B1c-h%C3%A0nh-l%E1%BA%A7n-1/","summary":"\u003cstyle\u003ebody {text-align: justify}\u003c/style\u003e","tags":[],"title":"Thực hành lần 1"},{"content":"Link to valuable, relevant resources.\n","date":"2024-02-27","id":25,"permalink":"/docs/resources/","summary":"\u003cp\u003eLink to valuable, relevant resources.\u003c/p\u003e","tags":[],"title":"Resources"},{"content":"","date":"2023-09-07","id":26,"permalink":"/de-docs/","summary":"","tags":[],"title":"DE Docs"},{"content":"","date":"2023-09-07","id":27,"permalink":"/docs/","summary":"","tags":[],"title":"Docs"},{"content":"","date":"2023-09-07","id":28,"permalink":"/privacy/","summary":"","tags":[],"title":"Privacy Policy"},{"content":"","date":"2023-09-07","id":29,"permalink":"/","summary":"","tags":[],"title":"Kho kiến thức của Dũng"},{"content":"","date":"0001-01-01","id":30,"permalink":"/categories/","summary":"","tags":[],"title":"Categories"},{"content":"","date":"0001-01-01","id":31,"permalink":"/contributors/","summary":"","tags":[],"title":"Contributors"},{"content":"","date":"0001-01-01","id":32,"permalink":"/tags/","summary":"","tags":[],"title":"Tags"}]