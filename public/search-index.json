[{"content":"Well-thought-through product announcements will help increase feature awareness and engage users with new functionality. Just like sharing your public roadmap, it\u0026rsquo;s also a great way to let potential customers see that you\u0026rsquo;re constantly improving.\nFurther reading Read How to announce product updates and features ","date":"2025-01-20","id":0,"permalink":"/blog/example-post/","summary":"You can use blog posts for announcing product updates and features.","tags":[],"title":"Example Post"},{"content":"","date":"2023-09-07","id":1,"permalink":"/blog/","summary":"","tags":[],"title":"Blog"},{"content":"","date":"2023-09-07","id":2,"permalink":"/de-docs/overview/","summary":"","tags":[],"title":"Giới thiệu"},{"content":"","date":"2023-09-07","id":3,"permalink":"/docs/overview/","summary":"","tags":[],"title":"Giới thiệu"},{"content":"","date":"2023-09-07","id":4,"permalink":"/de-docs/spark/","summary":"","tags":[],"title":"Mở đầu về Spark"},{"content":"\rSpark là gì? Apache Spark là một hệ thống xử lý dữ liệu phân tán mã nguồn mở và được sử dụng cho các công việc xử lý dữ liệu lớn. Có thể sử dụng trên: on premises hay trên cloud. Spark sử dụng khả năng lưu trữ và xử lý dữ liệu trên bộ nhớ thay vì sử dụng ổ cứng (HDD/SSD). Làm tăng khả năng xử lý dữ liệu, khả năng xử lý nhanh hơn Hadoop MapReduce. Apache Spark cung cấp API phát triển bằng ngôn ngữ Python, Scala, Java. Có thể sử dụng cho việc: Machine Learning (MLlib). Truy vấn, tương tác với dữ liệu (Spark SQL). Xử lý dữ liệu thời gian thực (Structured Streaming). Xử lý đồ thị (GraphX). Spark được thiết kế xung quanh 4 triết lý chính: Tốc độ (Speed) Dễ sử dụng (Ease of use) Tính mô đun (Modularity) Khả năng mở rộng (Extensibility) Spark có cộng đồng lớn và một danh sách các gói third-party Spark là một phần của sự phát triển của cộng đồng. ","date":"2023-09-07","id":5,"permalink":"/de-docs/spark/khai-niem/","summary":"\u003cstyle\u003ebody {text-align: justify}\u003c/style\u003e\r\n\u003ch3 id=\"spark-là-gì\"\u003eSpark là gì?\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cspan style=\"color: orange; font-weight:bold;\"\u003eApache Spark\u003c/span\u003e là một hệ thống xử lý dữ liệu phân tán mã nguồn mở và được sử dụng cho các công việc xử lý dữ liệu lớn.\u003c/li\u003e\n\u003cli\u003eCó thể sử dụng trên: \u003cspan style=\"color: orange; font-weight:bold;\"\u003eon premises\u003c/span\u003e hay \u003cspan style=\"color: orange; font-weight:bold;\"\u003etrên cloud\u003c/span\u003e.\u003c/li\u003e\n\u003cli\u003eSpark sử dụng khả năng lưu trữ và xử lý dữ liệu trên bộ nhớ thay vì sử dụng ổ cứng (HDD/SSD).\n\u003cul\u003e\n\u003cli\u003eLàm tăng khả năng xử lý dữ liệu, khả năng xử lý nhanh hơn Hadoop MapReduce.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cspan style=\"color: orange; font-weight:bold;\"\u003eApache Spark\u003c/span\u003e cung cấp API phát triển bằng ngôn ngữ Python, Scala, Java. Có thể sử dụng cho việc:\n\u003cul\u003e\n\u003cli\u003eMachine Learning (MLlib).\u003c/li\u003e\n\u003cli\u003eTruy vấn, tương tác với dữ liệu (Spark SQL).\u003c/li\u003e\n\u003cli\u003eXử lý dữ liệu thời gian thực (Structured Streaming).\u003c/li\u003e\n\u003cli\u003eXử lý đồ thị (GraphX).\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eSpark được thiết kế xung quanh 4 triết lý chính:\n\u003cul\u003e\n\u003cli\u003eTốc độ (Speed)\u003c/li\u003e\n\u003cli\u003eDễ sử dụng (Ease of use)\u003c/li\u003e\n\u003cli\u003eTính mô đun (Modularity)\u003c/li\u003e\n\u003cli\u003eKhả năng mở rộng (Extensibility)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eSpark có cộng đồng lớn và một danh sách các gói third-party Spark là một phần của sự phát triển của cộng đồng.\n\u003cbr\u003e \u003cbr\u003e\n\r\n\r\n\u003cimg\r\n  src=\"/de-docs/spark/images/spark-third-party_hu15587626843367224303.webp\"\r\n  width=\"940\"\r\n  height=\"701\"\r\n  decoding=\"async\"\r\n  fetchpriority=\"auto\"\r\n  loading=\"lazy\"\r\n  alt=\"spark-third-party\"id=\"h-rh-i-0\"\r\n/\u003e\u003c/li\u003e\n\u003c/ul\u003e","tags":[],"title":"1. Giới thiệu về Spark"},{"content":"\rSpark bao gồm 5 thành phần chính: Spark Core, Spark SQL, Spark Streaming, MLlib và GraphX.\nSpark SQL Như tên gọi của nó, thì mô-đun này làm việc tốt với structured data. Có thể đọc được dữ liệu lưu trữ trong bảng RDBMS hay các file có định dạng lưu trữ có cấu trúc như (csv, text, JSON, Avro, ORC, Parquet, \u0026hellip;). Khi sử dụng Spark qua API bằng Java, Python, Scala hay R. Bạn có thể sử dụng lệnh SQL để truy vấn dữ liệu. Ví dụ: Bạn có thể đọc file JSON lưu trữ trong Amazon S3, tạo một bảng tạm thời, và sử dụng lệnh SQL để đọc dữ liệu.\n// In Scala // Đọc dữ liệu từ S3 Bucket vào Spark DataFrame spark.read.json(\u0026#34;s3://apache_spark/data/committers.json\u0026#34;) .createOrReplaceTempView(\u0026#34;committers\u0026#34;) // Sử dụng SQL để trả về kết quả là một Spark DataFrame val results = spark.sql(\u0026#34;\u0026#34;\u0026#34;SELECT name, org, module, release, num_commits FROM committers WHERE module = \u0026#39;mllib\u0026#39; AND num_commits \u0026gt; 10 ORDER BY num_commits DESC\u0026#34;\u0026#34;\u0026#34;)\rBạn có thể viết một đoạn code như vậy với Python, R hay Java. Các mã byte sẽ được tạo giống hệt nhau, trả ra kết quả với hiệu suất tương tự.\nSpark MLlib Spark mang đến một thư viện các thuật toán Machine Learning phổ biến gọi là MLlib. Với hiệu suất được cải thiện đáng kể từ khi lần đầu ra mắt thư viện này. API này cho phép trích xuất hay chuyển đổi tính năng, xây dựng pipelines (cho việc huấn luyện và đánh giá), và persist models (cho việc saving và reloading models) trong quá trình triển khai. Thêm vào đó, có các tiện ích bổ sung bao gồm sử dụng các phép toán đại số tuyến tính và thống kê phổ biến. Ví dụ: Dưới đây là một đoạn code python bao gồm những hoạt động cơ bản của một data scientist sẽ làm khi xây một mô hình:\n# In Python from pyspark.ml.classification import LogisticRegression ... training = spark.read.csv(\u0026#34;s3://...\u0026#34;) test = spark.read.csv(\u0026#34;s3://...\u0026#34;) # Load training data lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8) # Fit the model lrModel = lr.fit(training) # Predict lrModel.transform(test) ...\rSpark Structured Streaming Được xây dựng dựa trên Spark SQL engine và DataFrame-based APIs. Từ phiên bản Spark 2.2, Structured Straming có thể được sử dụng cả trên môi trường production. Spark Structured Streaming cần thiết cho Big Data Developers để kết hợp và phản ứng với dữ liệu tại thời gian thực từ các nguồn như Apache Kafka, hay các nguồn khác. Developers có thể coi như đây là một bảng dữ liệu có cấu trúc và xử lý, truy vấn như một bảng tĩnh. Ví dụ: Dưới đây là một đoạn code mẫu về Spark Structured Streaming\n# In Python # Đọc luồng dữ liệu từ local host from pyspark.sql.functions import explode, split lines = (spark .readStream .format(\u0026#34;socket\u0026#34;) .option(\u0026#34;host\u0026#34;, \u0026#34;localhost\u0026#34;) .option(\u0026#34;port\u0026#34;, 9999) .load()) # Thực hiện việc chuyển đổi # Chia các dòng thành các từ words = lines.select(explode(split(lines.value, \u0026#34; \u0026#34;))).alias(\u0026#34;word\u0026#34;) # Tạo bộ đếm từ word_counts = words.groupBy(\u0026#34;word\u0026#34;).count() # Ghi vào Kafka query = (word_counts .writeStream .format(\u0026#34;kafka\u0026#34;) .option(\u0026#34;topic\u0026#34;, \u0026#34;output\u0026#34;))\rGraphX Như tên gọi, GraphX là một thư viện để thao tác với đồ thị (ví dụ: đồ thị mạng xã hội, đồ thị network,\u0026hellip;). GraphX cung cấp thuật toán đồ thị tiêu chuẩn cho việc phân tích, kết nối và truyền tải được đóng góp bởi cộng đồng như: PageRank, Connected Components và Triangle Counting. Ví dụ: Đoạn code dưới đây là một ví dụ về việc join 2 graph sử dụng GraphX APIs.\n// In Scala val graph = Graph(verticles, edges) messages = spark.textFile(\u0026#34;hdfs://...\u0026#34;) val graph2 = graph.joinVertices(messages) { (id, vertex, msg) =\u0026gt; ... }\r","date":"2023-09-07","id":6,"permalink":"/de-docs/spark/thanh-phan-cua-spark/","summary":"\u003cstyle\u003ebody {text-align: justify}\u003c/style\u003e\r\n\u003cp\u003e\u003cbr\u003e\n\r\n\r\n\u003cimg\r\n  src=\"/de-docs/spark/images/spark-components_hu5920700851516503745.webp\"\r\n  width=\"1314\"\r\n  height=\"391\"\r\n  decoding=\"async\"\r\n  fetchpriority=\"auto\"\r\n  loading=\"lazy\"\r\n  alt=\"Spark Components\"id=\"h-rh-i-0\"\r\n/\u003e\u003c/p\u003e\n\u003cp\u003eSpark bao gồm 5 thành phần chính: Spark Core, Spark SQL, Spark Streaming, MLlib và GraphX.\u003c/p\u003e","tags":[],"title":"2. Thành phần của Spark"},{"content":"\rHãy bắt đầu bằng cách nhìn từng thành phần của Spark trong ảnh. Spark Driver Là một phần của Spark application chịu trách nhiệm cho việc khởi tạo SparkSession. Spark Driver có nhiều vai trò như:\nGiao tiếp với cluster manager. Gửi yêu cầu sử dụng tài nguyên (CPU, memory,\u0026hellip;) từ cluster manager cho Spark\u0026rsquo;s Executor (JVMs). Chuyển đổi spark operations thành DAG computations, schedules (tính toán, lên lịch). Spark Session Kể tử Spark 2.0, SparkSession được coi như là cách kết nối thống nhất đến tất cả các hoạt động của Spark và dữ liệu. Gộp các entrypoint cũ như SparkContext, SQLContext, HiveContext, SparkConf và StreamingContext. Khiến việc sử dụng Spark trở nên dễ dàng hơn. Tức là kể từ giờ SparkSession là entrypoint cho tất cả các tính năng của Spark. Ví dụ: Dưới đây là một ví dụ về việc sử dụng SparkSession và thực hiện một số hoạt động truy vấn dữ liệu.\n// In Scala import org.apache.spark.sql.SparkSession // Build SparkSession val spark = SparkSession .builder .appName(\u0026#34;LearnSpark\u0026#34;) .config(\u0026#34;spark.sql.shuffle.partitions\u0026#34;, 6) .getOrCreate() // Sử dụng session để đọc file JSON val people = spark.read.json(\u0026#34;...\u0026#34;) // Sử dụng sesion để truy vấn một query và lưu thành DataFrame val resultsDF = spark.sql(\u0026#34;SELECT city, pop, state, zip FROM table_name\u0026#34;)\rCluster Manager Cluster Manager chịu trách nhiệm cho việc quản lý và phân bố tài nguyên cho cụm các node cho Spark. Hiện tại, Spark hỗ trợ 4 cluster managers:\ncluster managers được tích hợp sẵn trong Spark. Apache Hadoop YARN Apache Mesos Kubernetes Spark Executor Spark Executor chạy trên từng worker node trong cụm. Spark Executor giao tiếp với Spark Driver và chịu trách nhiệm cho việc thực thi các nhiệm vụ trên các worker. Trong hầu hết các chế độ triển khai (Deployment modes), chỉ có một executor trên một node.\nDeployment modes Hỗ trợ nhiều chế độ triển khai, giúp Spark chạy trên các configurations và environments khác nhau. Một số môi trường phổ biến như Apache Hadoop Yarn và Kubernetes và có thể hoạt động trên nhiều chế độ.\nDistributed data and partitions Dữ liệu vật lý thực tế được phân phối trên bộ lưu trữ dưới dạng các phân vùng (partitions) nằm trong HDFS hay cloud storage. Trong khi dữ liệu được phân phối thành các partitions trên các cụm vật lý (physical cluster). Spark xử lý từng partition dưới dạng trừu tượng dữ liệu logic cấp cao - như một DataFrame trong bộ nhớ.\nViệc phân vùng dữ liệu giúp cho hiệu suất xử lý song song hiệu quả. Việc này giúp cho Spark Executor chỉ xử lý dữ liệu gần với chúng nhất, giảm thiểu băng thông mạng.\nVí dụ: Đoạn code sau đây sẽ chuyển đổi dữ liệu vật lý lưu trên clusters thành 8 partition, và từng executor sẽ có một hoặc nhiều partitions để đọc vào memory.\n# In Python log_df = spark.read.text(\u0026#34;path_to_large_text_file\u0026#34;).repartition(8) print(log_df.rdd.getNumPartitions())\rVí dụ 2: Đoạn code dưới đây sẽ tạo một DataFrame với 10000 số nguyên phân bố trên 8 partitions trong bộ nhớ:\n# In Python df = spark.range(0, 10000, 1, 8) print(df.rđ.getNumPartitions())\r2 ví dụ trên đều có output là 8.\n","date":"2023-09-07","id":7,"permalink":"/de-docs/spark/cach-hoat-dong/","summary":"\u003cstyle\u003ebody {text-align: justify}\u003c/style\u003e\r\n\u003cp\u003eHãy bắt đầu bằng cách nhìn từng thành phần của Spark trong ảnh.\n\u003cbr\u003e\n\u003cbr\u003e\n\r\n\r\n\u003cimg\r\n  src=\"/de-docs/spark/images/architecture_hu4005592828857137198.webp\"\r\n  width=\"724\"\r\n  height=\"372\"\r\n  decoding=\"async\"\r\n  fetchpriority=\"auto\"\r\n  loading=\"lazy\"\r\n  alt=\"Architecture\"id=\"h-rh-i-0\"\r\n/\u003e\u003c/p\u003e\n\u003ch3 id=\"spark-driver\"\u003eSpark Driver\u003c/h3\u003e\n\u003cp\u003eLà một phần của Spark application chịu trách nhiệm cho việc khởi tạo SparkSession.\nSpark Driver có nhiều vai trò như:\u003c/p\u003e","tags":[],"title":"3. Cách Spark hoạt động"},{"content":"\rMột số khái niệm cần biết: Application: Một chương trình người dùng được xây dựng trên Spark và sử dụng API của nó. Bao gồm chương trình điều khiển (driver program) và executors trong cụm. SparkSession Một đối tượng cung cấp điểm vào để tương tác với các chức năng của Spark. Trong Spark Shell đã cung cấp sẵn SparkSession cho bạn nhưng trong Spark Application, bạn phải tự tạo SparkSession riêng. Job Một chương chình tính toán song song bao gồm nhiều tác vụ được tạo ra để phản hồi cho một Spark action (ví dụ: save(), collect()) Stage Mỗi job được phân chia thành các tác vụ nhỏ hơn được gọi là stages, chúng phụ thuộc lẫn nhau. Task Một đơn vị công việc (work) hay thực thi (executor) được gửi cho Spark Executor.\nSpark Jobs Spark Driver chuyển đổi Spark Application của bạn thành một hay nhiều Spark Jobs. Nó được chuyển đổi từng job vào một DAG (Directed Acyclic Graph)\nHình 1: Spark Driver tạo thành một hay nhiều Spark Jobs.\nSpark Stages Là một phần của DAG nodes, stages được tạo nên dựa trên các hoạt động có thể được thực hiện theo chuỗi hay song song. Không phải hoạt động nào của Spark cũng có thể xảy ra trên 1 stage, vì vậy nó có thể chia thành nhiều stage.\nHình 2: Spark Job tạo một hay nhiều stage (giai đoạn)\nSpark Tasks Mỗi stage bao gồm các Spark Tasks (một đơn vị thực thi), được liên kết trên mỗi Spark executor. Ví dụ một Executor với 16 cores có thể có 16 task hoặc hơn làm việc trên 16 partitions hoặc hơn, làm cho việc xử lý task cực kỳ song song!\nHình 3: Spark Stage tạo một hay nhiều task được phân phối đến executor.\nTransformation, Actions and Lazy Evaluation. Spark operations trên dữ liệu phân tán có thể phân thành 2 loại: tranformations và actions. Transformations: Chuyển đổi Spark DataFrame thành một DataFrame mới mà không làm thay đổi data gốc, mang lại cho nó đặc tính bất biến (immutability). Tất cả các tranformations được đánh giá một cách lười biếng (Evaluated Lazily), có nghĩa là kết quả không được tính toán trực tiếp, mà nó được ghi lại như là một lineage . Việc này cho phép Spark sau đó sắp xếp lại một số transformations nhất định, hợp nhất hay tối ưu hóa chúng thành các giai đoạn với hiệu quả cao hơn. Đồng thời mang lại khả năng phục hồi khi xảy ra lỗi.\nHình 4: Trong hình, Các transformations T được lưu lại cho đến khi action A được gọi đến. Mỗi tranformations tạo ra một DataFrame mới. Hình 5: Một số ví dụ về Tranformations và Actions trong Spark\nVí dụ: Không có gì xuất hiện khi lệnh .conut() dược thực thi:\n# In Python strings = spark.read.text(\u0026#34;../README.md\u0026#34;) filtered = strings.filter(strings.value.contains(\u0026#34;Spark\u0026#34;)) filtered.count() # Output: 20\r","date":"2023-09-07","id":8,"permalink":"/de-docs/spark/spark-application-concept/","summary":"\u003cstyle\u003ebody {text-align: justify}\u003c/style\u003e\r\n\u003ch3 id=\"một-số-khái-niệm-cần-biết\"\u003eMột số khái niệm cần biết:\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eApplication:\u003c/em\u003e\u003c/strong\u003e\n\u003cbr\u003e\nMột chương trình người dùng được xây dựng trên Spark và sử dụng API của nó. Bao gồm chương trình điều khiển (driver program) và executors trong cụm.\n\u003cbr\u003e\n\u003cbr\u003e\n\u003cstrong\u003e\u003cem\u003eSparkSession\u003c/em\u003e\u003c/strong\u003e\n\u003cbr\u003e\nMột đối tượng cung cấp điểm vào để tương tác với các chức năng của Spark.\nTrong Spark Shell đã cung cấp sẵn SparkSession cho bạn nhưng trong Spark Application, bạn phải tự tạo SparkSession riêng.\n\u003cbr\u003e\n\u003cbr\u003e\n\u003cstrong\u003e\u003cem\u003eJob\u003c/em\u003e\u003c/strong\u003e\n\u003cbr\u003e\nMột chương chình tính toán song song bao gồm nhiều tác vụ được tạo ra để phản hồi cho một Spark action (ví dụ: save(), collect())\n\u003cbr\u003e\n\u003cbr\u003e\n\u003cstrong\u003e\u003cem\u003eStage\u003c/em\u003e\u003c/strong\u003e\n\u003cbr\u003e\nMỗi job được phân chia thành các tác vụ nhỏ hơn được gọi là stages, chúng phụ thuộc lẫn nhau.\n\u003cbr\u003e\n\u003cbr\u003e\n\u003cstrong\u003e\u003cem\u003eTask\u003c/em\u003e\u003c/strong\u003e\n\u003cbr\u003e\nMột đơn vị công việc (work) hay thực thi (executor) được gửi cho Spark Executor.\u003c/p\u003e","tags":[],"title":"4. Các khái niệm Spark Application"},{"content":"","date":"2023-09-07","id":9,"permalink":"/de-docs/overview/loi-mo-dau/","summary":"\u003cstyle\u003ebody {text-align: justify}\u003c/style\u003e","tags":[],"title":"Lời mở đầu"},{"content":"\rGiới thiệu bản thân Xin chào mình tên là Lương Tiến Dũng. Là sinh viên công nghệ thông tin năm thứ 2 tại trường đại học Giao thông Vận tải. Đang theo đuổi con đường trở thành một Data Engineer.\n","date":"2023-09-07","id":10,"permalink":"/docs/overview/l%E1%BB%9Di-m%E1%BB%9F-%C4%91%E1%BA%A7u/","summary":"\u003cstyle\u003ebody {text-align: justify}\u003c/style\u003e\r\n\u003ch4 id=\"giới-thiệu-bản-thân\"\u003eGiới thiệu bản thân\u003c/h4\u003e\n\u003cp\u003eXin chào mình tên là Lương Tiến Dũng. Là sinh viên công nghệ thông tin năm thứ 2 tại trường đại học Giao thông Vận tải.\nĐang theo đuổi con đường trở thành một Data Engineer.\u003c/p\u003e","tags":[],"title":"Lời mở đầu"},{"content":"","date":"2023-09-07","id":11,"permalink":"/docs/ly-thuyet/","summary":"","tags":[],"title":"Lý thuyết"},{"content":"","date":"2023-09-07","id":12,"permalink":"/docs/thuc-hanh/","summary":"","tags":[],"title":"Thực hành"},{"content":"\rĐiện toán đám mây là gì? Điện toán đám mây là việc phân phối các tài nguyên CNTT theo nhu cầu qua internet với chính sách thanh toán theo mức sử dụng.\nLợi ích của điện toán đám mây Sử dụng bao nhiêu tính tiền bấy nhiêu, cung cấp khả năng tối ưu hóa chi phí Tăng tốc độ phát triển, nhờ tận dụng các tính năng tự động hóa và quản trị bởi nhà cung cấp dịch vụ Linh hoạt, thêm bớt tài nguyên tùy ý. Mở rộng quy mô ứng dụng lên toàn cầu AWS, điều gì tạo nên sự khác biệt? AWS là nhà cung cấp hạ tầng Cloud dẫn đầu trong 13 năm liên tiếp. [Tính tới hết 2023] AWS là nhà cung cấp khác biệt về tầm nhìn và văn hóa. Triết lý về giá của AWS: Khách hàng sẽ càng ngày càng trả ít tiền hơn cho cùng dịch vụ / tính năng / tài nguyên sử dụng. AWS đưa việc mang lại giá trị thực sự cho khách hàng lên hàng đầu trong tất cả các nguyên tắc lãnh đạo (Leadership Principle) của mình Bắt đầu hành trình lên mây như thế nào? Là nhà cung cấp điện toán đám mây hàng đầu, số lượng khóa học nhiều nhất, có chiều sâu nhất trong tất cả các nhà cung cấp dịch vụ Hoàn toàn có thể tự học để trỏ thành chuyên gia Hãy kết bạn và cùng nhau học hỏi. Đăng ký tài khoản AWS càng sớm càng tốt và trải nghiệm thông qua \u0026lt;span style=\u0026ldquo;color: oran ","date":"2023-09-07","id":13,"permalink":"/docs/ly-thuyet/1.-%C4%91i%E1%BB%87n-to%C3%A1n-%C4%91%C3%A1m-m%C3%A2y/","summary":"\u003cstyle\u003ebody {text-align: justify}\u003c/style\u003e\r\n\u003ch3 id=\"điện-toán-đám-mây-là-gì\"\u003eĐiện toán đám mây là gì?\u003c/h3\u003e\n\u003cp\u003eĐiện toán đám mây là việc phân phối các tài nguyên CNTT theo nhu cầu qua internet\nvới \u003cspan style=\"color: orange; font-weight:bold;\"\u003echính sách thanh toán theo mức sử dụng.\u003c/span\u003e\u003c/p\u003e","tags":[],"title":"1. Điện toán đám mây"},{"content":"\rTrung tâm dữ liệu của AWS (Data Center) Một trung tâm dữ liệu có thể chứa hàng chục ngàn máy chủ.\nTất cả trung tâm dữ liệu của AWS đều sử dụng các thiết bị được tối ưu hóa dành riêng cho hoạt động của AWS\nMore Info\nAvailability Zone Một Availability Zone (AZ) bao gồm một hoặc nhiều trung tâm dữ liệu, các (AZ) được thiết kế để không xảy ra sự cố ảnh hưởng đồng thời 2 AZ một lúc (fault isolation) Giữa 2 AZ là đường kết nối riêng tốc độ cao AWS khuyến nghị nên triển khai ứng dụng tối thiểu trên 2 AZ Region Một AWS Region bao gồm tối thiểu 3 Availability Zone. Hiện tại có hơn 25 Region trên toàn cầu. Các Region được kết ối với nhau bởi mạng backbone của AWS. Mặc định dữ liệu và dịch vụ ở các Region độc ập với nhau. (Trừ một số dịch vụ quy mô Global) Edge Locations Là mạng lưới trung tâm dữ liệu AWS được thiết kế để cung cấp dịch vụ với độ trễ thấp nhất có thể. Các dịch vụ AWS hoạt động tại Edge Locations (POP) bao gồm CloudFront (CDN) Web Application Fir ","date":"2023-09-07","id":14,"permalink":"/docs/ly-thuyet/2.-h%E1%BA%A1-t%E1%BA%A7ng-to%C3%A0n-c%E1%BA%A7u-c%E1%BB%A7a-aws/","summary":"\u003cstyle\u003ebody {text-align: justify}\u003c/style\u003e\r\n\u003ch3 id=\"trung-tâm-dữ-liệu-của-aws-data-center\"\u003eTrung tâm dữ liệu của AWS (Data Center)\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eMột trung tâm dữ liệu có thể chứa hàng chục ngàn máy chủ.\u003c/p\u003e","tags":[],"title":"2. Hạ tầng toàn cầu của AWS"},{"content":"\rAWS Management Console - Root Login Chúng ta có thể login bằng tài khoản root hoặc bằng tài khoản IAM User (tài khoản con giúp quản lý truy xuất các tài nguyên của AWS) Link AWS Console\nAWS Management Console - IAM Login Khi login bằng IAM User chúng ta cần cung cấp thêm thông tin Account ID (Chuỗi 12 chữ số) để xác định account AWS\nAWS Management Console - Service Search Sau khi login có thể tìm kiếm và sử dụng các dịch vụ của AWS AWS Management Console - Support Center Tạo support case để yêu cầu trợ giúp\nAWS Command Line Interface (CLI) Open source, cho phép tương tác với các dịch vụ AWS bằng command. Chức năng tương đương với chức năng được cung cấp bởi AWS Management Console dựa trên trình duyệt Sử dụng Access key / Secret Access Key để sử dụng CLI. AWS SDK Đơn giản hóa việc sử dụng AWS bằng cách cung cấp một bộ thư viện nhất quán và quen thuộc cho đội ngũ phát triển ứng dụng. Sử dụng Access key / Secret Access Key để sử dụng SDK. Phát triển ứng dụng sử dụng AWS dễ dàng. ","date":"2023-09-07","id":15,"permalink":"/docs/ly-thuyet/3.-c%C3%B4ng-c%E1%BB%A5-qu%E1%BA%A3n-l%C3%BD-aws-services/","summary":"\u003cstyle\u003ebody {text-align: justify}\u003c/style\u003e\r\n\u003ch3 id=\"aws-management-console---root-login\"\u003eAWS Management Console - Root Login\u003c/h3\u003e\n\u003cp\u003eChúng ta có thể login bằng tài khoản root hoặc bằng tài khoản IAM User (tài khoản con giúp quản lý truy xuất các tài nguyên của AWS)\n\u003cbr\u003e \u003ca href=\"https://aws.amazon.com/console/\"\u003eLink AWS Console\u003c/a\u003e\u003c/p\u003e","tags":[],"title":"3. Công cụ quản lý AWS Services"},{"content":"\rTối ưu hóa chi phí trên AWS Lựa chọn cấu hình tài nguyên tính toán và nơi lưu trữ dữ liệu phù hợp. Tận dụng các phương thức thanh toán giảm giá như reserved instance , saving plan, spot. Xoá các tài nguyên không sử dụng, bật tắt tự động các tài nguyên không cần chạy 24/7. Tận dụng các dịch vụ serverless. Thiết kế kiến trúc tối ưu Cài đặt và sử dụng AWS Budget. Quản lý chi phí theo phòng ban / ứng dụng với cost allocation tag. Liên tục theo dõi và tối ưu hóa chi phí. Công cụ tính toán chi phí https://calculator.aws/#/\nCho phép tạo các estimate các dịch vụ thông dụng. Có thể chia sẻ ca estimate cho nguười khác. Chi phí sẽ khác biệt theo từng Region Làm việc với AWS Support AWS có 4 gói hỗ trợ chính: Basic (Explore) Developer (Dev/Test) Business (Production) Enterprise (Large Enterprise) Có thể nâng cấp gói hỗ trợ trong thời gian ngắn để đẩy nhanh tốc độ hỗ trợ khi có sự cố quan trọng cần xử lý nhanh. ","date":"2023-09-07","id":16,"permalink":"/docs/ly-thuyet/4.-t%E1%BB%91i-%C6%B0u-h%C3%B3a-chi-ph%C3%AD-tr%C3%AAn-aws-v%C3%A0-l%C3%A0m-vi%E1%BB%87c-v%E1%BB%9Bi-aws-support/","summary":"\u003cstyle\u003ebody {text-align: justify}\u003c/style\u003e\r\n\u003ch3 id=\"tối-ưu-hóa-chi-phí-trên-aws\"\u003eTối ưu hóa chi phí trên AWS\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eLựa chọn cấu hình tài nguyên tính toán và nơi lưu trữ dữ liệu phù hợp.\u003c/li\u003e\n\u003cli\u003eTận dụng các phương thức thanh toán giảm giá như \u003cspan style=\"color: orange; font-weight:bold;\"\u003ereserved instance\u003c/span\u003e\n, \u003cspan style=\"color: orange; font-weight:bold;\"\u003esaving plan\u003c/span\u003e, \u003cspan style=\"color: orange; font-weight:bold;\"\u003espot\u003c/span\u003e.\u003c/li\u003e\n\u003cli\u003eXoá các tài nguyên không sử dụng, bật tắt tự động các tài nguyên không cần chạy 24/7.\u003c/li\u003e\n\u003cli\u003eTận dụng các dịch vụ \u003cspan style=\"color: orange; font-weight:bold;\"\u003eserverless\u003c/span\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"thiết-kế-kiến-trúc-tối-ưu\"\u003eThiết kế kiến trúc tối ưu\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eCài đặt và sử dụng \u003cspan style=\"color: orange; font-weight:bold;\"\u003eAWS Budget\u003c/span\u003e.\u003c/li\u003e\n\u003cli\u003eQuản lý chi phí theo phòng ban / ứng dụng với \u003cspan style=\"color: orange; font-weight:bold;\"\u003ecost allocation tag\u003c/span\u003e.\u003c/li\u003e\n\u003cli\u003e\u003cspan style=\"color: orange; font-weight:bold;\"\u003eLiên tục\u003c/span\u003e theo dõi và tối ưu hóa chi phí.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"công-cụ-tính-toán-chi-phí\"\u003eCông cụ tính toán chi phí\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://calculator.aws/#/\"\u003ehttps://calculator.aws/#/\u003c/a\u003e\u003c/p\u003e","tags":[],"title":"4. Tối ưu hóa chi phí trên AWS và Làm việc với AWS Support"},{"content":"","date":"2023-09-07","id":17,"permalink":"/docs/thuc-hanh/th%E1%BB%B1c-h%C3%A0nh-l%E1%BA%A7n-1/","summary":"\u003cstyle\u003ebody {text-align: justify}\u003c/style\u003e","tags":[],"title":"Thực hành lần 1"},{"content":"Link to valuable, relevant resources.\n","date":"2024-02-27","id":18,"permalink":"/docs/resources/","summary":"\u003cp\u003eLink to valuable, relevant resources.\u003c/p\u003e","tags":[],"title":"Resources"},{"content":"","date":"2023-09-07","id":19,"permalink":"/de-docs/","summary":"","tags":[],"title":"DE Docs"},{"content":"","date":"2023-09-07","id":20,"permalink":"/docs/","summary":"","tags":[],"title":"Docs"},{"content":"","date":"2023-09-07","id":21,"permalink":"/privacy/","summary":"","tags":[],"title":"Privacy Policy"},{"content":"","date":"2023-09-07","id":22,"permalink":"/","summary":"","tags":[],"title":"Kho kiến thức của Dũng"},{"content":"","date":"0001-01-01","id":23,"permalink":"/categories/","summary":"","tags":[],"title":"Categories"},{"content":"","date":"0001-01-01","id":24,"permalink":"/contributors/","summary":"","tags":[],"title":"Contributors"},{"content":"","date":"0001-01-01","id":25,"permalink":"/tags/","summary":"","tags":[],"title":"Tags"}]