[{"content":"\rKhái niệm Với việc sử dụng Data Warehouse cho những công việc phân tích các bảng có cấu trúc cao nhưng lại không phù hợp khi lưu trữ những dữ liệu semi-structured và unstructured. Mặc dù nó tốt trong công việc xử lý dựa trên SQL nhưng việc xử lý dữ liệu không chỉ dừng lại ở SQL. Ví dụ như trích xuất metadata từ dữ liệu unstructured, như audio hay images, phù hợp với các công cụ machine learning.\nMột cloud data lake là trung tâm dữ liệu có khả năng mở rộng cao khi cần, bao gồm:\nStructured Data (row-column based tables) Semi-structured data (như JSON và XML file, log records, và sensor data streams) Unstructured data (như audio, video streams, Word/PDF và email) Dữ liệu từ nhiều nguồn có thể load nhanh chóng vào data lake giữ nguyên format và cấu trúc. Khác với data warehouse, data lake không cần phải tiêu chuẩn thành một cấu trúc trước khi đưa vào sử dụng.\nMột cloud data lake cũng tích hợp nhiều công cụ phân tích phong phú, bao gồm SQL, code-based tool (như Apache Spark), đc biệt là machine learning tool và BI visualization tools\nKiến trúc logic của Data Lake Hình dưới đây mô tả một tập các thành phần đc lập với nhau tổ chức thành 5 lớp logic, có thể phát triển qua thời gian cùng với sự đổi mới của data management và phương pháp analytics với các công cụ mới.\nThe storage layer and storage zones Như trong hình phần storage layer được xây dựng trên cloud object store là Amazon S3. Cung cấp kho lưu trữ không giới hạn, chi phí thấp có thể lưu trữ đa dạng loại dữ liệu. Phần storage layer được tổ chức thành nhiều zones, mỗi zone có một mục đích cụ thể. Không có luật nào giới hạn số zone trong một data lake, hay là tên zone, nhưng ta sẽ thường thấy:\nLanding/raw zone: Đây là khu vực mà Ingestion Layer ghi dữ liệu vào từ nguồn. Trực tiếp lưu trữ dữ liệu thô từ nguồn. Clean/transform zone: Đây là khu vực để lưu trữ sau khi được xử lý như là xác thực, làm sạch, và tối ưu dataset. Dữ liệu ở khu vực này thường được lưu dưới dạng tối ưu như là Parquet và nó thường được phân vùng để tăng tốc thực thi truy vấn và các downstream processing. Dữ liệu PII Information (dữ liệu cá nhân) có thể được xóa, che giấu hoặc thay thế với tokens. Curated/enrich zone: Dữ liệu trong khu vực clean/transform có thể được tinh chỉnh và làm phong phú hơn với từng mục đích cụ thể. Dữ liệu ở đây thường được phân vùng, phân loại và tối ưu cho consumption layer. Dựa vào từng yêu cầu business cụ thể, vài data lake sẽ có nhiều hơn hoặc ít hơn số zone. Ví dụ là một data lake cơ bản có thể chỉ cần có 2 zone (raw và curated zone), nhưng có vài data lake sẽ có đến hơn 5 zones để xử lý giai đoạn trung gian hay yêu cầu cụ thể nào đó.\nCatalog and search layers Một Data Lake thường chứa một lướng lớn datasets, từ nhiều nguồn từ bên trong lẫn bên ngoài. Những datasets này hữu ích với nhiều teams trong tổ chức nên cần phải có khả năng để tìm kiếm dataset và xem các lược đồ hay metadata của dataset.\nMột technical catalog được dùng để ánh xạ nhiều file lưu trong storage layer dưới dạng cơ sử dữ liệu và bảng.\nMột business catalog tập trung vào metadata quan trọng với business. Có thể bao gồm các thuộc tính như: chủ data, lần cuối dữ liệu của dataset được update, tóm tắt mục đích của bảng, định nghĩa các cột,\u0026hellip; Hỗ trợ khả năng tìm kiếm nâng cao, hỗ trợ các team tìm thấy data họ muốn tùy use case.\nIngestion Layer Ingestion layer chịu trách nhiệm cho việc kết nối với các dữ liệu nguồn và mang data vào landing/raw zone trong storage layer. Có thể bao gồm đa dạng các tools khác nhau, mỗi tools có thể connect với các loại data source khác nhau:\nData Structured (structured, semi-structured, hay unstructured) Data delivery type (table rows, data stream, data file) Data production cadence (batch hay streaming) Cách tiếp cận này giúp việc thêm tool và data source mới dễ dàng và linh hoạt.\nProcessing layer Processing layer là nơi transforms dữ liệu thông qua nhiều bước khác nhau: làm sạch, tiêu chuẩn hóa và làm giàu dữ liệu. Processing layer lưu trữ transformed data vào các zones khác nhau, ghi nó vào clean zone và curated zone, rồi đảm bảo rằng technical catalog được cập nhật. Các công cụ trong AWS thường được sử dụng cho việc này bao gồm: AWS Glue và Amazon EMR\nConsumption layer Khi dữ liệu đã sẵn sàng để được tiêu thụ, nó có thể được phân tích bằng nhiều công cụ, Để thực hiện phân tích dữ liệu trong data lake, consumption layer cung cấp công cụ có mục đích truy cập dữ liệu từ storage layer và đọc schema từ catalog layer.\n","date":"2023-09-07","id":0,"permalink":"/de-docs/de-basic/datalake/overview/","summary":"\u003cstyle\u003ebody {text-align: justify}\u003c/style\u003e\r\n\u003ch3 id=\"khái-niệm\"\u003eKhái niệm\u003c/h3\u003e\n\u003cp\u003eVới việc sử dụng Data Warehouse cho những công việc phân tích các bảng có cấu trúc cao nhưng lại không phù hợp khi lưu trữ những dữ liệu semi-structured và unstructured.\nMặc dù nó tốt trong công việc xử lý dựa trên SQL nhưng việc xử lý dữ liệu không chỉ dừng lại ở SQL. Ví dụ như trích xuất metadata từ dữ liệu unstructured, như audio hay images, phù hợp với các công cụ machine learning.\u003c/p\u003e","tags":[],"title":"Data Lake cơ bản"},{"content":"\rSự kết hợp tốt nhất của data warehouses và data lakes Một data lake phù hợp cho việc lưu trữ tất cả các loại data không tốn kém và cung cấp nhiều tools để làm việc và tiêu thụ data. Bao gồm việc transform data với framework như Apache Spark, train model machine learning như Amazon SageMaker, truy vấn dữ liệu bằng SQL như Amazon Athena, Presto hay Trino\nTuy nhiên, có vài hạn chế với data lake truyền thống. Ví dụ, data lake không hỗ trợ ACID (tính nguyên tử, tính nhất quán, tính độc lập và độ bền) là thuộc tính thông thường của hầu hết databases. Cũng như là với việc lưu trữ giá rẻ, hiệu suất truy vấn cũng cao bằng nó có thể thực hiện trên data warehouse.\nVới những hạn chế đó gây ra độ phức tạp khi làm việc với nhiều team cùng một dataset, với một team update dữ liệu và một team truy vấn dữ liệu cùng lúc thì có thể dẫn đến data không nhất quán, cũng như là bạn có nhiều dashboard và báo cáo với các công cụ Business Intelligence, hiệu suất truy vấn trong data lake không phù hợp với requirements.\nNhững khó khăn này thường được xử lý bằng cách tải một phần data từ data lake vào data warehouse, như Amazon Redshift hay Snowflake. Đã cung cấp hiệu suất và tính nhất quán theo yêu cầu khi các team cùng làm việc trên cùng một dataset. Tuy nhiên, kho lưu trữ data warehouse đắt đỏ và với một số trường hợp cần kết hợp nhiều dữ liệu từ nhiều nguồn khác nhau, việc này không kinh tế khi tải tất cả dữ liệu vào trong data warehouse.\nĐể đối mặt với những thách thức này, một định dạng bảng mới được ra đời có thể đơn giản hóa quy trình updating data lake tables với một cách an toàn, cho phép các truy vấn được liên kết (kết hợp data từ nhiều storage engines khác nhau) và phương pháp này được gọi với cái tên data lakehouse\nĐã có nhiều blogs, bài báo về các cách tiếp cận data lakehouse khác nhau từ nhiều doanh nghiệp: AWS, Azure, Google, Snowflake, Databricks, Dremio và Starburst. Cuối cùng, không có định nghĩa tiêu chuẩn nào về data lakehouse, các nhà cung cấp dịch vụ này thường cung cấp sự kết hợp tốt nhất về data lake và data warehouse với bộ công cụ của riêng họ.\nTuy nhiên, có một số công nghệ và phương pháp được áp dụng rộng rãi cho phép làm mờ ranh giói giữa data warehouse và datalake.\nNew data lake table formats Vài năm qua, một lượng các table formats mới được đề xuất là thế hệ mới của định dạng ban đầu Hive - một định dạng ra đời bởi Facebook hơn một thập kỷ trước. Hiện tại, có 3 định dạng mới cạnh tranh chính với nhau để phát triển modern data lakes.\nMỗi table formats có thế mạnh và điểm yếu riêng, tất cả đều nhằm mục đích cho phép các bản cập nhật nhất quán và đọc dữ liệu đơn giản hơn, cũng như là hiệu suất cao hơn và khả năng truy vấn tại một điểm thời gian nhất định. Các table formats mới này cung cấp chức năng làm việc với dữ liệu trong data lake đơn giản hơn và tương tự những gì chỉ có ở data warehouse và databases thông thường. Ba table formats chính được coi là thế hệ mới là:\nDelta Lake: Được tạo bởi Databricks, cung cấp cả phiên bản trả phí và open-source. Một số các công cụ thương mại và open-source có thể làm việc với Delta Lake files, bao gồm Apache Spark, Presto, Snowflake, Redshift và nhiều công cụ khác. Apache Hudi: Được tạo bởi Uber, và đã ủng hộ cho Apache Sofware Foundation, và giờ nó là công cụ open-source. Hudi đã được áp dụng rộng rãi và các blog bài viết/case studies đề cập đến sử dụng Apache Hudi từ nhiều công ty như Amazon Transportation Service, Walmart, Robinhood và GE Aviation. Apache Iceberg: Được tạo bởi 2 engineers từ Netflix và sau đó cũng ủng hộ cho Apache Software Foundation, khiến nó thành công cụ open-source. Các công ty đã tham khảo công khai sử dụng Apache Iceberge, và có đóng góp cho dự án open-source này, bao gồm: Airbnb, Expedia, Adobe, Apple và Lyft Federated queries across database engines Một cách tiếp cận khác cũng trở nên phổ biến cùng với câu hỏi kết hợp tốt nhất của data warehouse và data lake. Đó là chức năng có thể truy vấn thông qua nhiều databases engines hay storage platforms. Ví dụ, cloud data warehouse như Amazon Redshift và Snowflake có thể truy vấn dữ liệu và load vào data warehouse, cũng như là dữ liệu từ trong data lake (như dựa trên Amazon S3).\nVới cách này, ta có thể load 12 tháng gần nhất vào data warehouse trong khi 4 năm trước vẫn có thể lưu trong data lake. Vì hầu hết truy vấn chỉ truy vấn 12 tháng gần nhất của dữ liệu và lưu nó trong kho có hiệu suất cao trong data warehouse. Tuy nhiên, với các câu truy vấn cần truy cập đến dữ liệu lịch sử, data warehouse có thể join với tables trong S3 data lake với dữ liệu gần nhất trong data warehouse.\nVới những câu truy vấn liên kết này, yêu cầu copy dữ liệu giữa nhiều hệ thống data thông qua ETL pipelines được giảm thiểu. Tuy nhiên, truy vấn trên nhiều các hệ thống khác nhau không thực hiện tốt như truy vấn dữ liệu local-only. Tuy nhiên, khả năng truy vấn trên nhiều hệ thống hữu dụng trong nhiều tình hoáng và giúp tạo ra hệ sinh thái big data tích hợp hơn.\n","date":"2023-09-07","id":1,"permalink":"/de-docs/de-basic/datalakehouse/overview/","summary":"\u003cstyle\u003ebody {text-align: justify}\u003c/style\u003e\r\n\u003ch3 id=\"sự-kết-hợp-tốt-nhất-của-data-warehouses-và-data-lakes\"\u003eSự kết hợp tốt nhất của data warehouses và data lakes\u003c/h3\u003e\n\u003cp\u003eMột data lake phù hợp cho việc lưu trữ tất cả các loại data không tốn kém và cung cấp nhiều tools để làm việc và tiêu thụ data.\nBao gồm việc transform data với framework như \u003cstrong\u003eApache Spark\u003c/strong\u003e, train model machine learning như \u003cstrong\u003eAmazon SageMaker\u003c/strong\u003e, truy vấn dữ liệu bằng SQL như \u003cstrong\u003eAmazon Athena, Presto\u003c/strong\u003e hay \u003cstrong\u003eTrino\u003c/strong\u003e\u003c/p\u003e","tags":[],"title":"Data lakehouse cơ bản"},{"content":"\rKhái niệm Một Enterprise Data Warehouse (EDW) là một kho lưu trữ tập trung bao gồm những tài sản dữ liệu có cấu trúc, được chọn lọc, nhất quán và đáng tin cậy.\nTài sản dữ liệu này được lấy từ nguồn thông tin từ lĩnh vực kinh doanh chính của công ty và được tích hợp từ nhiều nguồn dữ liệu như:\nCác ứng dụng giao dịch điều hành doanh nghiệp (ERP, CRM và các ứng dụng trong kinh doanh) hỗ trợ tất cả các lĩnh vực chính của doanh nghiệp. Các nguồn dữ liệu bên ngoài như dữ liệu từ đối tác, bên thứ 3. Một EDW cung cấp khả năng đưa ra quyết định, để đo lường hiệu suất của doanh nghiệp hay tìm các xu hướng hiện tại hay quá khứ của doanh nghiệp, tìm những cơ hội kinh doanh và tìm ra hành vi người dùng.\nThông thường, một kiến data warehouse bao gồm:\nNguồn dữ liệu từ doanh nghiệp đi vào DW thông qua quá trình Extract, Transform, Load (ETL) hay Extract, Load, Transform (ELT) Một hoặc nhiều data warehouses (thường là nhiều chủ đề tập trung vào nhiều data marts) Người dùng cuối tiêu thụ dữ liệu từ DW như là (Analytic tools và BI visualize) Dimensional modeling trong Data Warehouses Dữ liệu trong data warehouse thường được lưu trữ thành các relational tables được tổ chức thành các dimensional models được sử dụng rộng rãi:\nstar schema snowflake schema Việc sử dụng dimensional modeling khiến cho việc dễ dàng lọc, thu thập dữ liệu và khiến cho các câu truy vấn linh hoạt, dễ dàng và hiệu suất cao.\nTrong data warehouse, các bảng thường được tách biệt làm fact tables và dimenion tables.\nFact table thường lưu trữ các phép đo/số liệu chi tiết dạng số cho một lĩnh vực cụ thể (VD: sales) Dimension table lưu trữ bối cảnh mà các phép đo lường thực tế (fact measurements) được ghi lại. Ví dụ: thông tin cửa hàng, sản phẩm,\u0026hellip; Star schema Các bảng dữ liệu được tổ chức thành như một ngôi sao, với bảng sales fact nằm ở giữa của ngôi sao và các dimension table nằm ở các góc.\nƯu điểm:\nDễ dàng phân tích dữ liệu với ít lần JOIN Dễ nhìn, tổng hợp, tính toán Nhược điểm: Thường là denormalized (không chuẩn hóa) nên với lượng lớn dữ liệu dimension thì dẫn đến việc data duplacation và inconsistencies trong bảng dimension. Với lượng denormalized lớn thì có thể khiến việc update dữ liệu chậm chạp. Snowflake schema Các bảng dữ liệu được tổ chức như bông tuyết.\nƯu điểm:\nĐược normalization tránh việc inconsistencies và duplication. Giảm thiểu dung lượng phải lưu trữ. Nhược điểm:\nCác truy vấn JOIN phức tạp khiến truy vấn có thể chậm hơn. NOTE: Để chọn được việc sử dụng star schema hay snowflake schema bạn nên xem xét loại câu truy vấn được thực hiện trong dataset. Lựa chọn giữa việc câu truy vấn phức tạp hơn hay việc update bảng sẽ chậm hơn.\nData Marts Data Warehouse bao gồm tất cả các dữ liệu liên quan đến lĩnh vực kinh doanh và một lược đồ phức tạp. Được dùng để phân tích chéo nhau giữa các lĩnh vực, cung cấp thông tin cho các chiến lược kinh doanh.\nData mart là kho lưu trữ của một lĩnh vực duy nhất (ví dụ: marketing, bán hàng hoặc tài chính). Phục vụ một nhóm nhỏ người dùng doanh nghiệp, như một phòng ban với lược đồ dễ dàng nắm bắt và chỉ chứa những dữ liệu liên quan.\nData mart thường có một tập hợp bảng denormalized fact được tổ chức dễ dàng hơn EDW. Giảm thiểu khối lượng dữ liệu khiến data mart dễ xây dựng, dễ hiểu và dễ sử dụng.\nData mart có thể được tạo:\nTop-down: Dữ liệu được lấy từ data warehouse có sẵn, tập trung vào các khía cạnh cụ thể của doanh nghiệp. Bottom-up: Dữ liệu có nguồn góc trực tiếp từ transactional databases của lĩnh vực củ thể. ","date":"2023-09-07","id":2,"permalink":"/de-docs/de-basic/datawarehouse/data-warehouse-basic/","summary":"\u003cstyle\u003ebody {text-align: justify}\u003c/style\u003e\r\n\u003ch3 id=\"khái-niệm\"\u003eKhái niệm\u003c/h3\u003e\n\u003cp\u003eMột \u003cstrong\u003eEnterprise Data Warehouse (EDW)\u003c/strong\u003e là một kho lưu trữ tập trung bao gồm những tài sản dữ liệu có cấu trúc, được chọn lọc, nhất quán và đáng tin cậy.\u003c/p\u003e","tags":[],"title":"Data Warehouse cơ bản"},{"content":"\rBước đầu khi xây dựng giải pháp phân tích big data là ingest data từ nhiều nguồn vào AWS. Trong phần này, ta sẽ cùng tìm hiểu một số thành phần core của AWS service được thiết kế để làm việc này. Tuy nhiên, đây không phải là một đánh giá toàn diện cho tất cả các khả năng để ingesst data vào AWS.\nĐừng bị choáng ngợp bởi số lượng service trong phần này. Chúng ta sẽ cùng tìm hiểu phương pháp tiếp cận để quyết định dịch vụ đúng cho từng trường hợp cụ thể.\n","date":"2023-09-07","id":3,"permalink":"/de-docs/de-basic/ingestion-tool/overview/","summary":"\u003cstyle\u003ebody {text-align: justify}\u003c/style\u003e\r\n\u003cp\u003eBước đầu khi xây dựng giải pháp phân tích big data là \u003cstrong\u003eingest data\u003c/strong\u003e từ nhiều nguồn vào AWS. Trong phần này, ta sẽ cùng tìm hiểu một số thành phần core của AWS service được thiết kế để làm việc này. Tuy nhiên, đây không phải là một đánh giá toàn diện cho tất cả các khả năng để ingesst data vào AWS.\u003c/p\u003e","tags":[],"title":"Giới thiệu"},{"content":"","date":"2023-09-07","id":4,"permalink":"/de-docs/overview/","summary":"","tags":[],"title":"Giới thiệu"},{"content":"\rVai trò của một Data Engineer. Vai trò của một data engineer bao gồm:\nThiết kế, triển khai và giám sát pipelines có khả năng tích hợp những dữ liệu thô vào một kho lưu trữ. Chuyển đổi data để tối ưu cho việc phân tích, dựa trên yêu cầu của người tiêu thụ data (Ví dụ: Data Analysis) Khiến dữ liệu luôn sẵn sàng cho nhiều data consumers sử dụng các tool mà họ chọn. Đồng thời DE cũng phải tuân thủ với tất cả những yêu cầu về bảo mật và pháp luật khi thực hiện các nhiệm vụ trên.\nĐầu tiên, để có thể thiết kế, và triển khai một pipelines. DE sử dụng nhiều tools khác nhau, dựa trên source system và cách dữ liệu được thực hiện theo batch ingestion hay near real-time streaming ingestion.\nTiếp theo, DE cũng phải chịu trách nhiệm cho việc đảm bảo chất lượng của data, thêm vào các metadata cho vào data catalog, quản lý vòng đời của code khi thực hiện việc transformation.\nCuối cùng, DE cần phải tích hợp với các công cụ tiêu thụ data cho phép Data Analysts và Data Scientists có thể sử dụng tool họ thích để tìm ra Insight từ dữ liệu.\nLợi ích của việc sử dụng Cloud khi xây dựng giải pháp Big Data Trước đây, các tổ chức dựa vào các hệ thống phức tạp trong data center của riêng họ, giúp họ nắm bắt, lưu trữ và xử lý một lượng lớn dữ liệu. Nhưng trong thập kỷ qua, một lượng dữ liệu khổng lồ mà tổ chức muốn lưu trữ và phan tích, và họ đã gặp khó khăn khi scale up hệ thống on-premise theo nhu cầu. Scale up khiến những công cụ truyền thống đẻ quản lý dữ liệu ngày càng lớn khiến nó càng phức tạp, đắt đỏ và tốn rất nhiều thời gian. Họ đã phải tìm kiếm một giải pháp thay đổi để phù hợp với khối lượng dữ liệu ngày càng lớn này.\nVà khi AWS được chạy vào 2006, các tổ chức đã nhận ra lợi ích khi chạy khối lượng công việc của họ trên cloud về:\nKhả năng mở rộng Hiệu quả chi phí Bảo mật Tự động hóa Một trong những dịch vụ đầu tiên của AWS, Amazon Simple Storage Service (Amazon S3) đã mang lại hiệu quả, giá thành thấp cho hàng nghìn dự án lớn cho tới ngày nay.\n","date":"2023-09-07","id":5,"permalink":"/de-docs/de-basic/1-gioi-thieu-ve-de/","summary":"\u003cstyle\u003ebody {text-align: justify}\u003c/style\u003e\r\n\u003ch3 id=\"vai-trò-của-một-data-engineer\"\u003eVai trò của một Data Engineer.\u003c/h3\u003e\n\u003cp\u003eVai trò của một \u003cstrong\u003edata engineer\u003c/strong\u003e bao gồm:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThiết kế, triển khai và giám sát pipelines có khả năng tích hợp những dữ liệu thô vào một kho lưu trữ.\u003c/li\u003e\n\u003cli\u003eChuyển đổi data để tối ưu cho việc phân tích, dựa trên \u003cstrong\u003eyêu cầu\u003c/strong\u003e của người tiêu thụ data (\u003cstrong\u003eVí dụ: Data Analysis\u003c/strong\u003e)\u003c/li\u003e\n\u003cli\u003eKhiến dữ liệu luôn sẵn sàng cho nhiều data consumers sử dụng các tool mà họ chọn.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eĐồng thời DE cũng phải tuân thủ với tất cả những yêu cầu về bảo mật và pháp luật khi thực hiện các nhiệm vụ trên.\u003c/p\u003e","tags":[],"title":"Giới thiệu về Data Engineer"},{"content":"\r","date":"2023-09-07","id":6,"permalink":"/de-docs/kafka/overview/","summary":"\u003cstyle\u003ebody {text-align: justify}\u003c/style\u003e","tags":[],"title":"Giới thiệu về Kafka"},{"content":"\rMột trong những trường hợp ingestion phổ biến là đồng bộ dữ liệu từ hệ thống database vào trong một analytic pipeline, hoặc đặt dữ liệu trong Amazon S3-based data lake hay trong data warehouse system như Amazon Redshift.\nAWS DMS là công cụ linh hoạt có thể sử dụng để di chuyển cơ sở dữ liệu hiện có sang một cơ sở dữ liệu mới, ví dụ như di chuyển từ Oracle database hiện có sang Amazon Aurora PostgreSQL database. Thêm vào đó, Amazon DMS có thể sử dụng để tạo bản sao giữa 2 database engine giống nhau.\nKhi di chuyển đến cùng một engines, DMS sử dụng database engine gốc để khiến việc di chuyên dẽ dàng và hiệu suất. Từ góc độ phân tích, AWS DMS còn có thể sử dụng để chạy replication liên tục từ nhiều database engines phổ biến vào Amazon S3 data lake.\nTrong trường hợp sử dụng của tôi, tôi muốn đồng bộ sản xuất customer, products, và order databases vào data lake. Sử dụng DMS, ta có thể thực hiện tải dữ liệu từ đầu từ databases vào S3, xác định format mà file được ghi (như là CSV hay Parquet), và xác định nơi ingestion trong S3. Cùng thời điểm đó, tta có thể cài 1 DMS task để sao chép liên tục từ source databases vào S3 sau khi tải xong hết dữ liệu.\nVới transactional databases, các rows thường xuyên được updated, như là khách hàng muốn thay đổi địa chỉ hay số điện thoại. Khi truy vấn bằng SQL, ta có thể thấy thông tin updated, nhưng trong hầu hết các trường hợp, không có một phương pháp nào thực tế để theo dõi sự thay đổi của database bằng SQL. Do đó, DMS sử dụng database transaction log file từ database để theo dõi các bản cập nhật lên rows trong database và ghi nó vào file đích trong S3 với một số cột được thêm vào cho biết thao tác nào được phản ánh trong hàng đó - insert, update hay deletion. Quy trình theo dõi và ghi lại sự thay đổi này thường được gọi là Change Data Capture (CDC).\nHình dùng với một tình hoáng cụ thể là bạn có một lược đồ với các cột custid, lastname, firstname, address và phone, và các sự kiện liên tiếp xảy ra:\nMột khách hàng mới được thêm vào với tất cả các cột đầy đủ. Số điện thoại nhập vào không đúng, nên cột có số điện thoại được updated. Hồ sơ khách hàng sau đó xóa khỏi database. Ta có thể thấy CDC file được tạo ra bởi DMS: Hàng đầu tiên thể hiện một hàng mới được thêm vào trong bảng (thể hiện bởi chữ I trong cột đầu tiên). Hàng thứ 2 thể hiện là một record được cập nhật (thể hiện bởi chữ U trong cột đầu tiên). Cuối cùng, dòng thứ 3 thể hiện rằng record này đã bị xóa khỏi bảng (chữ D trong cột đầu tiên).\nSau đó ta sẽ có một quy trình cập nhật riêng biệt để đọc và áp dụng những bản updates vào full load, tạo một point-in-time snapshot mới của cơ sở dữ liệu nguồn. Quy trình cập nhật này sẽ được lên lịch để chạy thường xuyên, và mỗi khi nó chạy, nó apply lần cập nhật cuối cùng thành các record bởi DMS đến các snapshot trước, tạo một point-in-time.\nAmazon DMS có sẵn ở provisioned mode (trong đó bạn chọn size của replication instance sử dụng để kết nối với source database, làm mọi transformation, và ghi vào đích) hay chế độ serverless (trong đó DMS sẽ tự động cấu hình, mổ rộng và quản lý tài nguyên cần thiết cho việc migration dựa theo yêu cầu)\nWhen to use: Amazon DMS đơn giản hóa việc migrating từ một database engine này đến database engine khác hoặc đồng bộ dữ liệu từ database có sẵn tới Amazon S3 trên cơ sở liên tục.\nWhen not to use: Amazon DMS gây ra một số tải cho production database trong quá trình migrations, vì vậy bạn cần lưu ý điều này\nXem thêm ở: https://aws.amazon.com/dms/\n","date":"2023-09-07","id":7,"permalink":"/de-docs/de-basic/ingestion-tool/amazon-dms/","summary":"\u003cstyle\u003ebody {text-align: justify}\u003c/style\u003e\r\n\r\n\u003cimg\r\n  srcset=\"data:image/webp;base64,UklGRngAAABXRUJQVlA4IGwAAADwAwCdASoQABIAP1mKtkspJKKYBACTFYT2AFR6EfcHyH6pfkbTbfL0AP7gpuDCEvfMhRWjj/eW7L5sGeApyF4yFwW6yxhFxFECx8vuSfbz8YlTTqULcUmd9h4jxmdVKJCYxJ\u0026#43;Zwjj1YxCAAAA=\"\r\n      data-srcset=\"/de-docs/de-basic/ingestion-tool/aws-dms_hu_567cb72a459f0ad0.webp 197w\"\r\n      data-sizes=\"auto\"\r\n  src=\"/de-docs/de-basic/ingestion-tool/aws-dms_hu_c8c8b2899201d1a3.jpg\"\r\n  width=\"197\"\r\n  height=\"222\"\r\n  decoding=\"async\"\r\n  fetchpriority=\"auto\"\r\n  loading=\"lazy\"\r\n  alt=\"\"\r\n  class=\"lazyload blur-up center-image\"\r\n\u003e\n\u003cp\u003eMột trong những trường hợp ingestion phổ biến là đồng bộ dữ liệu từ hệ thống database vào trong một analytic pipeline, hoặc đặt dữ liệu trong Amazon S3-based data lake hay trong data warehouse system như \u003cstrong\u003eAmazon Redshift\u003c/strong\u003e.\u003c/p\u003e","tags":[],"title":"Amazon Database Migration Service"},{"content":"","date":"2023-09-07","id":8,"permalink":"/de-docs/de-basic/","summary":"","tags":[],"title":"Data Engineer Cơ Bản"},{"content":"\rDatabases và Data Warehouses Có 2 loại hệ thống cơ sở dữ liệu, và cả 2 đều được sử dụng cho rất nhiều năm:\nOnline Transaction Processing (OLTP): Là hệ thống được sử dụng với mục đích chính là lưu trữ và cập nhật những transactional data (Dữ liệu giao dịch) với khối lượng lớn. Ví dụ: Loại OLTP databases thường được sử dụng để lưu trữ customer records (hồ sơ khách hàng), bao gồm chi tiết giao dịch như việc mua hàng, trả hàng, hoàn tiền,\u0026hellip; Online Analytical Processing (OLAP): Là hệ thống được sử dụng với mục đích chính là xây dựng báo cáo trên khối lượng lớn dữ liệu. Ta thường thấy hệ thống OLAP sẽ lấy dữ liệu từ các OLTP databases, và cung cấp một kho lưu trữ chung cho thể sử dụng cho mục đích báo cáo. Vào những năm 90s, các công ty lớn có hàng chục hay hàng trăm databases thường là OLTP, và khả năng để phân tích thông qua các hệ thống đó hạn chế.\nSau đó kết quả là, Data Warehouses (OLAP systems) được phổ biến là hệ thống có thể tích hợp nhiều database systems vào một kho lưu trữ chung, tập trung vào việc phân tích.\nData Warehouse được thiết kế để lưu trữ dữ liệu được tích hợp, quản lý và độ tin cậy cao, rất có cấu trúc (well-defined schema)\nDealing with big, unstructured data Trong khi Data Warehouses truyền thống tốt trong việc lưu trữ và quản lý dữ liệu cấu trúc dạng bảng, tổ chức dưới các lược đồ quan hệ. Nhưng nó lại không phù hợp để xử lý một lượng lớn semi-structured và unstructured data đang dẫn trở nên phổ biến.\nSau đó, vào năm 2010, một công cụ xử lý dữ liệu lớn trở nên phổ biến. Hadoop là một framework mã nguồn mở để xử lý lượng lớn datasets trên các cụm máy tính, đi đầu trong việc xử lý big data. Các cụm bao gồm hằng chục, hàng trăm máy móc được gắn với ổ cứng có thể lưu trữ hàng ngàn terabytes dữ liệu được quản lý dưới một distributed filesystem hay còn gọi là Hadoop Distributed File System (HDFS).\nNhưng để cài đặt Hadoop clusters thường cần lượng lớn tiền đầu tư ban đầu cho máy móc và ổ cứng. Và cần các team chuyên biệt để xử lý những yêu cầu bao gồm:\nHadoop administrators specialized cho cụm, phần cứng và phần mềm Data Engineer specialized trong việc dùng các framework xử lý dữ liệu lớn như Spark, Hive, và Presto Cloud-based solutions for big data analytic Dựa vào những yêu cầu cao đó, nhiều tổ chức đã sử dụng cloud là một giải pháp để xử lý big data, với các lợi ích:\nOn-demand capacity (Năng lực theo yêu cầu) Limitless and elastic scaling (Không hạn chế và mở rộng tùy ý) Global footprint (Có thể đặt trên toàn cầu) Usage-based cost models (Mô hình chi phí theo những gì sử dụng) Freedom from managing hardware (Tự do quản lý phần cứng) Các công ty lớn như Amazon Web Services (AWS), Google Cloud Platform (GCP), Microsoft Azure, Snowflake và Databricks cung cấp rất nhiều giải pháp để tích hợp, lưu trữ và phân tích lượng lớn dữ liệu trên cloud.\nVới việc lưu trữ trên cloud objects stores đã cho phép tổ chức xây dựng một cách quản lý data mới có thể lưu trữ mọi data (structured, semi-structured, unstructured) và có thể scale mọi kích cỡ, được gọi là data lake. Với phương pháp này, tất cả dữ liệu được tích hợp vào, xử lý và lưu trữ trong data lake, sau đó những dữ liệu cần độ trễ thấp (lower-latency) sẽ được load vào cloud data warehouse.\nTrong những năm gần đây, một khai niệm mới được được đặt ra có thể tích hợp được tối ưu khả năng của cả data lakes và data warehousing, được gọi là data lakehouse, đã có những giải pháp từ các công ty như (AWS, GCP, Snowflake và Databricks), cũng như là các công cụ mã nguồn mở (Apache Hudi và Apache Iceberg) cũng đề cập đến data lakehouse. Với các khả năng:\nTích hợp nhanh chóng mọi loại dữ liêu. Lưu trữ và xử lý nhiều petabytes cho dữ liệu unstructured, semi-structured và structured Hỗ trợ ACID (đề cập đến 4 thuộc tính của một giao dịch - là atomicity (tính nguyên tử), consistency (tính nhất quán), isolation (tính độc lập) và durability (độ bền cao) cho phép nhiều người dùng đồng thời read, insert, update, và delete các hàng được quản lý bởi data lakehouse) Độ trễ truy cập dữ liệu thấp. Khả năng tiêu thụ dữ liệu với nhiều tool, bao gồm SQL, Spark, machine learning và BI tools. ","date":"2023-09-07","id":9,"permalink":"/de-docs/de-basic/2-data-management-architecture/","summary":"\u003cstyle\u003ebody {text-align: justify}\u003c/style\u003e\r\n\u003ch3 id=\"databases-và-data-warehouses\"\u003eDatabases và Data Warehouses\u003c/h3\u003e\n\u003cp\u003eCó 2 loại hệ thống cơ sở dữ liệu, và cả 2 đều được sử dụng cho rất nhiều năm:\u003c/p\u003e","tags":[],"title":"Data Management Architectures"},{"content":"\rChúng ta sẽ tìm hiểu kiến trúc của cụm Amazon Redshift. Có rất nhiều loại node khác nhau trong Redshift, trong hình duới đây sẽ là cụm dựa trên RA3 nodes.\nNhư hình trên, cụm Amazon Redshift bao gồm leader node và một hay nhiều compute node:\nleader node giao tiếp với ứng dụng client, nhận và phân tích truy vấn, và điều phối thực thi câu truy vấn đến compute node. Các computer nodes có hiệu suất cao để lưu trữ một phần con của Data Warehouse và thực thi câu truy vấn song song trên dữ liệu mà nó chứa. Đối với các RA3 node, Amazon S3 được sử dụng như là Redshift Managed Storage (RMS) cho dữ liệu warehouse, và local storage của compute node được dùng để lưu trữ cache cho \u0026ldquo;hot\u0026rdquo; data. Mỗi compute node có tài nguyên riêng như processors, memory, và high-performance storage tách biệt với các compute node khác, còn được gọi là shared-nothing architecture.\nVà data warehouse trên cloud triển khai cấu trúc truy vấn phân tán song song hay còn gọi là Massively Parallel Processing (MPP).\n","date":"2023-09-07","id":10,"permalink":"/de-docs/de-basic/datawarehouse/dtamsp/","summary":"\u003cstyle\u003ebody {text-align: justify}\u003c/style\u003e\r\n\u003cp\u003eChúng ta sẽ tìm hiểu kiến trúc của cụm \u003cstrong\u003eAmazon Redshift\u003c/strong\u003e. Có rất nhiều loại node khác nhau trong Redshift, trong hình duới đây sẽ là cụm dựa trên \u003cstrong\u003eRA3 nodes\u003c/strong\u003e.\u003c/p\u003e","tags":[],"title":"Kho dữ liệu phân tán và xử lý dữ liệu song song"},{"content":"\rAmazon Kinesis là một dịch vụ được quản lý để đơn giản hóa quy trình ingesting và xử lý streaming data trong thời gian thực, hoặc gần thực. Có nhiều trường hợp Kinesis có thể sử dụng, bao gồm ingestion dữ liệu streaming (như log files, website clickstreams, hay IoT data), cũng như là video và audio streams.\nDựa vào từng trường hợp cụ thể, có nhiều dịch vụ khác nhau bạn có thể chọn là một phần của dịch vụ Kinesis. Dưới đây là một số cái nhìn chung về các dịch vụ có trong Amazon Kinesis:\nKinesis Data Firehose: Ingests streaming data, buffers (vùng nhớ tạm thời để lưu trữ dữ liệu trong quá trình truyền tải giữa các thành phần khác nhau của hệ thống) cho 1 khoảng thời gian có thể cấu hình, sau đó truyền tải dữ liệu đến một số đích nhất định (như S3, Redshift, OpenSearch Service, Splunk, \u0026hellip;) Kinesis Data Streams: Ingests real-time data streams, xử lý dữ liệu đến với custom application và low latency. Kinesis Data Analytics: Đọc dữ liệu từ streaming source và sử dụng SQL hay Apache Flink code để thực hiện phân tích trực tiếp trong luồng dữ liệu đó. Kinesis Video Streams: Xử lý streaming video hoặc audio streams, cũng như là dữ liệu được sắp xếp theo thời gian như hình ảnh nhiệt và dữ liệu RADAR. Amazon Kinesis Agent AWS cung cấp Kinesis Agent để dễ dàng tiêu thụ dữ liệu từ một hệ thống và ghi dữ liệu đó vào 1 luồng stream hoặc Kinesis Data Streams hoặc Kinesis Data Firehose.\nAgent có thể cấu hình để giám sát một nhóm các tệp, khi dữ liệu mới được ghi vào file, Agent sẽ lưu trữ dữ liệu trong buffers - bộ nhớ đệm (thời gian lưu trữ có thể tùy chỉnh trong khoảng 1s đến 15m) và ghi dữ liệu vào Kinesis Data Streams hoặc Kinesis Data Firehose. Agent tự động xử lý việc thử lại khi gặp lỗi truyền dữ liệu, đồng thời quản lý xoay vòng tệp và thực hiện kiểm tra điểm để đảm bảo tính toàn vẹn dữ liệu.\nMột hoàn cảnh sử dụng thường thấy là khi bạn muốn phân tích sự kiện xảy ra trên website của bạn với gần thời gian thực. Kinesis Agent có thể cấu hình để giám sát Apache web server log files trên website của bạn, chuyển nó thành JSON format và ghi các bản ghi phản ánh hoạt động của trang web mỗi 30s vào Kinesis, nơi Kinesis Data Analytics có thể được sử dụng để phân tích các sự kiện và tạo ra số liệu tùy chỉnh dựa trên cửa sổ trượt 5 phút (có nghĩa là dữ liệu được phân tích theo các khoảng thời gian 5 phút liên tiếp, không chồng chéo. Ví dụ: 0-5 phút, 5-10 phút, 10-15 phút, v.v.)\nWhen to use: Khi bạn muốn truyền dữ liệu luồng đến Kinesis mà dữ liệu đó đang được ghi vào một tệp trong một quy trình riêng biệt (vd: log files)\nWhen not to use: Nếu bạn có ứng dụng phát ra luồng dữ liệu trực tuyến (vd mobile application hay IoT device), bạn nên xem xét sử dụng Amazon Kinesis Producer Library (KPL), hoặc AWS SDK, để tích hợp việc gửi thẳng dữ liệu trực tiếp từ ứng dụng của bạn.\nAmazon Kinesis Firehose Amazon Kinesis Firehose được thiết kế để cho phép bạn dễ dàng tích họ dữ liệu gần thời gian thực từ nguồn streaming và ghi vào target, bao gồm Amazon S3, Redshift, OpenSearch Service, cũng như là nhiều dịch vụ third-party (như Splunk, Datadog và New Relic).\nMột trường hợp thường thấy cho mục đích data engineering là tích hơ webiste clickstream từ Apache web logs trên web server và ghi vào S3 data lake (hoặc Redshift data warehouse). Trong trường hợp này, bạn có thể cài đặt Kinesis Agent trên web server và cấu hình nó để giám sát Apache web server log files. Dựa trên cấu hình agent, theo lịch trình thường xuyên, agent sẽ ghi các record từ log files vào Kinesis Firehose endpoint.\nKinesis Firehose endpoint lưu các record trong bọ nhớ đệm, và sau khoảng thời gian cụ thể (1-15 phút) hoặc dựa trên kích thước của record (1MB - 128MB), nó sẽ ghi lại dữ liệu vào target cụ thể. Kinesis Firehose yêu cầu bạn xác định cụ thể size và time limit, và khi đạt được điều kiện đầu tin thỏa mãn sẽ kích hoạt việc ghi file.\nKhi ghi file vào Amazon S3, bạn có thể chọn data được chuyển đổi dưới dạng Parquet hoặc ORC hoặc chuyển đổi tùy chỉnh sử dụng hàm Amazon Lambda. Kinesis Data Firehouse cũng hỗ trợ dynamic partitioning, cho phép bạn xác định, cấu hình phân vùng tùy chọn.\nWhen to use: Lý tưởng khi bạn muốn nhận luồng dữ liệu trực tiếp, lưu lại bộ đệm trong một khoảng thời gian, và ghi dữ liệu vào target hỗ trợ bởi Kinesis Firehose.\nWhen not to use: Nếu trường hợp bạn cần độ trễ rất thấp cho luồng dữ liệu trực tiếp hoặc bạn muốn sử dụng ứng dụng tùy chỉnh để xử lý dữ liệu đến hoặc chuyên giao record không được hỗ trợ bởi Kinesis Firehose, bạn có thể xem xét sử dụng Amazon Kinesis Data Streams hoặc Amazon Managed Streaming for Apache Kafka (MSK)\nXem thêm tại: https://aws.amazon.com/firehose/\nAmazon Kinesis Data Streams Kinesis Data Streams cung cấp tính linh hoạt cao cho cách dữ liệu được tiêu thụ và làm cho data đến có sẵn trong ứng dụng streaming của bạn với độ trễ cực thấp (AWS chỉ ra dữ liệu có thể được tiêu thụ trong phạm vi chỉ trong 70 mili giây dữ liệu được ghi vào kinesis)\nBạn có thể ghi vào Kinesis Data Streams sử dụng Kinesis Agent, hoặc bạn có thể phát triển ứng dụng của bạn sử dụng AWS SDK hoặc Amazon KPL, thư viện đơn giản hóa việc ghi lại dữ liệu với thông lượng cao vào Kinesis data stream.\nCó nhiều cách để tạo ứng dụng đọc dữ liệu từ Kinesis data stream, bao gồm:\nSử dụng dịch vụ Kinesis khác (như Kinesis Firehose hay Kinesis Data Analytics) Chạy code tùy chỉnh sử dụng AWS Lambda (môi trường serverless để chạy code không cần cung cấp và quản lý server) Cài đặt cụm Amazon EC2 servers để xử lý luồng của bạn. Bạn có thể sử dụng KCL để xử lý nhiệm vụ phức tạp liên kết với nhiều servers để xử lý, như load balancing, phản hồi khi lỗi, checkpointing records đã được dữ lý, và phản ứng với resharding - phân đoạn lại (tăng hoặc giảm số lượng phân đoạn để xử lý streaming data). ","date":"2023-09-07","id":11,"permalink":"/de-docs/de-basic/ingestion-tool/amazon-kinesis/","summary":"\u003cstyle\u003ebody {text-align: justify}\u003c/style\u003e\r\n\u003cp\u003e\u003cstrong\u003eAmazon Kinesis\u003c/strong\u003e là một dịch vụ được quản lý để đơn giản hóa quy trình ingesting và xử lý streaming data trong thời gian thực, hoặc gần thực.\nCó nhiều trường hợp Kinesis có thể sử dụng, bao gồm ingestion dữ liệu streaming (như log files, website clickstreams, hay IoT data), cũng như là video và audio streams.\u003c/p\u003e","tags":[],"title":"Amazon Kinesis for streaming data ingestion"},{"content":"\rColumnar Data Storage Một modern data warehouse còn được tăng hiệu suất thông qua việc lưu trữ column-oriented (hướng cột) và data compression (nén dữ liệu).\nMột ứng dụng OLTP thường làm việc với tất cả các hàng, bao gồm tất cả các cột (để đọc ghi nhanh chóng) thì backend database phải đọc và ghi tất cả các hàng trong ổ đĩa. OLTP database sử dụng row-oriented để lưu trữ các hàng của bảng trong ổ đĩa.\nNhưng trong phân tích dữ liệu, người phân tích thường sử dụng những câu hỏi về nhóm (grouping) và tổng hợp (aggregations) trên một lượng hàng lớn. Điều này khiến câu truy vấn phải quét toàn bộ các hàng nhưng dữ liệu chúng ta cần chỉ trong những cột cụ thể. Khiến cho việc câu truy vấn cần lượng disk I/O operations nhiều hơn cần thiết.\nModern data warehouses lưu trữ dữ liệu trong disk dưới dạng column-oriented. Phù hợp với những câu truy vấn phân tích chỉ cần một phần cột nhỏ trong bảng. Khi lưu trữ dữ liệu dưới dạng column-oriented, data warehouse sẽ chia nhỏ bảng thành các nhóm rows, gọi là row chunk/groups. Nó lưu trữ trong disk như sau:\nData warehouse cũng đồng thời giám sát địa chỉ của những chunk này trong bộ nhớ. Modern data warehouse sử dụng những địa chỉ này để xác định vị trí của cột trên đĩa và đọc các giá trị vị trí vật lý của cột. Bằng cách này, disk I/O được giảm thiểu đáng kể khi so sánh cùng câu truy vấn so với row-oriented.\nData Compression Ngoài ra, modern data warehouse còn triển khai nhiều thuật toán nén (compression algorithms) cho một bảng. Data warehouse có thể kết hợp các cột với thuật toán nén phù hợp với kiểu dữ liệu và đặc điểm của cột đó. Thuật toán nén hoạt động hiệu quả khi các giá trị được nén có cùng kiểu dữ liệu và tỉ lệ trùng lặp cao.\nMục đích: Nén dữ liệu giúp giảm dung lượng lưu trữ cần thiết và giảm thiểu số lượng thao tác đọc ghi đĩa (disk I/O). Làm tăng tốc độ truy vấn và giảm thiểu chi phí.\nĐồng thời viết kết hợp với column-oriented, sắp xếp các giá trị của cùng một cột (cùng kiểu dữ liệu) với nhau, data warehouse đạt được tỉ lệ nén tốt hơn , dẫn đến đọc ghi nhanh hơn và giảm thiểu dung lượng lưu trữ.\n","date":"2023-09-07","id":12,"permalink":"/de-docs/de-basic/datawarehouse/columnar-data-storage/","summary":"\u003cstyle\u003ebody {text-align: justify}\u003c/style\u003e\r\n\u003ch3 id=\"columnar-data-storage\"\u003eColumnar Data Storage\u003c/h3\u003e\n\u003cp\u003eMột modern data warehouse còn được tăng hiệu suất thông qua việc lưu trữ \u003cstrong\u003ecolumn-oriented (hướng cột)\u003c/strong\u003e và \u003cstrong\u003edata compression (nén dữ liệu)\u003c/strong\u003e.\u003c/p\u003e","tags":[],"title":"Columnar Data Storage And Data Compression"},{"content":"","date":"2023-09-07","id":13,"permalink":"/de-docs/de-basic/datawarehouse/","summary":"","tags":[],"title":"Data Warehouse"},{"content":"","date":"2023-09-07","id":14,"permalink":"/de-docs/spark/","summary":"","tags":[],"title":"Mở đầu về Spark"},{"content":"","date":"2023-09-07","id":15,"permalink":"/de-docs/de-basic/datalake/","summary":"","tags":[],"title":"Data Lake"},{"content":"\rCác tổ chức thường xây dựng data pipelines của họ như sau:\nTrích xuất dữ liệu từ các hệ thống nguồn. Chuyển đổi dữ liệu bằng cách kiểm tra, làm sạch, tiêu chuẩn hóa, và chọn lọc nó. Tải những dữ liệu được chuyển đổi đến lược đồ của enterprise data warehouse, và thường có cả data marts. Trong pipeline này, bước đầu tiên là Extract dữ liệu từ nguồn, nhưng 2 bước tiếp theo có thể là Transform-Load hoặc là Load-Transform được gọi là ETL hoặc ELT.\nViệc lựa chọn xây dựng ETL hay ELT dựa trên những nguyên nhân:\nĐộ phức tạp của công việc data transformations. Các kỹ năng và công cụ mà tổ chức có để xây dựng bước data transformations. Tốc độ mà dữ liệu nguồn có thể được phân tích trong data warehouse sau khi mà dữ liệu được sản xuất. Với ETL pipeline, công đoạn transformations được thực hiện bên ngài data warehouse sử dụng đonạ code tùy chỉnh, sử dụng ETL service từ cloud như AWS Glue, hoặc ETL tool từ các nhà cung cấp thương mại như Informatica, Talend, DataStage, Microsoft hay Pentaho.\nVí dụ một ETL pipeline có thể bao gồm các công đoạn:\nMột hay nhiều hệ thống trích xuất dữ liệu từ nhiều nguồn (databases, SaaS solutions, file storage,\u0026hellip;) và ghi dữ liệu vào khu vực raw/staging storage. Một hay nhiều transformations đọc dữ liệu từ khu vực raw/staging storage, chuyển đổi dữ liệu, và ghi nó vào trong khu vực transformed storage. Và hệ thống đọc các dữ liệu từ khu vực transformed storage và tải nó vào trong data warehouse. Một cách tiếp cận xây dựng ETL pipeline thường sử dụng khi thỏa mãn những điều kiện dưới:\nSource databases và formats khác với data warehouse. Engineering team muốn thực hiện transformations sử dụng ngôn ngữ lập trình (vd: PySpark) hơn là SQL thông thường. Data Transformations phức tạp và tính toán chuyên sâu. Ngược lại, một ELT pipeline lấy dữ liệu (thường là có cấu trúc cao) từ nhiều nguồn và load vào staging area của data warehouse. Và sau đó hoạt động transformations được lấy từ staging và ghi vào khực production (sẵn sàng để sử dụng)\nCách tiếp cận này cho phép tải nhanh chóng số lượng lớn data từ source vào data warehouse. Hơn nữa, kiến trúc MPP có thể cải thiện đáng kế tốc độ thực hiện transformations trong ELT pipelines. Cách tiếp cận này thường được tận dụng khi thỏa mãn các điều kiện sau:\nNguồn data và warehouse có cùng công nghệ database, khiến nó dễ dàng load trực tiếp từ source vào khu vực staging trong warehouse. Một lượng lớn dữ liệu cần được load nhanh chóng vào warehouse Tất cả các yêu cầu về transformations có thể thực hiện bằng cách thực thi những cầu lệnh SQL trong engine của warehouse\u0026rsquo;s database Sự khác biệt chính giữa ETL và ELT, nằm ở công đoạn transformations. Với ELT, dữ liệu được load trực tiếp vào warehouse, và cần phải sử dụng data warehouse engine để thực hiện transformation (thường là SQL). Còn với ETL, một engine bên ngoài data warehouse sẽ transformations dữ liệu rồi mới load vào data warehouse.\n","date":"2023-09-07","id":16,"permalink":"/de-docs/de-basic/datawarehouse/etl-elt-pipelines/","summary":"\u003cstyle\u003ebody {text-align: justify}\u003c/style\u003e\r\n\u003cp\u003eCác tổ chức thường xây dựng data pipelines của họ như sau:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eTrích xuất dữ liệu từ các hệ thống nguồn.\u003c/li\u003e\n\u003cli\u003eChuyển đổi dữ liệu bằng cách kiểm tra, làm sạch, tiêu chuẩn hóa, và chọn lọc nó.\u003c/li\u003e\n\u003cli\u003eTải những dữ liệu được chuyển đổi đến lược đồ của enterprise data warehouse, và thường có cả data marts.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTrong pipeline này, bước đầu tiên là \u003cstrong\u003eExtract\u003c/strong\u003e dữ liệu từ nguồn, nhưng 2 bước tiếp theo có thể là \u003cstrong\u003eTransform\u003c/strong\u003e-\u003cstrong\u003eLoad\u003c/strong\u003e hoặc là \u003cstrong\u003eLoad-Transform\u003c/strong\u003e được gọi là \u003cstrong\u003eETL\u003c/strong\u003e hoặc \u003cstrong\u003eELT\u003c/strong\u003e.\u003c/p\u003e","tags":[],"title":"ETL and ELT pipelines"},{"content":"","date":"2023-09-07","id":17,"permalink":"/de-docs/kafka/","summary":"","tags":[],"title":"Mở đầu về Kafka"},{"content":"","date":"2023-09-07","id":18,"permalink":"/de-docs/de-basic/datalakehouse/","summary":"","tags":[],"title":"Data Lakehouse"},{"content":"","date":"2023-09-07","id":19,"permalink":"/de-docs/de-basic/ingestion-tool/","summary":"","tags":[],"title":"AWS for Ingesting Data"},{"content":"Well-thought-through product announcements will help increase feature awareness and engage users with new functionality. Just like sharing your public roadmap, it\u0026rsquo;s also a great way to let potential customers see that you\u0026rsquo;re constantly improving.\nFurther reading Read How to announce product updates and features ","date":"2025-01-20","id":20,"permalink":"/blog/example-post/","summary":"You can use blog posts for announcing product updates and features.","tags":[],"title":"Example Post"},{"content":"","date":"2023-09-07","id":21,"permalink":"/blog/","summary":"","tags":[],"title":"Blog"},{"content":"","date":"2023-09-07","id":22,"permalink":"/docs/overview/","summary":"","tags":[],"title":"Giới thiệu"},{"content":"\rSpark là gì? Apache Spark là một hệ thống xử lý dữ liệu phân tán mã nguồn mở và được sử dụng cho các công việc xử lý dữ liệu lớn. Có thể sử dụng trên: on premises hay trên cloud. Spark sử dụng khả năng lưu trữ và xử lý dữ liệu trên bộ nhớ thay vì sử dụng ổ cứng (HDD/SSD). Làm tăng khả năng xử lý dữ liệu, khả năng xử lý nhanh hơn Hadoop MapReduce. Apache Spark cung cấp API phát triển bằng ngôn ngữ Python, Scala, Java. Có thể sử dụng cho việc: Machine Learning (MLlib). Truy vấn, tương tác với dữ liệu (Spark SQL). Xử lý dữ liệu thời gian thực (Structured Streaming). Xử lý đồ thị (GraphX). Spark được thiết kế xung quanh 4 triết lý chính: Tốc độ (Speed) Dễ sử dụng (Ease of use) Tính mô đun (Modularity) Khả năng mở rộng (Extensibility) Spark có cộng đồng lớn và một danh sách các gói third-party Spark là một phần của sự phát triển của cộng đồng. ","date":"2023-09-07","id":23,"permalink":"/de-docs/spark/khai-niem/","summary":"\u003cstyle\u003ebody {text-align: justify}\u003c/style\u003e\r\n\u003ch3 id=\"spark-là-gì\"\u003eSpark là gì?\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cspan style=\"color: orange; font-weight:bold;\"\u003eApache Spark\u003c/span\u003e là một hệ thống xử lý dữ liệu phân tán mã nguồn mở và được sử dụng cho các công việc xử lý dữ liệu lớn.\u003c/li\u003e\n\u003cli\u003eCó thể sử dụng trên: \u003cspan style=\"color: orange; font-weight:bold;\"\u003eon premises\u003c/span\u003e hay \u003cspan style=\"color: orange; font-weight:bold;\"\u003etrên cloud\u003c/span\u003e.\u003c/li\u003e\n\u003cli\u003eSpark sử dụng khả năng lưu trữ và xử lý dữ liệu trên bộ nhớ thay vì sử dụng ổ cứng (HDD/SSD).\n\u003cul\u003e\n\u003cli\u003eLàm tăng khả năng xử lý dữ liệu, khả năng xử lý nhanh hơn Hadoop MapReduce.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cspan style=\"color: orange; font-weight:bold;\"\u003eApache Spark\u003c/span\u003e cung cấp API phát triển bằng ngôn ngữ Python, Scala, Java. Có thể sử dụng cho việc:\n\u003cul\u003e\n\u003cli\u003eMachine Learning (MLlib).\u003c/li\u003e\n\u003cli\u003eTruy vấn, tương tác với dữ liệu (Spark SQL).\u003c/li\u003e\n\u003cli\u003eXử lý dữ liệu thời gian thực (Structured Streaming).\u003c/li\u003e\n\u003cli\u003eXử lý đồ thị (GraphX).\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eSpark được thiết kế xung quanh 4 triết lý chính:\n\u003cul\u003e\n\u003cli\u003eTốc độ (Speed)\u003c/li\u003e\n\u003cli\u003eDễ sử dụng (Ease of use)\u003c/li\u003e\n\u003cli\u003eTính mô đun (Modularity)\u003c/li\u003e\n\u003cli\u003eKhả năng mở rộng (Extensibility)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eSpark có cộng đồng lớn và một danh sách các gói third-party Spark là một phần của sự phát triển của cộng đồng.\n\u003cbr\u003e \u003cbr\u003e\n\r\n\r\n\u003cimg\r\n  src=\"/de-docs/spark/images/spark-third-party_hu_842eb3a923965df8.webp\"\r\n  width=\"940\"\r\n  height=\"701\"\r\n  decoding=\"async\"\r\n  fetchpriority=\"auto\"\r\n  loading=\"lazy\"\r\n  alt=\"spark-third-party\"id=\"h-rh-i-0\"\r\n/\u003e\u003c/li\u003e\n\u003c/ul\u003e","tags":[],"title":"1. Giới thiệu về Spark"},{"content":"\rSpark bao gồm 5 thành phần chính: Spark Core, Spark SQL, Spark Streaming, MLlib và GraphX.\nSpark SQL Như tên gọi của nó, thì mô-đun này làm việc tốt với structured data. Có thể đọc được dữ liệu lưu trữ trong bảng RDBMS hay các file có định dạng lưu trữ có cấu trúc như (csv, text, JSON, Avro, ORC, Parquet, \u0026hellip;). Khi sử dụng Spark qua API bằng Java, Python, Scala hay R. Bạn có thể sử dụng lệnh SQL để truy vấn dữ liệu. Ví dụ: Bạn có thể đọc file JSON lưu trữ trong Amazon S3, tạo một bảng tạm thời, và sử dụng lệnh SQL để đọc dữ liệu.\n// In Scala // Đọc dữ liệu từ S3 Bucket vào Spark DataFrame spark.read.json(\u0026#34;s3://apache_spark/data/committers.json\u0026#34;) .createOrReplaceTempView(\u0026#34;committers\u0026#34;) // Sử dụng SQL để trả về kết quả là một Spark DataFrame val results = spark.sql(\u0026#34;\u0026#34;\u0026#34;SELECT name, org, module, release, num_commits FROM committers WHERE module = \u0026#39;mllib\u0026#39; AND num_commits \u0026gt; 10 ORDER BY num_commits DESC\u0026#34;\u0026#34;\u0026#34;)\rBạn có thể viết một đoạn code như vậy với Python, R hay Java. Các mã byte sẽ được tạo giống hệt nhau, trả ra kết quả với hiệu suất tương tự.\nSpark MLlib Spark mang đến một thư viện các thuật toán Machine Learning phổ biến gọi là MLlib. Với hiệu suất được cải thiện đáng kể từ khi lần đầu ra mắt thư viện này. API này cho phép trích xuất hay chuyển đổi tính năng, xây dựng pipelines (cho việc huấn luyện và đánh giá), và persist models (cho việc saving và reloading models) trong quá trình triển khai. Thêm vào đó, có các tiện ích bổ sung bao gồm sử dụng các phép toán đại số tuyến tính và thống kê phổ biến. Ví dụ: Dưới đây là một đoạn code python bao gồm những hoạt động cơ bản của một data scientist sẽ làm khi xây một mô hình:\n# In Python from pyspark.ml.classification import LogisticRegression ... training = spark.read.csv(\u0026#34;s3://...\u0026#34;) test = spark.read.csv(\u0026#34;s3://...\u0026#34;) # Load training data lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8) # Fit the model lrModel = lr.fit(training) # Predict lrModel.transform(test) ...\rSpark Structured Streaming Được xây dựng dựa trên Spark SQL engine và DataFrame-based APIs. Từ phiên bản Spark 2.2, Structured Straming có thể được sử dụng cả trên môi trường production. Spark Structured Streaming cần thiết cho Big Data Developers để kết hợp và phản ứng với dữ liệu tại thời gian thực từ các nguồn như Apache Kafka, hay các nguồn khác. Developers có thể coi như đây là một bảng dữ liệu có cấu trúc và xử lý, truy vấn như một bảng tĩnh. Ví dụ: Dưới đây là một đoạn code mẫu về Spark Structured Streaming\n# In Python # Đọc luồng dữ liệu từ local host from pyspark.sql.functions import explode, split lines = (spark .readStream .format(\u0026#34;socket\u0026#34;) .option(\u0026#34;host\u0026#34;, \u0026#34;localhost\u0026#34;) .option(\u0026#34;port\u0026#34;, 9999) .load()) # Thực hiện việc chuyển đổi # Chia các dòng thành các từ words = lines.select(explode(split(lines.value, \u0026#34; \u0026#34;))).alias(\u0026#34;word\u0026#34;) # Tạo bộ đếm từ word_counts = words.groupBy(\u0026#34;word\u0026#34;).count() # Ghi vào Kafka query = (word_counts .writeStream .format(\u0026#34;kafka\u0026#34;) .option(\u0026#34;topic\u0026#34;, \u0026#34;output\u0026#34;))\rGraphX Như tên gọi, GraphX là một thư viện để thao tác với đồ thị (ví dụ: đồ thị mạng xã hội, đồ thị network,\u0026hellip;). GraphX cung cấp thuật toán đồ thị tiêu chuẩn cho việc phân tích, kết nối và truyền tải được đóng góp bởi cộng đồng như: PageRank, Connected Components và Triangle Counting. Ví dụ: Đoạn code dưới đây là một ví dụ về việc join 2 graph sử dụng GraphX APIs.\n// In Scala val graph = Graph(verticles, edges) messages = spark.textFile(\u0026#34;hdfs://...\u0026#34;) val graph2 = graph.joinVertices(messages) { (id, vertex, msg) =\u0026gt; ... }\r","date":"2023-09-07","id":24,"permalink":"/de-docs/spark/thanh-phan-cua-spark/","summary":"\u003cstyle\u003ebody {text-align: justify}\u003c/style\u003e\r\n\u003cp\u003e\u003cbr\u003e\n\r\n\r\n\u003cimg\r\n  src=\"/de-docs/spark/images/spark-components_hu_15d87a9c5be03bb3.webp\"\r\n  width=\"1314\"\r\n  height=\"391\"\r\n  decoding=\"async\"\r\n  fetchpriority=\"auto\"\r\n  loading=\"lazy\"\r\n  alt=\"Spark Components\"id=\"h-rh-i-0\"\r\n/\u003e\u003c/p\u003e\n\u003cp\u003eSpark bao gồm 5 thành phần chính: Spark Core, Spark SQL, Spark Streaming, MLlib và GraphX.\u003c/p\u003e","tags":[],"title":"2. Thành phần của Spark"},{"content":"\rHãy bắt đầu bằng cách nhìn từng thành phần của Spark trong ảnh. Spark Driver Là một phần của Spark application chịu trách nhiệm cho việc khởi tạo SparkSession. Spark Driver có nhiều vai trò như:\nGiao tiếp với cluster manager. Gửi yêu cầu sử dụng tài nguyên (CPU, memory,\u0026hellip;) từ cluster manager cho Spark\u0026rsquo;s Executor (JVMs). Chuyển đổi spark operations thành DAG computations, schedules (tính toán, lên lịch). Spark Session Kể tử Spark 2.0, SparkSession được coi như là cách kết nối thống nhất đến tất cả các hoạt động của Spark và dữ liệu. Gộp các entrypoint cũ như SparkContext, SQLContext, HiveContext, SparkConf và StreamingContext. Khiến việc sử dụng Spark trở nên dễ dàng hơn. Tức là kể từ giờ SparkSession là entrypoint cho tất cả các tính năng của Spark. Ví dụ: Dưới đây là một ví dụ về việc sử dụng SparkSession và thực hiện một số hoạt động truy vấn dữ liệu.\n// In Scala import org.apache.spark.sql.SparkSession // Build SparkSession val spark = SparkSession .builder .appName(\u0026#34;LearnSpark\u0026#34;) .config(\u0026#34;spark.sql.shuffle.partitions\u0026#34;, 6) .getOrCreate() // Sử dụng session để đọc file JSON val people = spark.read.json(\u0026#34;...\u0026#34;) // Sử dụng sesion để truy vấn một query và lưu thành DataFrame val resultsDF = spark.sql(\u0026#34;SELECT city, pop, state, zip FROM table_name\u0026#34;)\rCluster Manager Cluster Manager chịu trách nhiệm cho việc quản lý và phân bố tài nguyên cho cụm các node cho Spark. Hiện tại, Spark hỗ trợ 4 cluster managers:\ncluster managers được tích hợp sẵn trong Spark. Apache Hadoop YARN Apache Mesos Kubernetes Spark Executor Spark Executor chạy trên từng worker node trong cụm. Spark Executor giao tiếp với Spark Driver và chịu trách nhiệm cho việc thực thi các nhiệm vụ trên các worker. Trong hầu hết các chế độ triển khai (Deployment modes), chỉ có một executor trên một node.\nDeployment modes Hỗ trợ nhiều chế độ triển khai, giúp Spark chạy trên các configurations và environments khác nhau. Một số môi trường phổ biến như Apache Hadoop Yarn và Kubernetes và có thể hoạt động trên nhiều chế độ.\nDistributed data and partitions Dữ liệu vật lý thực tế được phân phối trên bộ lưu trữ dưới dạng các phân vùng (partitions) nằm trong HDFS hay cloud storage. Trong khi dữ liệu được phân phối thành các partitions trên các cụm vật lý (physical cluster). Spark xử lý từng partition dưới dạng trừu tượng dữ liệu logic cấp cao - như một DataFrame trong bộ nhớ.\nViệc phân vùng dữ liệu giúp cho hiệu suất xử lý song song hiệu quả. Việc này giúp cho Spark Executor chỉ xử lý dữ liệu gần với chúng nhất, giảm thiểu băng thông mạng.\nVí dụ: Đoạn code sau đây sẽ chuyển đổi dữ liệu vật lý lưu trên clusters thành 8 partition, và từng executor sẽ có một hoặc nhiều partitions để đọc vào memory.\n# In Python log_df = spark.read.text(\u0026#34;path_to_large_text_file\u0026#34;).repartition(8) print(log_df.rdd.getNumPartitions())\rVí dụ 2: Đoạn code dưới đây sẽ tạo một DataFrame với 10000 số nguyên phân bố trên 8 partitions trong bộ nhớ:\n# In Python df = spark.range(0, 10000, 1, 8) print(df.rđ.getNumPartitions())\r2 ví dụ trên đều có output là 8.\n","date":"2023-09-07","id":25,"permalink":"/de-docs/spark/cach-hoat-dong/","summary":"\u003cstyle\u003ebody {text-align: justify}\u003c/style\u003e\r\n\u003cp\u003eHãy bắt đầu bằng cách nhìn từng thành phần của Spark trong ảnh.\n\u003cbr\u003e\n\u003cbr\u003e\n\r\n\r\n\u003cimg\r\n  src=\"/de-docs/spark/images/architecture_hu_3d09a1ec182b9465.webp\"\r\n  width=\"724\"\r\n  height=\"372\"\r\n  decoding=\"async\"\r\n  fetchpriority=\"auto\"\r\n  loading=\"lazy\"\r\n  alt=\"Architecture\"id=\"h-rh-i-0\"\r\n/\u003e\u003c/p\u003e\n\u003ch3 id=\"spark-driver\"\u003eSpark Driver\u003c/h3\u003e\n\u003cp\u003eLà một phần của Spark application chịu trách nhiệm cho việc khởi tạo SparkSession.\nSpark Driver có nhiều vai trò như:\u003c/p\u003e","tags":[],"title":"3. Cách Spark hoạt động"},{"content":"\rMột số khái niệm cần biết: Application: Một chương trình người dùng được xây dựng trên Spark và sử dụng API của nó. Bao gồm chương trình điều khiển (driver program) và executors trong cụm. SparkSession Một đối tượng cung cấp điểm vào để tương tác với các chức năng của Spark. Trong Spark Shell đã cung cấp sẵn SparkSession cho bạn nhưng trong Spark Application, bạn phải tự tạo SparkSession riêng. Job Một chương chình tính toán song song bao gồm nhiều tác vụ được tạo ra để phản hồi cho một Spark action (ví dụ: save(), collect()) Stage Mỗi job được phân chia thành các tác vụ nhỏ hơn được gọi là stages, chúng phụ thuộc lẫn nhau. Task Một đơn vị công việc (work) hay thực thi (executor) được gửi cho Spark Executor.\nSpark Jobs Spark Driver chuyển đổi Spark Application của bạn thành một hay nhiều Spark Jobs. Nó được chuyển đổi từng job vào một DAG (Directed Acyclic Graph)\nHình 1: Spark Driver tạo thành một hay nhiều Spark Jobs.\nSpark Stages Là một phần của DAG nodes, stages được tạo nên dựa trên các hoạt động có thể được thực hiện theo chuỗi hay song song. Không phải hoạt động nào của Spark cũng có thể xảy ra trên 1 stage, vì vậy nó có thể chia thành nhiều stage.\nHình 2: Spark Job tạo một hay nhiều stage (giai đoạn)\nSpark Tasks Mỗi stage bao gồm các Spark Tasks (một đơn vị thực thi), được liên kết trên mỗi Spark executor. Ví dụ một Executor với 16 cores có thể có 16 task hoặc hơn làm việc trên 16 partitions hoặc hơn, làm cho việc xử lý task cực kỳ song song!\nHình 3: Spark Stage tạo một hay nhiều task được phân phối đến executor.\nTransformation, Actions and Lazy Evaluation. Spark operations trên dữ liệu phân tán có thể phân thành 2 loại: tranformations và actions. Transformations: Chuyển đổi Spark DataFrame thành một DataFrame mới mà không làm thay đổi data gốc, mang lại cho nó đặc tính bất biến (immutability). Tất cả các tranformations được đánh giá một cách lười biếng (Evaluated Lazily), có nghĩa là kết quả không được tính toán trực tiếp, mà nó được ghi lại như là một lineage . Việc này cho phép Spark sau đó sắp xếp lại một số transformations nhất định, hợp nhất hay tối ưu hóa chúng thành các giai đoạn với hiệu quả cao hơn. Đồng thời mang lại khả năng phục hồi khi xảy ra lỗi.\nHình 4: Trong hình, Các transformations T được lưu lại cho đến khi action A được gọi đến. Mỗi tranformations tạo ra một DataFrame mới. Hình 5: Một số ví dụ về Tranformations và Actions trong Spark\nVí dụ: Không có gì xuất hiện khi lệnh .conut() dược thực thi:\n# In Python strings = spark.read.text(\u0026#34;../README.md\u0026#34;) filtered = strings.filter(strings.value.contains(\u0026#34;Spark\u0026#34;)) filtered.count() # Output: 20\rNarrow and Wide Transformations Transformations có thể phân loại thành:\nNarrow dependencies (Phụ thuộc hẹp): Bất kỳ Transformation nào mà không phải qua biến đổi mà đầu ra có thể được tính toán duy nhất từ partitions đầu vào. Ví dụ: filter(), contains() là narrow dependencies. Wide dependencies (Phụ thuộc rộng): Ngược lại thì để thực hiện narrow dependencies thì các partitions được đọc, kết hợp lại và được ghi vào trong disk. Ví dụ: groupBy() , orderBy() Hình 6: Narrow vs Wide transformations.\n","date":"2023-09-07","id":26,"permalink":"/de-docs/spark/spark-application-concept/","summary":"\u003cstyle\u003ebody {text-align: justify}\u003c/style\u003e\r\n\u003ch3 id=\"một-số-khái-niệm-cần-biết\"\u003eMột số khái niệm cần biết:\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eApplication:\u003c/em\u003e\u003c/strong\u003e\n\u003cbr\u003e\nMột chương trình người dùng được xây dựng trên Spark và sử dụng API của nó. Bao gồm chương trình điều khiển (driver program) và executors trong cụm.\n\u003cbr\u003e\n\u003cbr\u003e\n\u003cstrong\u003e\u003cem\u003eSparkSession\u003c/em\u003e\u003c/strong\u003e\n\u003cbr\u003e\nMột đối tượng cung cấp điểm vào để tương tác với các chức năng của Spark.\nTrong Spark Shell đã cung cấp sẵn SparkSession cho bạn nhưng trong Spark Application, bạn phải tự tạo SparkSession riêng.\n\u003cbr\u003e\n\u003cbr\u003e\n\u003cstrong\u003e\u003cem\u003eJob\u003c/em\u003e\u003c/strong\u003e\n\u003cbr\u003e\nMột chương chình tính toán song song bao gồm nhiều tác vụ được tạo ra để phản hồi cho một Spark action (ví dụ: save(), collect())\n\u003cbr\u003e\n\u003cbr\u003e\n\u003cstrong\u003e\u003cem\u003eStage\u003c/em\u003e\u003c/strong\u003e\n\u003cbr\u003e\nMỗi job được phân chia thành các tác vụ nhỏ hơn được gọi là stages, chúng phụ thuộc lẫn nhau.\n\u003cbr\u003e\n\u003cbr\u003e\n\u003cstrong\u003e\u003cem\u003eTask\u003c/em\u003e\u003c/strong\u003e\n\u003cbr\u003e\nMột đơn vị công việc (work) hay thực thi (executor) được gửi cho Spark Executor.\u003c/p\u003e","tags":[],"title":"4. Các khái niệm Spark Application"},{"content":"\rSpark DataFrame được lấy cảm hứng từ pandas DataFrame về cấu trúc, định dạng và một số thao tác cụ thể. Ví dụ về một Spark DataFrame được hiển thị tại Hình 1.\nHình 1: Table-like format of a DataFrame\nDataFrame trong Spark bất biến và Spark giữ lại và theo dõi tất cả các transformations (Được nêu ở phần 4). Vì thế bạn có thể sửa đổi tên hoặc data types của cột, tạo DataFrame mới mà bản cũ vẫn được bảo tồn.\nSpark\u0026rsquo;s Basic Data Types Những data types cơ bản có trong Python cũng được Spark hỗ trợ. Ví dụ như Hình 2 dưới đây.\nHình 2: Basic Python data types in Spark\nSpark\u0026rsquo;s Structured and Complex Data Types Để dùng cho những việc phân tích phức tạp, bạn không chỉ đối mặt với những kiểu dữ liệu cơ bản trên. Mà dữ liệu của bạn sẽ phức tạp hơn, thường có cấu trúc hoặc lồng nhau. Vì vậy, bạn cần Spark có thể xử lý được những kiểu dữ liệu phức tạp đó. Có thể như: maps, arrays, structs, dates, timestamps, fields,\u0026hellip;\nTrong Python, các kiểu dữ liệu tương đương cũng được Spark hỗ trợ, ví dụ\nHình 3: Python structured data in Spark\nSchemas and Creating DataFrames Một schema trong Spark xác định tên cột và liên kết với kiểu dữ liệu cho một DataFrame. Schema thường được sử dụng khi đọc dữ liệu có cáu trúc từ nguồn dữ liệu bên ngoài. Tác dụng của Schema:\nGiúp Spark có thể tránh khỏi việc tự suy luận ra kiểu dữ liệu. Giúp Spark ngăn chặn việc tạo một công việc riêng để đọc một khối lượng lớn dữ liệu để xác định ra lược đồ (schema). Với một lượng dữ liệu lớn, điều này có thể gây tốn kém và tốn thời gian. Bạn có thể phát hiện lỗi sớm nếu dữ liệu không khớp với lược đồ (schema). Có 2 cách để định nghĩa một schema Một là, định nghĩa một cách rất lập trình (programmatically). Hai là, triển khai một DDL (Data Definition Language) String, cách này đơn giản và dễ đọc hơn. Để định nghĩa một schema programmatically cho một DataFrame với 3 cột: author, title và pages, bạn có thể sử dụng Spark DataFrame API. Ví dụ:\n// In Scala import org.apache.spark.sql.types._ val schema = StructType(Array(StructField(\u0026#34;author\u0026#34;, StringType, false), StructField(\u0026#34;title\u0026#34;, StringField, false), StructField(\u0026#34;pages\u0026#34;, IntegerType, false)))\r# In Python from pyspark.sql.types import * schema = StructType([StructField(\u0026#34;author\u0026#34;, StringType(), False), StructField(\u0026#34;title\u0026#34;, StringType(), False), StructField(\u0026#34;pages\u0026#34;, IntegerType(), False)])\rCách định nghĩa thứ 2 sử dụng DDL đơn giản hơn rất nhiều, ví dụ:\n// In Scala val schema = \u0026#34;author STRING, title STRING, pages INT\u0026#34;\r# In Python schema = \u0026#34;author STRING, title STRING, pages INT\u0026#34;\rBạn có thể chọn cách định nghĩa schema tùy thích.\nVí dụ: Tạo SparkSession và xác định lược đồ và thêm dữ liệu vào trong DataFrame.\n# In Python # Example-3_6.py from pyspark.sql import SparkSession # Xác định lược đồ của dữ liệu bằng DDL schema = \u0026#34;`Id` INT, `First` STRING, `Url` STRING, `Published` STRING, `Hits` INT, `Campaigns` ARRAY\u0026lt;STRING\u0026gt;\u0026#34; # Tạo static data data = [[1, \u0026#34;Jules\u0026#34;, \u0026#34;Damji\u0026#34;, \u0026#34;https://tinyurl.1\u0026#34;, \u0026#34;1/4/2016\u0026#34;, 4535, [\u0026#34;twitter\u0026#34;, \u0026#34;LinkedIn\u0026#34;]], [2, \u0026#34;Brooke\u0026#34;,\u0026#34;Wenig\u0026#34;, \u0026#34;https://tinyurl.2\u0026#34;, \u0026#34;5/5/2018\u0026#34;, 8908, [\u0026#34;twitter\u0026#34;,\u0026#34;LinkedIn\u0026#34;]], [3, \u0026#34;Denny\u0026#34;, \u0026#34;Lee\u0026#34;, \u0026#34;https://tinyurl.3\u0026#34;, \u0026#34;6/7/2019\u0026#34;, 7659, [\u0026#34;web\u0026#34;, \u0026#34;twitter\u0026#34;, \u0026#34;FB\u0026#34;, \u0026#34;LinkedIn\u0026#34;]], [4, \u0026#34;Tathagata\u0026#34;, \u0026#34;Das\u0026#34;, \u0026#34;https://tinyurl.4\u0026#34;, \u0026#34;5/12/2018\u0026#34;, 10568, [\u0026#34;twitter\u0026#34;, \u0026#34;FB\u0026#34;]], [5, \u0026#34;Matei\u0026#34;,\u0026#34;Zaharia\u0026#34;, \u0026#34;https://tinyurl.5\u0026#34;, \u0026#34;5/14/2014\u0026#34;, 40578, [\u0026#34;web\u0026#34;, \u0026#34;twitter\u0026#34;, \u0026#34;FB\u0026#34;, \u0026#34;LinkedIn\u0026#34;]], [6, \u0026#34;Reynold\u0026#34;, \u0026#34;Xin\u0026#34;, \u0026#34;https://tinyurl.6\u0026#34;, \u0026#34;3/2/2015\u0026#34;, 25568, [\u0026#34;twitter\u0026#34;, \u0026#34;LinkedIn\u0026#34;]]] # Hàm chính if __name__ == \u0026#34;__main__\u0026#34;: # Khởi tạo SparkSession spark = (SparkSession .builder .appName(\u0026#34;CreatingDataFrame\u0026#34;) .getOrCreate()) # Tạo một DataFrame bằng schema đã được định nghĩa blogs_df = spark.createDataFrame(data=data, schema=schema) # Hiển thị DataFrame blogs_df.show() # Hiển thị schema print(blogs_df.printSchema())\rChạy chương trình trong console sẽ được output như sau:\n$ spark-submit Example-3_6.py ... +-------+---------+-------+-----------------+---------+-----+------------------+ |Id |First |Last |Url |Published|Hits |Campaigns | +-------+---------+-------+-----------------+---------+-----+------------------+ |1 |Jules |Damji |https://tinyurl.1|1/4/2016 |4535 |[twitter,...] | |2 |Brooke |Wenig |https://tinyurl.2|5/5/2018 |8908 |[twitter,...] | |3 |Denny |Lee |https://tinyurl.3|6/7/2019 |7659 |[web, twitter...] | |4 |Tathagata|Das |https://tinyurl.4|5/12/2018|10568|[twitter, FB] | |5 |Matei |Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter,...]| |6 |Reynold |Xin |https://tinyurl.6|3/2/2015 |25568|[twitter,...] | +-------+---------+-------+-----------------+---------+-----+------------------+ root |-- Id: integer (nullable = false) |-- First: string (nullable = false) |-- Last: string (nullable = false) |-- Url: string (nullable = false) |-- Published: string (nullable = false) |-- Hits: integer (nullable = false) |-- Campaigns: array (nullable = false) | |-- element: string (containsNull = false) Columns and Expression Có thể sử dụng biểu thức toán học, logic lên các cột. Ví dụ bạn có thể tạo một biểu thức cơ bản sử dụng expr(\u0026ldquo;ColumnName\u0026rdquo; * 5) hoặc (expr(\u0026ldquo;columnName\u0026rdquo;) - 5 \u0026gt; col(anotherColumnName)). Câu lệnh expr() là một phần gói của pyspark.sql.functions (Python) và org.apache.spark.sql.functions (Scala). Câu lệnh này lấy đối số mà Spark có thể phân tích thành biểu thức và tính toán ra kết quả.\nNOTE: Cả Scala, Java, Python đều có một phương thức public liên kết với cột. Trong Spark document đề cập đến col và Column. Column là tên của đối tượng, còn col() là một hàm để trả về một Column.\nBạn hãy xem ví dụ dưới đây để thấy được chúng ta có thể làm gì với cột trong Spark.\n// In Scala scala\u0026gt; import org.apache.spark.sql.functions._ scala\u0026gt; blogsDF.columns res2: Array[String] = Array(Campaigns, First, Hits, Id, Last, Published, Url) // Truy cập một cột cụ thể và trả về loại Cột scala\u0026gt; blogsDF.cols(\u0026#34;Id\u0026#34;) res3: org.apache.spark.sql.Column = id // Sử dụng biểu thức để tính toán scala\u0026gt; blogsDF.select(expr(\u0026#34;Hits * 2\u0026#34;)).show(2) // Hoặc sử dụng col() để tính toán giá trị cùng kết quả với câu lệnh trên scala\u0026gt; blogsDF.select(col(\u0026#34;Hits\u0026#34;) * 2).show(2) +----------+ |(Hits * 2)| +----------+ | 9070 | | 17816 | +----------+ // Sử dụng biểu thức để tính toán những người nổi tiếng về blog // Câu lệnh này sẽ tạo ra cột mới, tên \u0026#34;Big Hitters\u0026#34;, dựa trên biểu thức điều kiện blogsDF.withColumn(\u0026#34;Big Hitters\u0026#34;, (expr(\u0026#34;Hits \u0026gt; 10000\u0026#34;))).show() Output: +---+---------+-------+---+---------+-----+--------------------+-----------+ | Id| First | Last |Url|Published| Hits| Campaigns |Big Hitters| +---+---------+-------+---+---------+-----+--------------------+-----------+ | 1 | Jules | Damji |...| 1/4/2016| 4535| [twitter, LinkedIn]| false | | 2 | Brooke | Wenig |...| 5/5/2018| 8908| [twitter, LinkedIn]| false | | 3 | Denny | Lee |...| 6/7/2019| 7659|[web, twitter, FB...| false | | 4 |Tathagata| Das |...|5/12/2018|10568| [twitter, FB] | true | | 5 | Matei |Zaharia|...|5/14/2014|40578|[web, twitter, FB...| true | | 6 | Reynold | Xin |...| 3/2/2015|25568| [twitter, LinkedIn]| true | +---+---------+-------+---+---------+-----+--------------------+-----------+\r// Kết hợp 3 cột, tạo cột mới và hiển thị. blogsDF .withColumn(\u0026#34;AuthorsId\u0026#34;, (concat(expr(\u0026#34;First\u0026#34;), expr(\u0026#34;Last\u0026#34;), expr(\u0026#34;Id\u0026#34;)))) .select(col(\u0026#34;AuthorsId)) .show(4) Output: +-------------+ | AuthorsId | +-------------+ | JulesDamji1 | | BrookeWenig2| | DennyLee3 | |TathagataDas4| +-------------+\r// Những câu lệnh dưới đây cùng trả về chung giá trị: blogsDF.select(expr(\u0026#34;Hits\u0026#34;)).show(2) blogsDF.select(col(\u0026#34;Hits\u0026#34;)).show(2) blogsDF.select(\u0026#34;Hits\u0026#34;).show(2)\r// Sắp xếp giảm dần cột \u0026#34;Id\u0026#34; blogsDF.sort(col(\u0026#34;Id\u0026#34;).desc).show() blogsDF.sort($\u0026#34;Id\u0026#34;.desc).show()\rTrong ví dụ cuối, col(\u0026ldquo;Id\u0026rdquo;.desc) và $\u0026ldquo;Id\u0026rdquo;.desc đều giống nhau. Đều sắp xếp cột có tên \u0026ldquo;Id\u0026rdquo; theo thứ tự giảm dần. Trong đó:\ncol(\u0026ldquo;Id\u0026rdquo;) trả về một Column object Còn sử dụng $ trước tên cột sẽ chuyển đổi tên cột thành Column. Rows Một hàng trong Spark chứa một hay nhiều cột. Bạn có thể khởi tạo một Row trong Spark hay truy cập vào các field của nó bằng index bắt đầu từ 0:\n// In Scala import org.apache.spark.sql.Row // Tạo một Row val blogRow = Row(6, \u0026#34;Reynold\u0026#34;, \u0026#34;Xin\u0026#34;, \u0026#34;https://tinyurl.6\u0026#34;, 255568, \u0026#34;3/2/2015\u0026#34;, Array(\u0026#34;twitter\u0026#34;, \u0026#34;LinkedIn\u0026#34;)) // Truy cập bằng index scala\u0026gt; blogRow(1) res: Any = Reynold\r# In Python from pyspark.sql import Row blog_row = Row(6, \u0026#34;Reynold\u0026#34;, \u0026#34;Xin\u0026#34;, \u0026#34;https://tinyurl.6\u0026#34;, 255568, \u0026#34;3/2/2015\u0026#34;,[\u0026#34;twitter\u0026#34;, \u0026#34;LinkedIn\u0026#34;]) # Truy cập bằng index blog_row[1] \u0026gt; \u0026#39;Reynold\u0026#39;\rRow object có thể sử dụng để tạo DataFrame:\n# In Python rows = [Row(\u0026#34;Matei Zaharia\u0026#34;, \u0026#34;CA\u0026#34;), Row(\u0026#34;Reynold Xin\u0026#34;, \u0026#34;CA\u0026#34;)] authors_df = spark.createDataFrame(rows, [\u0026#34;Authors\u0026#34;, \u0026#34;State\u0026#34;]) authors_df.show() +-------------+-----+ | Author |State| +-------------+-----+ |Matei Zaharia| CA | | Reynold Xin | CA | +-------------+-----+\r","date":"2023-09-07","id":27,"permalink":"/de-docs/spark/dataframe-api/","summary":"\u003cstyle\u003ebody {text-align: justify}\u003c/style\u003e\r\n\u003cp\u003eSpark DataFrame được lấy cảm hứng từ pandas DataFrame về cấu trúc, định dạng và một số thao tác cụ thể. Ví dụ về một Spark DataFrame được hiển thị tại Hình 1.\u003c/p\u003e","tags":[],"title":"5. DataFrame API"},{"content":"\rDataFrameReader và DataFrame Writer DataFrameReader cho phép bạn đọc data từ nhiều nguồn khác nhau vào DataFrame, ví dụ JSON, CSV, Parquet, Text, Arvo, ORC, \u0026hellip; DataFrameWriter cho phép bạn ghi DataFrame vào nguồn data dưới một định dạng cụ thể. Cùng bắt đầu với một ví dụ, chúng ta sẽ đọc một file CSV lớn chứa dữ liệu từ San Franciso Fire Department calls. Bao gồm 28 cột và 4,380,660 records. Dữ liệu bạn có thể tải tại đây. Với một lượng dữ liệu lớn thì việc tự định nghĩa schema mang lại hiệu quả tốt hơn so với Spark tự xác định schema. Cùng xem cách để làm ví dụ này:\n# In Python from pyspark.sql import SparkSession from pyspark.sql.types import * # Định nghĩa schema theo cách lập trình fire_schema = StructType([StructField(\u0026#39;CallNumber\u0026#39;, IntegerType(), True), StructField(\u0026#39;UnitID\u0026#39;, StringType(), True), StructField(\u0026#39;IncidentNumber\u0026#39;, IntegerType(), True), StructField(\u0026#39;CallType\u0026#39;, StringType(), True), StructField(\u0026#39;CallDate\u0026#39;, StringType(), True), StructField(\u0026#39;WatchDate\u0026#39;, StringType(), True), StructField(\u0026#39;CallFinalDisposition\u0026#39;, StringType(), True), StructField(\u0026#39;AvailableDtTm\u0026#39;, StringType(), True), StructField(\u0026#39;Address\u0026#39;, StringType(), True), StructField(\u0026#39;City\u0026#39;, StringType(), True), StructField(\u0026#39;Zipcode\u0026#39;, IntegerType(), True), StructField(\u0026#39;Battalion\u0026#39;, StringType(), True), StructField(\u0026#39;StationArea\u0026#39;, StringType(), True), StructField(\u0026#39;Box\u0026#39;, StringType(), True), StructField(\u0026#39;OriginalPriority\u0026#39;, StringType(), True), StructField(\u0026#39;Priority\u0026#39;, StringType(), True), StructField(\u0026#39;FinalPriority\u0026#39;, IntegerType(), True), StructField(\u0026#39;ALSUnit\u0026#39;, BooleanType(), True), StructField(\u0026#39;CallTypeGroup\u0026#39;, StringType(), True), StructField(\u0026#39;NumAlarms\u0026#39;, IntegerType(), True), StructField(\u0026#39;UnitType\u0026#39;, StringType(), True), StructField(\u0026#39;UnitSequenceInCallDispatch\u0026#39;, IntegerType(), True), StructField(\u0026#39;FirePreventionDistrict\u0026#39;, StringType(), True), StructField(\u0026#39;SupervisorDistrict\u0026#39;, StringType(), True), StructField(\u0026#39;Neighborhood\u0026#39;, StringType(), True), StructField(\u0026#39;Location\u0026#39;, StringType(), True), StructField(\u0026#39;RowID\u0026#39;, StringType(), True), StructField(\u0026#39;Delay\u0026#39;, FloatType(), True)]) # Khởi tạo Spark Session spark = (SparkSession .builder .appName(\u0026#39;Common_DataFrame_Operations\u0026#39;) .getOrCreate()) # Sử dụng DataFrameReader để đọc CSV file. sf_fire_file = \u0026#34;/databricks-datasets/learning-spark-v2/sf-fire/sf-fire-calls.csv\u0026#34; fire_df = spark.read.csv(sf_fire_file, header=True, schema=fire_schema)\rCâu lệnh spark.read.csv() đọc CSV file và trả về DataFrame với các hàng và tên cột được định nghĩa từ schema đã làm. Bây giờ, chúng ta sẽ cùng ghi dữ liệu vào nguồn data với định dạng tùy bạn chọn với DataFrameWriter. Parquet là một định dạng kiểu columnar-format khá phổ biến, sử dụng linh hoạt để nén dữ liệu, schema sẽ được lưu như metadata trong file Parquet này, vì vậy khi sử dụng Spark sẽ không cần thủ công định nghĩa schema. Lưu DataFrame dưới dạng Parquet hoặc bảng SQL.\n# In Python và cũng tương tự với Scala parquetPath = ... fire_df.write.format(\u0026#34;parquet\u0026#34;).save(parquetPath)\rHoặc bạn có thể lưu dưới dạng bảng, sẽ được đăng ký metadata với Hive metastore (Sẽ được tìm hiểu sâu hơn vào phần sau).\n# In Python parquet_table = ... # Tên table fire_df.write.format(\u0026#34;parquet\u0026#34;).saveAsTable(parquet_table)\rTransformations và Actions 1. Truy vấn dữ liệu Trong Spark, sử dụng select() để truy vấn, đồng thời có thể sử dụng filter() hoặc where() để thực hiện các điều kiện truy vấn. Ví dụ:\n# In Python few_fire_df = (fire_df .select(\u0026#34;IncidentNumber\u0026#34;, \u0026#34;AvaiableDtTm\u0026#34;, \u0026#34;CallType\u0026#34;) .where(col(\u0026#34;CallType\u0026#34;) != \u0026#34;Medical Incident\u0026#34;)) few_fire_df.show(5, truncate=False) Output: +--------------+----------------------+--------------+ |IncidentNumber|AvailableDtTm |CallType | +--------------+----------------------+--------------+ |2003235 |01/11/2002 01:47:00 AM|Structure Fire| |2003235 |01/11/2002 01:51:54 AM|Structure Fire| |2003235 |01/11/2002 01:47:00 AM|Structure Fire| |2003235 |01/11/2002 01:47:00 AM|Structure Fire| |2003235 |01/11/2002 01:51:17 AM|Structure Fire| +--------------+----------------------+--------------+ only showing top 5 rows\rMột số lệnh khác như isNotNull(), countDistinct(), distinct().\n# In Python, trả về số lượng các loại cuộc gọi (Call of types) khác nhau sử dụng countDistinct() from pyspark.sql.functions import * (fire_df .select(\u0026#34;CallType\u0026#34;) .where(col(CallType).isNotNull()) .agg(countDistinct(\u0026#34;CallType\u0026#34;).alias(\u0026#34;DistinctCallTypes\u0026#34;)) .show()) Output: +-----------------+ |DistinctCallTypes| +-----------------+ | 32| +-----------------+\rChúng ta có thể hiện các loại cuộc gọi khác nhau sử dụng câu lệnh sau:\n# In Python (fire_df .select(\u0026#34;CallType\u0026#34;) .where(col(\u0026#34;CallType\u0026#34;).isNotNull()) .distinct() .show(10, False)) Output: 32 +-----------------------------------+ |CallType | +-----------------------------------+ |Elevator / Escalator Rescue | |Marine Fire | |Aircraft Emergency | |Confined Space / Structure Collapse| |Administrative | |Alarms | |Odor (Strange / Unknown) | |Lightning Strike (Investigation) | |Citizen Assist / Service Call | |HazMat | +-----------------------------------+ only showing top 10 rows\r2. Đổi tên, thêm và xóa cột Để đổi tên cột, ta có thể sử dụng withColumnRenamed().\n# In Python new_fire_df = fire_df.withColumnRenamed(\u0026#34;Delay\u0026#34;, \u0026#34;ResponseDelayedInMins\u0026#34;)\rTrong nhiều trường hợp, dữ liệu raw và không thể sử dụng được khi phân tích như dữ liệu ngày tháng là String thay vì Timestamp. Vì vậy, chúng ta có thể thay đổi bằng cách thêm cột mới với dữ liệu mong muốn và xóa đi cột. Thật may vì trong Spark cũng hỗ trợ việc này với withColumn() để có thể thêm cột, drop() để xóa cột và to_timestamp() hay to_date() để chuyển từ kiểu dữ liệu string sang date hay timestamp như sau:\n# In Python fire_ts_df = (new_fire_df .withColumn(\u0026#34;IncidentDate\u0026#34;, to_timestamp(col(\u0026#34;CallDate\u0026#34;), \u0026#34;MM/dd/yyyy\u0026#34;)) .drop(\u0026#34;CallDate\u0026#34;) .withColumn(\u0026#34;OnWatchDate\u0026#34;), to_timestamp(col(\u0026#34;WatchDate\u0026#34;), \u0026#34;MM/dd/yyyy\u0026#34;) .drop(\u0026#34;WatchDate\u0026#34;) .withColumn(\u0026#34;AvailableDtTS\u0026#34;), to_timestamp(col(\u0026#34;AvailableDtTm\u0026#34;), \u0026#34;MM/dd/yyyy hh:mm:ss a\u0026#34;) .drop(\u0026#34;AvailableDtTm\u0026#34;))\rSpark cũng hỗ trợ nhiều phương thức xử lý dữ liệu từ spark.sql.functions như month(), year() và day().\n3. Tổng hợp dữ liệu Một số phương thức thực hiện tổng hợp dữ liệu như: groupBy(), orderBy(), count(). Cùng thực hiện với một câu hỏi: Đâu là loại cuộc gọi cứu hóa nhiều nhất?\n# In Python (fire_ts_df .select(\u0026#34;CallType\u0026#34;) .where(col(\u0026#34;CallType\u0026#34;).isNotNull()) .groupBy(\u0026#34;CallType\u0026#34;) .count() .orderBy(\u0026#34;count\u0026#34;, ascending=False) .show(n=10, truncate=False)) Output: +-------------------------------+-------+ |CallType |count | +-------------------------------+-------+ |Medical Incident |2843475| |Structure Fire |578998 | |Alarms |483518 | |Traffic Collision |175507 | |Citizen Assist / Service Call |65360 | |Other |56961 | |Outside Fire |51603 | |Vehicle Fire |20939 | |Water Rescue |20037 | |Gas Leak (Natural and LP Gases)|17284 | +-------------------------------+-------+\rVậy câu trả lời là: Medical Incident.\n4. Một số thao tác với DataFrame khác DataFrame API cũng cung cấp một số phương thức thống kê như: min(), max(), sum() và avg(). Một cách import Pythonic để không gây xung đột giữa các built-in Python function như sau:\n# In Python import pyspark.sql.functions as F (fire_ts_df .select(F.sum(\u0026#34;NumAlarms\u0026#34;), F.avg(\u0026#34;ResponseDelayedInMins\u0026#34;), F.min(\u0026#34;ResponseDelayedInMins\u0026#34;), F.max(\u0026#34;ResponseDelayedInMins\u0026#34;)) .show()) Output: +--------------+--------------------------+--------------------------+---------+ |sum(NumAlarms)|avg(ResponseDelayedInMins)|min(ResponseDelayedInMins)|max(...) | +--------------+--------------------------+--------------------------+---------+ | 4403441| 3.902170335891614| 0.016666668|1879.6167| +--------------+--------------------------+--------------------------+---------+\rCác phương thức thống kê nâng cao thường gặp cho công việc khoa học dữ liệu như: stat(), describe(), correlation(), covariance(), sampleBy(), approxQuantile(), frequentItems(), \u0026hellip;\n","date":"2023-09-07","id":28,"permalink":"/de-docs/spark/common-dataframe-operation/","summary":"\u003cstyle\u003ebody {text-align: justify}\u003c/style\u003e\r\n\u003ch3 id=\"dataframereader-và-dataframe-writer\"\u003eDataFrameReader và DataFrame Writer\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eDataFrameReader\u003c/strong\u003e cho phép bạn đọc data từ nhiều nguồn khác nhau vào DataFrame, ví dụ JSON, CSV, Parquet, Text, Arvo, ORC, \u0026hellip;\n\u003cbr\u003e\n\u003cstrong\u003eDataFrameWriter\u003c/strong\u003e cho phép bạn ghi DataFrame vào nguồn data dưới một định dạng cụ thể.\n\u003cbr\u003e\n\u003cbr\u003e\nCùng bắt đầu với một ví dụ, chúng ta sẽ đọc một file CSV lớn chứa dữ liệu từ San Franciso Fire Department calls. Bao gồm 28 cột và 4,380,660 records.\nDữ liệu bạn có thể tải \u003ca href=\"https://github.com/databricks/LearningSparkV2/tree/master\"\u003etại đây\u003c/a\u003e. Với một lượng dữ liệu lớn thì việc tự định nghĩa schema mang lại hiệu quả tốt hơn so với Spark tự xác định schema.\n\u003cbr\u003e\n\u003cbr\u003e\nCùng xem cách để làm ví dụ này:\u003c/p\u003e","tags":[],"title":"6. Một số thao tác với DataFrame"},{"content":"\rĐề bài Hãy sử dụng data set San Francisco Fire Department và sử dụng Spark để thực hiện các yêu cầu sau:\nWhat were all the different types of fire call in 2018? What months within the year 2018 saw the highest number of fire calls? Which neighborhoods had the worst response times to ffire calls in 2018? Which week in the year in 2018 had the most fire calls? Is there a correlation between neighborhood, zip code, and number of fire calls? How can we use Parquet file or SQL tables to store this data and read it back? Lời giải Khuyến khích thực hiện những bài tập trên trước khi đọc lời giải, hãy chỉ lấy lời giải làm tham khảo, nếu chỉ đọc lời giải mà không thực hành sẽ không thể nhớ lâu được. Lời giải của sẽ được làm trên ngôn ngữ Python thông qua thư viện PySpark.\n","date":"2023-09-07","id":29,"permalink":"/de-docs/spark/bai-tap-lan-mot/","summary":"\u003cstyle\u003ebody {text-align: justify}\u003c/style\u003e\r\n\u003ch3 id=\"đề-bài\"\u003eĐề bài\u003c/h3\u003e\n\u003cp\u003eHãy sử dụng data set San Francisco Fire Department và sử dụng Spark để thực hiện các yêu cầu sau:\u003c/p\u003e","tags":[],"title":"7. Bài tập DataFrame lần 1"},{"content":"\rLưu ý: Đây là những kiến thức được rút ra khi đọc sách, đây là phiên bản được rút gọn theo ý hiểu với ngôn ngữ là Tiếng Việt.\nBản quyền đến từ những tác phẩm sau:\nData Engineering with AWS (Second Edition) bởi Gareth Eagar Learning Spark Lightning-Fast Data Analytics (2nd Edition) bởi nhiều tác giả. Kafka The Definitive Guide ","date":"2023-09-07","id":30,"permalink":"/de-docs/overview/loi-mo-dau/","summary":"\u003cstyle\u003ebody {text-align: justify}\u003c/style\u003e\r\n\u003ch3 id=\"lưu-ý\"\u003eLưu ý:\u003c/h3\u003e\n\u003cp\u003eĐây là những kiến thức được rút ra khi đọc sách, đây là phiên bản được rút gọn theo ý hiểu với ngôn ngữ là Tiếng Việt.\u003c/p\u003e","tags":[],"title":"Lời mở đầu"},{"content":"\rGiới thiệu bản thân Xin chào mình tên là Lương Tiến Dũng. Là sinh viên công nghệ thông tin năm thứ 2 tại trường đại học Giao thông Vận tải. Đang theo đuổi con đường trở thành một Data Engineer.\n","date":"2023-09-07","id":31,"permalink":"/docs/overview/l%E1%BB%9Di-m%E1%BB%9F-%C4%91%E1%BA%A7u/","summary":"\u003cstyle\u003ebody {text-align: justify}\u003c/style\u003e\r\n\u003ch4 id=\"giới-thiệu-bản-thân\"\u003eGiới thiệu bản thân\u003c/h4\u003e\n\u003cp\u003eXin chào mình tên là Lương Tiến Dũng. Là sinh viên công nghệ thông tin năm thứ 2 tại trường đại học Giao thông Vận tải.\nĐang theo đuổi con đường trở thành một Data Engineer.\u003c/p\u003e","tags":[],"title":"Lời mở đầu"},{"content":"","date":"2023-09-07","id":32,"permalink":"/docs/ly-thuyet/","summary":"","tags":[],"title":"Lý thuyết"},{"content":"","date":"2023-09-07","id":33,"permalink":"/docs/thuc-hanh/","summary":"","tags":[],"title":"Thực hành"},{"content":"\rĐiện toán đám mây là gì? Điện toán đám mây là việc phân phối các tài nguyên CNTT theo nhu cầu qua internet với chính sách thanh toán theo mức sử dụng.\nLợi ích của điện toán đám mây Sử dụng bao nhiêu tính tiền bấy nhiêu, cung cấp khả năng tối ưu hóa chi phí Tăng tốc độ phát triển, nhờ tận dụng các tính năng tự động hóa và quản trị bởi nhà cung cấp dịch vụ Linh hoạt, thêm bớt tài nguyên tùy ý. Mở rộng quy mô ứng dụng lên toàn cầu AWS, điều gì tạo nên sự khác biệt? AWS là nhà cung cấp hạ tầng Cloud dẫn đầu trong 13 năm liên tiếp. [Tính tới hết 2023] AWS là nhà cung cấp khác biệt về tầm nhìn và văn hóa. Triết lý về giá của AWS: Khách hàng sẽ càng ngày càng trả ít tiền hơn cho cùng dịch vụ / tính năng / tài nguyên sử dụng. AWS đưa việc mang lại giá trị thực sự cho khách hàng lên hàng đầu trong tất cả các nguyên tắc lãnh đạo (Leadership Principle) của mình Bắt đầu hành trình lên mây như thế nào? Là nhà cung cấp điện toán đám mây hàng đầu, số lượng khóa học nhiều nhất, có chiều sâu nhất trong tất cả các nhà cung cấp dịch vụ Hoàn toàn có thể tự học để trỏ thành chuyên gia Hãy kết bạn và cùng nhau học hỏi. Đăng ký tài khoản AWS càng sớm càng tốt và trải nghiệm thông qua Free Tier ","date":"2023-09-07","id":34,"permalink":"/docs/ly-thuyet/1.-%C4%91i%E1%BB%87n-to%C3%A1n-%C4%91%C3%A1m-m%C3%A2y/","summary":"\u003cstyle\u003ebody {text-align: justify}\u003c/style\u003e\r\n\u003ch3 id=\"điện-toán-đám-mây-là-gì\"\u003eĐiện toán đám mây là gì?\u003c/h3\u003e\n\u003cp\u003eĐiện toán đám mây là việc phân phối các tài nguyên CNTT theo nhu cầu qua internet\nvới \u003cspan style=\"color: orange; font-weight:bold;\"\u003echính sách thanh toán theo mức sử dụng.\u003c/span\u003e\u003c/p\u003e","tags":[],"title":"1. Điện toán đám mây"},{"content":"\rTrung tâm dữ liệu của AWS (Data Center) Một trung tâm dữ liệu có thể chứa hàng chục ngàn máy chủ.\nTất cả trung tâm dữ liệu của AWS đều sử dụng các thiết bị được tối ưu hóa dành riêng cho hoạt động của AWS\nMore Info\nAvailability Zone Một Availability Zone (AZ) bao gồm một hoặc nhiều trung tâm dữ liệu, các (AZ) được thiết kế để không xảy ra sự cố ảnh hưởng đồng thời 2 AZ một lúc (fault isolation) Giữa 2 AZ là đường kết nối riêng tốc độ cao AWS khuyến nghị nên triển khai ứng dụng tối thiểu trên 2 AZ Region Một AWS Region bao gồm tối thiểu 3 Availability Zone. Hiện tại có hơn 25 Region trên toàn cầu. Các Region được kết ối với nhau bởi mạng backbone của AWS. Mặc định dữ liệu và dịch vụ ở các Region độc ập với nhau. (Trừ một số dịch vụ quy mô Global) Edge Locations Là mạng lưới trung tâm dữ liệu AWS được thiết kế để cung cấp dịch vụ với độ trễ thấp nhất có thể. Các dịch vụ AWS hoạt động tại Edge Locations (POP) bao gồm CloudFront (CDN) Web Application Firewall (WAF) Route 53 (DNS Service) ","date":"2023-09-07","id":35,"permalink":"/docs/ly-thuyet/2.-h%E1%BA%A1-t%E1%BA%A7ng-to%C3%A0n-c%E1%BA%A7u-c%E1%BB%A7a-aws/","summary":"\u003cstyle\u003ebody {text-align: justify}\u003c/style\u003e\r\n\u003ch3 id=\"trung-tâm-dữ-liệu-của-aws-data-center\"\u003eTrung tâm dữ liệu của AWS (Data Center)\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eMột trung tâm dữ liệu có thể chứa hàng chục ngàn máy chủ.\u003c/p\u003e","tags":[],"title":"2. Hạ tầng toàn cầu của AWS"},{"content":"\rAWS Management Console - Root Login Chúng ta có thể login bằng tài khoản root hoặc bằng tài khoản IAM User (tài khoản con giúp quản lý truy xuất các tài nguyên của AWS) Link AWS Console\nAWS Management Console - IAM Login Khi login bằng IAM User chúng ta cần cung cấp thêm thông tin Account ID (Chuỗi 12 chữ số) để xác định account AWS\nAWS Management Console - Service Search Sau khi login có thể tìm kiếm và sử dụng các dịch vụ của AWS AWS Management Console - Support Center Tạo support case để yêu cầu trợ giúp\nAWS Command Line Interface (CLI) Open source, cho phép tương tác với các dịch vụ AWS bằng command. Chức năng tương đương với chức năng được cung cấp bởi AWS Management Console dựa trên trình duyệt Sử dụng Access key / Secret Access Key để sử dụng CLI. AWS SDK Đơn giản hóa việc sử dụng AWS bằng cách cung cấp một bộ thư viện nhất quán và quen thuộc cho đội ngũ phát triển ứng dụng. Sử dụng Access key / Secret Access Key để sử dụng SDK. Phát triển ứng dụng sử dụng AWS dễ dàng. ","date":"2023-09-07","id":36,"permalink":"/docs/ly-thuyet/3.-c%C3%B4ng-c%E1%BB%A5-qu%E1%BA%A3n-l%C3%BD-aws-services/","summary":"\u003cstyle\u003ebody {text-align: justify}\u003c/style\u003e\r\n\u003ch3 id=\"aws-management-console---root-login\"\u003eAWS Management Console - Root Login\u003c/h3\u003e\n\u003cp\u003eChúng ta có thể login bằng tài khoản root hoặc bằng tài khoản IAM User (tài khoản con giúp quản lý truy xuất các tài nguyên của AWS)\n\u003cbr\u003e \u003ca href=\"https://aws.amazon.com/console/\"\u003eLink AWS Console\u003c/a\u003e\u003c/p\u003e","tags":[],"title":"3. Công cụ quản lý AWS Services"},{"content":"\rTối ưu hóa chi phí trên AWS Lựa chọn cấu hình tài nguyên tính toán và nơi lưu trữ dữ liệu phù hợp. Tận dụng các phương thức thanh toán giảm giá như reserved instance , saving plan, spot. Xoá các tài nguyên không sử dụng, bật tắt tự động các tài nguyên không cần chạy 24/7. Tận dụng các dịch vụ serverless. Thiết kế kiến trúc tối ưu Cài đặt và sử dụng AWS Budget. Quản lý chi phí theo phòng ban / ứng dụng với cost allocation tag. Liên tục theo dõi và tối ưu hóa chi phí. Công cụ tính toán chi phí https://calculator.aws/#/\nCho phép tạo các estimate các dịch vụ thông dụng. Có thể chia sẻ ca estimate cho nguười khác. Chi phí sẽ khác biệt theo từng Region Làm việc với AWS Support AWS có 4 gói hỗ trợ chính: Basic (Explore) Developer (Dev/Test) Business (Production) Enterprise (Large Enterprise) Có thể nâng cấp gói hỗ trợ trong thời gian ngắn để đẩy nhanh tốc độ hỗ trợ khi có sự cố quan trọng cần xử lý nhanh. ","date":"2023-09-07","id":37,"permalink":"/docs/ly-thuyet/4.-t%E1%BB%91i-%C6%B0u-h%C3%B3a-chi-ph%C3%AD-tr%C3%AAn-aws-v%C3%A0-l%C3%A0m-vi%E1%BB%87c-v%E1%BB%9Bi-aws-support/","summary":"\u003cstyle\u003ebody {text-align: justify}\u003c/style\u003e\r\n\u003ch3 id=\"tối-ưu-hóa-chi-phí-trên-aws\"\u003eTối ưu hóa chi phí trên AWS\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eLựa chọn cấu hình tài nguyên tính toán và nơi lưu trữ dữ liệu phù hợp.\u003c/li\u003e\n\u003cli\u003eTận dụng các phương thức thanh toán giảm giá như \u003cspan style=\"color: orange; font-weight:bold;\"\u003ereserved instance\u003c/span\u003e\n, \u003cspan style=\"color: orange; font-weight:bold;\"\u003esaving plan\u003c/span\u003e, \u003cspan style=\"color: orange; font-weight:bold;\"\u003espot\u003c/span\u003e.\u003c/li\u003e\n\u003cli\u003eXoá các tài nguyên không sử dụng, bật tắt tự động các tài nguyên không cần chạy 24/7.\u003c/li\u003e\n\u003cli\u003eTận dụng các dịch vụ \u003cspan style=\"color: orange; font-weight:bold;\"\u003eserverless\u003c/span\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"thiết-kế-kiến-trúc-tối-ưu\"\u003eThiết kế kiến trúc tối ưu\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eCài đặt và sử dụng \u003cspan style=\"color: orange; font-weight:bold;\"\u003eAWS Budget\u003c/span\u003e.\u003c/li\u003e\n\u003cli\u003eQuản lý chi phí theo phòng ban / ứng dụng với \u003cspan style=\"color: orange; font-weight:bold;\"\u003ecost allocation tag\u003c/span\u003e.\u003c/li\u003e\n\u003cli\u003e\u003cspan style=\"color: orange; font-weight:bold;\"\u003eLiên tục\u003c/span\u003e theo dõi và tối ưu hóa chi phí.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"công-cụ-tính-toán-chi-phí\"\u003eCông cụ tính toán chi phí\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://calculator.aws/#/\"\u003ehttps://calculator.aws/#/\u003c/a\u003e\u003c/p\u003e","tags":[],"title":"4. Tối ưu hóa chi phí trên AWS và Làm việc với AWS Support"},{"content":"\rAmazon Virtual Private Cloud (Amazon VPC) cho phép bạn khởi chạy các tài nguyên AWS vào một mạng ảo mà bạn đã xác định. VPC nằm trong 1 Region, khi tạo VPC cần khai báo 1 lớp mạng CIDR IPv4 (bắt buộc) và IPv6 (optional) Giới hạn của VPC hiện tại là 5 VPC trên 1 AWS Region trên 1 AWS Account. Mục đích chính sử dụng VPC thường dùng để phân tách các môi trường (Production / Dev / Test / Staging) Lưu ý: Nếu muốn các tài nguyên tách biệt hẳn (User không thể nhìn thầy một tài nguyên cụ thể thì cần tách thành nhiều AWS Account, nhiều VPC không giải quyết được vấn đề này) Amazon Virtual Private Cloud (VPC) - Subnet Amazon VPC cho phép tạo nhiều mạng ảo và chia các mạng ảo này thành các mạng con (subnet) VPC Subnet sẽ năm trong 1 Availability Zone cụ thể. Khi tạo Subnet, chúng ta chỉ định CIDR cho mạng con đó và đây là một tập hợp con của khối VPC CIDR Trong mỗi Subnet, AWS sẽ giữ 5 địa chỉ IP. Ví dụ nếu Subnet có CIDR là 10.10.1.0/24 Địa chỉ netwwork (10.10.1.0) Địa chỉ broadcast (10.10.1.255) Địa chỉ cho bộ định tuyến (10.10.1.1) Địa chỉ cho DNS (10.10.1.2) Địa chỉ cho tính năng tương lai (10.10.1.3) Amazon Virtual Private Cloud (VPC) - Route Table Route Table (Bảng định tuyến), tập hợp các Route, để xác định đường đi cho mạng. Khi tạo VPC, AWS sẽ tạo một Default Rote table, không thể bị xóa và chỉ chứa 1 Route duy nhất là Route cho phép tất cả các Subnet trong VPC liên lạc với nhau. Route table sẽ được gắn vào Subnet Chúng ta có thể tạo Custom Route table, tuy nhiên sẽ không thể xóa default route (VPC CIDR - local). Amazon Virtual Private Cloud (VPC) - Elastic Network Interface (ENI) Elastic Network Interface (ENI) là một card mạng ảo, chúng ta có thể chuyển sang các EC2 Instance khác. Khi chuyển sang một máy chủ mới, một card mạng ảo sẽ vẫn duy trì: Địa chỉ IP Private Địa chỉ Elastic address Địa chỉ MAC Amazon Virtual Private Cloud (VPC) - Elastic IP address (EIP) Elastic IP address (EIP) là một địa chỉ public IPv4 tĩnh, có thể liên kết với một Elastic Network Interface. Khi không sử dụng, sẽ bị charge phí (tránh lãng phí) Khi tạo một EC2 Instance, sẽ tạo ra một Elastic Network Interface bao gồm\nVPC Private IP, Địa chỉ Private IPv4 trong subnet CIDR. Ví dụ trong ảnh Public Subnet với dải CIDR 10.10.1.0/24 thì EC2 sẽ được gán trong khoảng từ 10.10.1.4 -\u0026gt; 10.10.1.254. Trường hợp trong ảnh là 10.10.1.6 Elastic Ip Address, Địa chỉ Public IPv4 tĩnh, không thay đổi khi máy ảo restart. Trong ảnh là 134.23.42.15 Amazon Virtual Private Cloud (VPC) - VPC Endpoint VPC Endpoint cho phép chúng ta kết nối các tài nguyên nằm trong VPC tới các dịch vụ AWS được hỗ trợ (AWS PrivateLink - đi qua mạng private của AWS) mà không cần thông qua kết nối internet. Có 2 kiểu VPC Endpoint: Interface Endpoint: Sử dụng một Elastic Network Interface trong VPC cùng với một địa chỉ IP Private để kết nối tới một dịch vụ hỗ trợ. Gateway Endpoint: Sử dụng một route table để định tuyến tới endpoint của dịch vụ hỗ trợ (S3 và DynamoDB) Amazon Virtual Private Cloud (VPC) - Internet Gateway Internet Gateway là một thành phần của Amazon VPC có khả năng mở rộng quy mô theo chiều nang (scale out) cho phép các EC2 Instance trong VPC có thể truyền thông tin ra ngoài Internet. Internet Gateway được quản lý bởi AWS, chúng ta không cần cấu hình autosclae hoặc high availability. Amazon Virtual Private Cloud (VPC) - NAT Gateway NAT gateway cho phép các EC2 instance trong subnet truy ập tới interrnet hoặc các dịch vụ AWS khác. Chỉ chấp nhận kết nối chiều ra và không chấp nhận kết nối chiều vào. ","date":"2023-09-07","id":38,"permalink":"/docs/ly-thuyet/5.-aws-virtual-private-cloud/","summary":"\u003cstyle\u003ebody {text-align: justify}\u003c/style\u003e\r\n\u003cul\u003e\n\u003cli\u003eAmazon Virtual Private Cloud (Amazon VPC) cho phép bạn khởi chạy các tài nguyên AWS vào một mạng ảo mà bạn đã xác định.\u003c/li\u003e\n\u003cli\u003eVPC nằm trong 1 Region, khi tạo VPC cần khai báo 1 lớp mạng CIDR IPv4 (bắt buộc) và IPv6 (optional)\u003c/li\u003e\n\u003cli\u003eGiới hạn của VPC hiện tại là 5 VPC trên 1 AWS Region trên 1 AWS Account.\u003c/li\u003e\n\u003cli\u003eMục đích chính sử dụng VPC thường dùng để \u003cspan style=\"color: orange; font-weight:bold;\"\u003ephân tách\u003c/span\u003e các môi trường\n(Production / Dev / Test / Staging)\u003c/li\u003e\n\u003cli\u003eLưu ý: Nếu muốn các tài nguyên tách biệt hẳn (User không thể nhìn thầy một tài nguyên cụ thể thì cần tách thành nhiều AWS Account, nhiều VPC không giải quyết được vấn đề này)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"amazon-virtual-private-cloud-vpc---subnet\"\u003eAmazon Virtual Private Cloud (VPC) - Subnet\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eAmazon VPC cho phép tạo nhiều mạng ảo và chia các mạng ảo này thành các mạng con (\u003cspan style=\"color: orange; font-weight:bold;\"\u003esubnet\u003c/span\u003e)\u003c/li\u003e\n\u003cli\u003eVPC Subnet sẽ năm trong 1 Availability Zone cụ thể.\u003c/li\u003e\n\u003cli\u003eKhi tạo Subnet, chúng ta chỉ định CIDR cho mạng con đó và đây là một \u003cspan style=\"color: orange; font-weight:bold;\"\u003etập hợp con của khối VPC CIDR\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003eTrong mỗi Subnet, AWS sẽ giữ 5 địa chỉ IP. Ví dụ nếu Subnet có CIDR là 10.10.1.0/24\n\u003cul\u003e\n\u003cli\u003eĐịa chỉ netwwork (10.10.1.0)\u003c/li\u003e\n\u003cli\u003eĐịa chỉ broadcast (10.10.1.255)\u003c/li\u003e\n\u003cli\u003eĐịa chỉ cho bộ định tuyến (10.10.1.1)\u003c/li\u003e\n\u003cli\u003eĐịa chỉ cho DNS (10.10.1.2)\u003c/li\u003e\n\u003cli\u003eĐịa chỉ cho tính năng tương lai (10.10.1.3)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\r\n\r\n\u003cimg\r\n  src=\"/docs/ly-thuyet/images/subnet_hu_b4e59fd829997557.webp\"\r\n  width=\"1571\"\r\n  height=\"954\"\r\n  decoding=\"async\"\r\n  fetchpriority=\"auto\"\r\n  loading=\"lazy\"\r\n  alt=\"alt\"id=\"h-rh-i-0\"\r\n/\u003e\u003c/p\u003e","tags":[],"title":"5. AWS Virtual Private Cloud"},{"content":"\rAmazon Virtual Private Cloud (VPC) - Security Group Security Group (SG) là một tường lửa ảo có lưu trữ trạng thái (stateful) giúp kiểm soát lượng truy cập đến và đi trong tài nguyên của AWS. Security Group rule được hẹn chế theo giao thức, địa chỉ nguồn, cổng kết nối, hoặc một Security Group khác. Security Group rule chỉ cho phép rule allow. Security Group được áp dụng lên các Elastic Network Interface. Amazon Virtual Private Cloud (VPC) - Network Access Control List (NACL) Network Access Control List (NACL) là một tường lửa ảo không lưu trữ trạng thái (stateless) giúp kiểm soát lượng truy cập đến và đi trong tài nguyên của AWS NACL được hạn chế theo giao thức, địa chỉ nguồn, cổng kết nối. NACL được áp dụng lên các Amazon VPC Subnets. Mặc định NACL cho phép mọi truy cập đến và đi. Amazon Virtual Private Cloud (VPC) - VPC Flow Logs VPC Flow Logs là một tính năng cho phép bạn nắm bắt thông tin về lưu lượng IP đến và đi từ các giao diện mạng trong VPC của bạn. Các tập tin logs có thể được xuất bản lên Amazon CloudWatch Logs hoặc Amazon S3. VPC Flow Logs không capture nội dung gói tin. VPC Peering VPC Peering là tính năng giúp kết nối hai hay nhiều VPC để các tài nguyên bên trong hai VPC đó có thể liên lạc trực tiếp với nhau không cần phải thông qua Internet, góp phần gia tăng tính bảo mật cho VPC. VPC Peering là kết nối cần tạo 1:1 giữa hai VPC thành viên, không hỗ trợ transitive routing. VPC Peering không hỗ trợ khi 2 VPC bị overlap IP address space. Transit Gateway Transit Gateway được dùng để kết nối các VPC và mạng on-premises thông qua một hub trung tâm. Điều này đơn giản hóa mạng và kết thúc các mối quan hệ định tuyến phức tạp. Transit Gateway Attachment là một công cụ để gán các subnet của từng VPC cần kết nối với nhau vào một TGW đã được khởi tạo. Transit Gateway Attachment hoạt động ở quy mô AZ-level. Trong VPC, khi một subnet ở một AZ có Transit Gateway Attachment với một TGW, các subnet khác trong cuùng AZ đều có thể kết nối với TGW đó. ","date":"2023-09-07","id":39,"permalink":"/docs/ly-thuyet/6.-vpc-security-and-multi-vpc-features/","summary":"\u003cstyle\u003ebody {text-align: justify}\u003c/style\u003e\r\n\u003ch3 id=\"amazon-virtual-private-cloud-vpc---security-group\"\u003eAmazon Virtual Private Cloud (VPC) - Security Group\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cspan style=\"color: orange; font-weight:bold;\"\u003eSecurity Group\u003c/span\u003e (SG) là một tường lửa ảo có lưu trữ trạng thái (stateful) giúp kiểm soát lượng truy cập đến và đi trong tài nguyên của AWS.\u003c/li\u003e\n\u003cli\u003eSecurity Group rule được hẹn chế theo giao thức, địa chỉ nguồn, cổng kết nối, hoặc một Security Group khác.\u003c/li\u003e\n\u003cli\u003eSecurity Group rule \u003cspan style=\"color: orange; font-weight:bold;\"\u003echỉ cho phép rule allow.\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003eSecurity Group được áp dụng lên các Elastic Network Interface.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"amazon-virtual-private-cloud-vpc---network-access-control-list-nacl\"\u003eAmazon Virtual Private Cloud (VPC) - Network Access Control List (NACL)\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eNetwork Access Control List (NACL) là một tường lửa ảo không lưu trữ trạng thái (stateless) giúp kiểm soát lượng truy cập đến và đi trong tài nguyên của AWS\u003c/li\u003e\n\u003cli\u003eNACL được hạn chế theo giao thức, địa chỉ nguồn, cổng kết nối.\u003c/li\u003e\n\u003cli\u003eNACL được áp dụng lên các Amazon VPC Subnets.\u003c/li\u003e\n\u003cli\u003eMặc định NACL cho phép mọi truy cập đến và đi.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"amazon-virtual-private-cloud-vpc---vpc-flow-logs\"\u003eAmazon Virtual Private Cloud (VPC) - VPC Flow Logs\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cspan style=\"color: orange; font-weight:bold;\"\u003eVPC Flow Logs\u003c/span\u003e là một tính năng cho phép bạn nắm bắt thông tin về lưu lượng IP đến và đi từ các giao diện mạng trong VPC của bạn.\u003c/li\u003e\n\u003cli\u003eCác tập tin logs có thể được xuất bản lên Amazon CloudWatch Logs hoặc Amazon S3.\u003c/li\u003e\n\u003cli\u003eVPC Flow Logs không capture nội dung gói tin.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\r\n\r\n\u003cimg\r\n  src=\"/docs/ly-thuyet/images/vpc-flowlogs_hu_dc47f7e4cc43aafa.webp\"\r\n  width=\"1696\"\r\n  height=\"698\"\r\n  decoding=\"async\"\r\n  fetchpriority=\"auto\"\r\n  loading=\"lazy\"\r\n  alt=\"alt\"id=\"h-rh-i-0\"\r\n/\u003e\u003c/p\u003e","tags":[],"title":"6. VPC Security and Multi-VPC features"},{"content":"\rVPN Site to Site VPN Site to Site dùng trong mô hình hybird để thiết lập kết nối liên tục giữa môi trường trung tâm dữ liệu truyền thống tới môi trường VPC của AWS. Việc thiết lập kết nối sẽ cần 2 đầu endpoint ở phía AWS và phía khách hàng: Virtual Private Gateway: Được quản lý hoàn toàn bởi AWS (chia 2 endpoints ở 2 đầu ra AZ). Customer Gateway: Đầu enpoint phía khách hàng, có thể là thiết bị phần cứng hoặc software appliance. VPN Client to Site VPN Client to Site: Cho phép một host truy cập tới tài nguyên trong VPC. Khuyến kích sử dụng VPN Client to Site trong AWS Market Place AWS Direct Connect AWS Direct Connect là dịch vụ cho phép tạo kết nối riêng từ trung tâm dữ liệu truyền thống tới AWS.. Độ trễ khoảng 20ms - 30ms. AWS Direct Connect ở Việt Nam hiện tại sẽ thông qua AWS Direct Connect partners và hoạt động dưới dạng Hosted Connections. (Nếu trực tiếp tới AWS là Đeicated Connections). Băng thông Direct Connect có thể thay đổi lên / xuống tùy nhu cầu. Elastic Load Balancing Elastic Load Balancing (ELB) là một dịch vụ cân bằng tải được quản lý bởi AWS, có chức năng phân phối lưu lượng cho nhiều EC2 Instance hoặc Container. Sử dụng giao thức HTTP, HTTPS, TCP và SSL (TCP bảo mật). Có thể nằm ở public hoặc private subnet. Mỗi ELB sẽ được cấp tên DNS và kt nối thông qua DNS. Chỉ có Network Load Balancer hỗ trợ gắn IP tĩnh. ELB có tính năng health check, không gửi lưu lượng đến các Instance không đạt health check. Bao gồm 4 loại: Application Load Balancer Network Load Balancer Classic Load Balancer Gateway Load Balancer Sticky session (sesion afinity): Tính năng cho phép các kết nối được gán vào một target nhất định. Việc này đảm bảo các requests từ một user trong một session sẽ được gửi tới cùng một target. Sticky session là cần thiết trong trường hợp các máy chủ ứng dụng lưu trữ thông tin trạng thái người dùng tại server. Hoạt động trên Network Load Balancer, Application Load Balancer, Classic Load Balancer ELB cung cấp tính năng lưu trữ logs truy cập (access logs) chúng ta có thể sử dụng access logs để phân tiích truy cập, trouble shoot. Log truy cập sẽ đựược lưu trữ vào một dịch vụ lưu trữ đối tượng là Amazon S3 (Simple Storage Service) Elastic Load Balancer - Application Load Balancer Application Load Balancer (ALB) là một dịch vụ cân bằng tải được quản lý bởi AWS, hoạt động ở Layer 7. Sử dụng giao thức HTTP, HTTPS. Hỗ trợ tính năng path-based routing. (/mobile /desktop sẽ được route tới 2 target group khác nhau) Cho phép route traffic tới cả target nằm ngoài VPC (IP Address), EC2, Lambda, Container (ECS, EKS). Elastic Load Balancer - Network Load Balancer Network Load Balancer (NLB) là một dịch vụ cân bằng tải được quản lý bởi AWS, hoạt động ở layer 4. Sử dụng giao thức TCP, TLS. Hỗ trợ tính năng set IP tĩnh Hỗ trợ hiệu năng cao nhất trong các loại Load Balancer có khả năng xử lý đến hàng triệu request. Cho phép route traffic tới cả target nằm ngoài VPC (IP Address), EC2, Container (ECS, EKS). Elastic Load Balancer - Classic Load Balancer Classic Load Balancer (CLB) là một dịch vụ cân bằng tải được quản lý bởi AWS, hoạt động ở Layer 4, và Layer 7 Sử dụng giao thức HTTP, HTTPS, TCP, TLS Chi phí cao hơn so với ALB và NLB. Ít tính năng cao cấp hơn ALB và NLB, hiện tại rất ít được sử dụng. Cho phép route traffic tơởi EC2. Elastic Load Balancer - Gateway Load Balancer Gateway Load Balancer (GLB) là một dịch vụ cân bằng tải được quản lý bởi AWS, hoạt động ở Layer 3. Gateway Load Balancer lắng nghe toàn bộ IP packets và forward tới target group được chỉ định. Sử dụng GENEVE protocol trên port 6081. Cho phép route traffic tới các virtual appliance được AWS hỗ trợ. Danh sách các vendor hỗ trợ: https://aws.amazon.com/vi/elasticloadbalancing/partners/ ","date":"2023-09-07","id":40,"permalink":"/docs/ly-thuyet/7.-vpn-direct-connect-loadbalancer-extraresources/","summary":"\u003cstyle\u003ebody {text-align: justify}\u003c/style\u003e\r\n\u003ch3 id=\"vpn-site-to-site\"\u003eVPN Site to Site\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cspan style=\"color: orange; font-weight:bold;\"\u003eVPN Site to Site\u003c/span\u003e dùng trong mô hình hybird để thiết lập kết nối liên tục giữa môi trường trung tâm dữ liệu truyền thống tới môi trường VPC của AWS. Việc thiết lập kết nối sẽ cần 2 đầu endpoint ở phía AWS và phía khách hàng:\n\u003cul\u003e\n\u003cli\u003e\u003cspan style=\"color: orange; font-weight:bold;\"\u003eVirtual Private Gateway\u003c/span\u003e: Được quản lý hoàn toàn bởi AWS (chia 2 endpoints ở 2 đầu ra AZ).\u003c/li\u003e\n\u003cli\u003e\u003cspan style=\"color: orange; font-weight:bold;\"\u003eCustomer Gateway\u003c/span\u003e: Đầu enpoint phía khách hàng, có thể là thiết bị phần cứng hoặc software appliance.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"vpn-client-to-site\"\u003eVPN Client to Site\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eVPN Client to Site: Cho phép một host truy cập tới tài nguyên trong VPC.\u003c/li\u003e\n\u003cli\u003eKhuyến kích sử dụng VPN Client to Site trong AWS Market Place\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"aws-direct-connect\"\u003eAWS Direct Connect\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cspan style=\"color: orange; font-weight:bold;\"\u003eAWS Direct Connect\u003c/span\u003e là dịch vụ cho phép tạo kết nối riêng từ trung tâm dữ liệu truyền thống tới AWS..\u003c/li\u003e\n\u003cli\u003eĐộ trễ khoảng 20ms - 30ms.\u003c/li\u003e\n\u003cli\u003eAWS Direct Connect ở Việt Nam hiện tại sẽ thông qua AWS Direct Connect partners và hoạt động dưới dạng Hosted Connections. (Nếu trực tiếp tới AWS là Đeicated Connections).\n\u003cul\u003e\n\u003cli\u003eBăng thông Direct Connect có thể thay đổi lên / xuống tùy nhu cầu.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"elastic-load-balancing\"\u003eElastic Load Balancing\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eElastic Load Balancing (ELB) là một dịch vụ cân bằng tải được quản lý bởi AWS, có chức năng phân phối lưu lượng cho nhiều EC2 Instance hoặc Container.\u003c/li\u003e\n\u003cli\u003eSử dụng giao thức HTTP, HTTPS, TCP và SSL (TCP bảo mật).\u003c/li\u003e\n\u003cli\u003eCó thể nằm ở public hoặc private subnet.\u003c/li\u003e\n\u003cli\u003eMỗi ELB sẽ được cấp tên DNS và kt nối thông qua DNS. Chỉ có Network Load Balancer hỗ trợ gắn IP tĩnh.\u003c/li\u003e\n\u003cli\u003eELB có tính năng health check, không gửi lưu lượng đến các Instance không đạt health check.\u003c/li\u003e\n\u003cli\u003eBao gồm 4 loại:\n\u003cul\u003e\n\u003cli\u003eApplication Load Balancer\u003c/li\u003e\n\u003cli\u003eNetwork Load Balancer\u003c/li\u003e\n\u003cli\u003eClassic Load Balancer\u003c/li\u003e\n\u003cli\u003eGateway Load Balancer\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cspan style=\"color: orange; font-weight:bold;\"\u003eSticky session (sesion afinity)\u003c/span\u003e: Tính năng cho phép các kết nối được gán vào một target nhất định. Việc này đảm bảo các requests từ một user trong một session sẽ được gửi tới cùng một target.\nSticky session là cần thiết trong trường hợp các máy chủ ứng dụng lưu trữ thông tin trạng thái người dùng tại server.\n\u003cul\u003e\n\u003cli\u003eHoạt động trên Network Load Balancer, Application Load Balancer, Classic Load Balancer\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eELB cung cấp tính năng lưu trữ logs truy cập (access logs) chúng ta có thể sử dụng access logs để phân tiích truy cập, trouble shoot. Log truy cập sẽ đựược lưu trữ vào một dịch vụ lưu trữ đối tượng là Amazon S3 (Simple Storage Service)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"elastic-load-balancer---application-load-balancer\"\u003eElastic Load Balancer - Application Load Balancer\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cspan style=\"color: orange; font-weight:bold;\"\u003eApplication Load Balancer (ALB)\u003c/span\u003e là một dịch vụ cân bằng tải được quản lý bởi AWS, hoạt động ở Layer 7.\u003c/li\u003e\n\u003cli\u003eSử dụng giao thức HTTP, HTTPS.\u003c/li\u003e\n\u003cli\u003eHỗ trợ tính năng \u003cspan style=\"color: orange; font-weight:bold;\"\u003epath-based routing.\u003c/span\u003e (/mobile /desktop sẽ được route tới 2 target group khác nhau)\u003c/li\u003e\n\u003cli\u003eCho phép route traffic tới cả target nằm ngoài VPC (IP Address), EC2, Lambda, Container (ECS, EKS).\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"elastic-load-balancer---network-load-balancer\"\u003eElastic Load Balancer - Network Load Balancer\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cspan style=\"color: orange; font-weight:bold;\"\u003eNetwork Load Balancer (NLB)\u003c/span\u003e là một dịch vụ cân bằng tải được quản lý bởi AWS, hoạt động ở layer 4.\u003c/li\u003e\n\u003cli\u003eSử dụng giao thức TCP, TLS.\u003c/li\u003e\n\u003cli\u003e\u003cspan style=\"color: orange; font-weight:bold;\"\u003eHỗ trợ tính năng set IP tĩnh\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003eHỗ trợ \u003cspan style=\"color: orange; font-weight:bold;\"\u003ehiệu năng cao nhất trong các loại Load Balancer\u003c/span\u003e có khả năng xử lý đến hàng triệu request.\u003c/li\u003e\n\u003cli\u003eCho phép route traffic tới cả target nằm ngoài VPC (IP Address), EC2, Container (ECS, EKS).\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"elastic-load-balancer---classic-load-balancer\"\u003eElastic Load Balancer - Classic Load Balancer\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cspan style=\"color: orange; font-weight:bold;\"\u003eClassic Load Balancer (CLB)\u003c/span\u003e là một dịch vụ cân bằng tải được quản lý bởi AWS, hoạt động ở Layer 4, và Layer 7\n\u003cul\u003e\n\u003cli\u003eSử dụng giao thức HTTP, HTTPS, TCP, TLS\u003c/li\u003e\n\u003cli\u003eChi phí cao hơn so với ALB và NLB.\u003c/li\u003e\n\u003cli\u003eÍt tính năng cao cấp hơn ALB và NLB, hiện tại rất ít được sử dụng.\u003c/li\u003e\n\u003cli\u003eCho phép route traffic tơởi EC2.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"elastic-load-balancer---gateway-load-balancer\"\u003eElastic Load Balancer - Gateway Load Balancer\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cspan style=\"color: orange; font-weight:bold;\"\u003eGateway Load Balancer (GLB)\u003c/span\u003e là một dịch vụ cân bằng tải được quản lý bởi AWS, hoạt động ở Layer 3. Gateway Load Balancer lắng nghe toàn bộ IP packets và forward tới target group được chỉ định.\u003c/li\u003e\n\u003cli\u003eSử dụng GENEVE protocol trên port 6081.\u003c/li\u003e\n\u003cli\u003eCho phép route traffic tới các virtual appliance được AWS hỗ trợ.\u003c/li\u003e\n\u003cli\u003eDanh sách các vendor hỗ trợ:\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://aws.amazon.com/vi/elasticloadbalancing/partners/\"\u003ehttps://aws.amazon.com/vi/elasticloadbalancing/partners/\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e","tags":[],"title":"7. VPN - Direct Connect - LoadBalancer - ExtraResources"},{"content":"\rAmazon EC2 giống với máy chủ áo hoặc máy chủ vật lý truyền thống.\nCó khả năng:\nkhởi tạo nhanh khả năng co dãn tài nguyên mạnh mẽ linh hoạt Amazon EC2 có thể hỗ trợ workload như lưu trữ web, ứng dụng, cơ sở dử liệu và bất cứ công việc nào khác mà máy chủ thông thường có thể đáp ứng.\nCấu hình của Amazon EC2 không được tùy chọn tùy ý, mà lựa chọn thông qua các EC2 Instance Type sẽ quyết định:\nCPU (Intel / AMD / ARM (Graviton 1/2/3) / GPU ) Memory Network Storage AMI / Backup / Key Pair Sử dụng AMI (Amazon Machine Image) có thể provision ra một hoặc nhiều EC2 Instances cùng lúc. AMI có sẵn của AWS, trên AWS Market Place và custom AMI tự tạo từ EC2 AMI bao gồm root OS volumes, quyền sử dụng AMI quy định tài khoản AWS được sử dụng và mapping EBS volume sẽ được tạo và gán vào EC2 Instances EC2 Instance có thể được backup bằng cách tạo snapshot. Key pair (public key và private key) dùng để mã hóa thông tin đăng nhập cho EC2 Instance. Elastic Block Store Amazon EBS cung cấp block storage và được gán trực tiếp vào EC2 Instance, tuy được gán trực tiếp như một RAW device, EBS về bản chất hoạt động độc lập với EC2 và được kết nối thông qua mạng riêng của EBS. EBS ó 2 nhóm đĩa chính là HDD và SSD, được thiết kế để đạt độ sẵn sàng 99.999% bằng cách replicate dữ liệu giữa 3 Storage Node trong 1 AZ Có một số EC2 Instances đặc thù được tối ưu hóa hiệu năng EBS (Optimized EBS Instances) EBS volumes, mặc định chỉ được gán vào 1 EC2 Instance, EC2 Instances chạy trên Hypervisor Nitro có thể dùng 1 EBS volume gắn vào nhiều EC2 Instances. (EBS Multi attach) EBS được backup bằng cách thực hiện snapshot vào S3 (Simple Storage Service) Snapshot đầu tiên là full, tất cả các snapshot tiếp theo là incremental. Instance Store Instance store là vùng đĩa NVME tốc độ cực cao, nằm trên physical node chạy các máy ảo EC2. Instance store sẽ bị xóa hết dữ liệu khi chúng ta thực hiện stop EC2 Instance Instance store sẽ không bị xóa dữ liệu khi thực hiện restart máy hoặc bị crash. Instance store không replicate dữ liệu dự phòng nên thường không khuyến khích lưu trữ dữ liệu quan trọng. Sử dụng dữ liệu trong các trường hợp cần hiệu năng cực cao lên tới hàng triệu IOPS. Khi sử dụng thường được replicate dữ liệu vào một EBS volume để đảm bảo an toàn. User Data EC2 user data là đoạn script chạy 1 lần khi provision EC2 Instance từ AMI. Tùy hệ điều hành mà sẽ sử dụng shell scripts (Linux) / powershell (Windows) EC2 Metadata EC2 Metadata là các thông tin liên quan đến bản thân EC2 instances, ví dụ địa chỉ IP Private, Public, Hostname, Security groups\u0026hellip; Dùng thông tin EC2 Metadata để thiết lập hostname cho EC2 Instance Linux với EC2 user data. EC2 Auto Scaling EC2 Auto Scaling alf tính năng hỗ trợ tăng giảm số lượng EC2 Instance dựa theo các điều kiện cụ thể (scaling policy) EC2 Auto Scaling có thể tự đăng ký các EC2 Instance vào Elastic Load Balancer EC22 Auto Scaling hoạt động trên nhiều AWS Availability Zone EC2 Auto Scaling có thể hỗ trợ nhiều Pricing options khác nhau. EC2 Auto Scaling - EPS/FSx - Lightsail - MGN EC2 Pricing Options Bao gồm 4 tùy chọn giá:\nOn-demand Trả theo giờ / phút / giây, xài nhiêu tính nhiêu, mắc nhất. Phù hợp cho các workload chạy lên tới 6 tiếng 1 ngày. Reserved Instance: Cam kết sử dụng theo kì hạn 1-3 năm để lấy discount, tuy nhiên bị giới hạn theo EC2 Instance type/ family Saving Plans: Cam kết sử dụng theo kì hạn 1-3 năm để lấy discount, có thể không bị giới hạn bởi EC2 Instance type family. Spot Instance: Tận dụng tài nguyên dư, giá rẻ tuy nhiên khi cần thì AWS sẽ terminate instance trong 2 phút. Kết hợp nhiều Pricing Options trong EC2 Auto Scaling Group.\nAmazon Lightsail Amazon Lightsail là dịch vụ tính toán có chi phí thấp (giá tính theo tháng chỉ bắt đầu từ 3.5 $ / tháng) ngoài ra mỗi Instance Lightsail tạo ra sẽ có một mức data transfer đi kèm (data transfer này có mức giá rẻ hơn data transfer từ EC2 tương đối nhiều) Phù hợp cho các workload nhẹ, môi trường test dev, không yêu cầu tải CPU cao liên tục \u0026gt; 2h mỗi ngày. Có khả năng backup bắng snapshot tương tự EC2. Chạy trong VPC đặc biệt, có thể kết nối tới VPC thông thường qua VPC Peering. Amazon EFS EFS (Elastic File System) cho phép tạo các NFSv4 Network volume và gán vào nhiều EC2 Instance cùng lúc, quy mô lưu trữ đến hàng petrabyte. EFS chỉ support Linux. Chi phí hteo dung lượng sử dụng (trong khi EBS tính phí theo dung lượng ấp phát). EFS có thể được cấu hình để mount vào môi trường on-premise qua DX hoặc VPN. Amazon FSX FSx cho phép tạo các NTFS volume và gán vào nhiều EC2 Instances cùng lúc sử dụng giao thức SMB (Server Message Block), FSx support Windows và Linux Sử dụng FSx chỉ tính phí theo dung lượng sử dụng. FSx hỗ trợ tính năng deduplication, giúp giảm chi phí 30-50% cho các trường hợp sử dụng thông thường. AWS Application Migration Servie (MGN) AWS Application Migration Service (MGN) dùng để migrate và replicate phục vụ mục đích xây dựng Disaster Recovery Site cho các máy chủ thực, ảo lên môi trường AWS. MGN liên tục sao chép các máy chủ nguồn sang EC2 Instance trên tài khoản AWS (asynchronous / synchronous) MGN trong quá trình sao chép sẽ sử dụng các máy staging có số lượng và quy mô cấu hình nhỏ hơn máy chủ gốc rất nhiều. Khi thực hiện cut-over MGN sẽ tự động tạo và chạy các máy chủ EC2 tren AWS. ","date":"2023-09-07","id":41,"permalink":"/docs/ly-thuyet/8.-amazon-elastic-compute-cloud-ec2/","summary":"\u003cstyle\u003ebody {text-align: justify}\u003c/style\u003e\r\n\u003cp\u003e\u003cspan style=\"color: orange; font-weight:bold;\"\u003eAmazon EC2\u003c/span\u003e giống với máy chủ áo hoặc máy chủ vật lý truyền thống.\u003c/p\u003e\n\u003cp\u003eCó khả năng:\u003c/p\u003e","tags":[],"title":"8. Amazon Elastic Compute Cloud (EC2)"},{"content":"","date":"2023-09-07","id":42,"permalink":"/docs/thuc-hanh/th%E1%BB%B1c-h%C3%A0nh-l%E1%BA%A7n-1/","summary":"\u003cstyle\u003ebody {text-align: justify}\u003c/style\u003e","tags":[],"title":"Thực hành lần 1"},{"content":"Link to valuable, relevant resources.\n","date":"2024-02-27","id":43,"permalink":"/docs/resources/","summary":"\u003cp\u003eLink to valuable, relevant resources.\u003c/p\u003e","tags":[],"title":"Resources"},{"content":"","date":"2023-09-07","id":44,"permalink":"/de-docs/","summary":"","tags":[],"title":"DE Docs"},{"content":"","date":"2023-09-07","id":45,"permalink":"/docs/","summary":"","tags":[],"title":"Docs"},{"content":"","date":"2023-09-07","id":46,"permalink":"/privacy/","summary":"","tags":[],"title":"Privacy Policy"},{"content":"","date":"2023-09-07","id":47,"permalink":"/","summary":"","tags":[],"title":"Kho kiến thức của Dũng"},{"content":"","date":"0001-01-01","id":48,"permalink":"/categories/","summary":"","tags":[],"title":"Categories"},{"content":"","date":"0001-01-01","id":49,"permalink":"/contributors/","summary":"","tags":[],"title":"Contributors"},{"content":"","date":"0001-01-01","id":50,"permalink":"/tags/","summary":"","tags":[],"title":"Tags"}]